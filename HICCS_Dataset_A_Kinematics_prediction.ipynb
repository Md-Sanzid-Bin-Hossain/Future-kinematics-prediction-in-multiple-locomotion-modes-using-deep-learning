{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Md-Sanzid-Bin-Hossain/Future-kinematics-prediction-in-multiple-locomotion-modes-using-deep-learning/blob/main/HICCS_Dataset_A_Kinematics_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJgSNrI2pTfC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "577c396a-0a5f-4f4b-f6f3-db407b61f85f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# !pip install torchviz\n",
        "# !pip install tsf\n",
        "\n",
        "\n",
        "import h5py\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import numpy\n",
        "import statistics\n",
        "from numpy import loadtxt\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas\n",
        "import math\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from statistics import stdev\n",
        "import math\n",
        "import h5py\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "from scipy.signal import butter,filtfilt\n",
        "import sys\n",
        "import numpy as np # linear algebra\n",
        "from scipy.stats import randint\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv), data manipulation as in SQL\n",
        "import matplotlib.pyplot as plt # this is used for the plot the graph\n",
        "import seaborn as sns # used for plot interactive graph.\n",
        "import pandas\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# from tsf.model import TransformerForecaster\n",
        "\n",
        "\n",
        "# from tensorflow.keras.utils import np_utils\n",
        "import itertools\n",
        "###  Library for attention layers\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "#from tqdm import tqdm # Processing time measurement\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import statistics\n",
        "import gc\n",
        "import torch.nn.init as init\n",
        "\n",
        "############################################################################################################################################################################\n",
        "############################################################################################################################################################################\n",
        "\n",
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.utils.weight_norm as weight_norm\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "from tqdm import tqdm_notebook\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W17jUoA2pwjm"
      },
      "source": [
        "# Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1qJ-_XVjY3z"
      },
      "outputs": [],
      "source": [
        "def data_loader(subject):\n",
        "  with h5py.File('/content/drive/My Drive/Kinematics Prediction/All_subjects_data_kinetics.h5', 'r') as hf:\n",
        "    All_subjects = hf['All_subjects']\n",
        "    Subject = All_subjects[subject]\n",
        "\n",
        "    HOF=Subject['hof']\n",
        "    IMU_KIN=Subject['IMU_Kin']\n",
        "\n",
        "    treadmill_hof = HOF['Treadmill']\n",
        "    levelground_hof = HOF['Levelground']\n",
        "    slope_hof = HOF['Slope']\n",
        "    stair_hof = HOF['Stair']\n",
        "    round_hof = HOF['Round']\n",
        "    obstacles_hof = HOF['Obstacles']\n",
        "\n",
        "    treadmill_IMU_kin = IMU_KIN['Treadmill']\n",
        "    levelground_IMU_kin = IMU_KIN['Levelground']\n",
        "    slope_IMU_kin = IMU_KIN['Slope']\n",
        "    stair_IMU_kin = IMU_KIN['Stair']\n",
        "    round_IMU_kin= IMU_KIN['Round']\n",
        "    obstacles_IMU_kin = IMU_KIN['Obstacles']\n",
        "\n",
        "\n",
        "    hof_data=np.concatenate((treadmill_hof,levelground_hof,slope_hof,stair_hof,round_hof,obstacles_hof),axis=0)\n",
        "    IMU_kin_data=np.concatenate((treadmill_IMU_kin,levelground_IMU_kin,slope_IMU_kin,stair_IMU_kin,round_IMU_kin,obstacles_IMU_kin),axis=0)\n",
        "\n",
        "    return np.array(hof_data), np.array(IMU_kin_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "id": "53dWXPgXkt4w",
        "outputId": "4f4e05c8-e2b4-49c4-c44b-cc351e047a10"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-0fecc3347aac>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msubject_1_data_hof\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubject_1_data_IMU_Kin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Subject_1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msubject_2_data_hof\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubject_2_data_IMU_Kin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Subject_2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msubject_3_data_hof\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubject_3_data_IMU_Kin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Subject_3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-1ec19bad2eb9>\u001b[0m in \u001b[0;36mdata_loader\u001b[0;34m(subject)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/Kinematics Prediction/All_subjects_data_kinetics.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mAll_subjects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'All_subjects'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mSubject\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAll_subjects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    565\u001b[0m                                  \u001b[0mfs_persist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_persist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m                                  fs_page_size=fs_page_size)\n\u001b[0;32m--> 567\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to open file (unable to open file: name = '/content/drive/My Drive/Kinematics Prediction/All_subjects_data_kinetics.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
          ]
        }
      ],
      "source": [
        "subject_1_data_hof, subject_1_data_IMU_Kin=data_loader('Subject_1')\n",
        "gc.collect()\n",
        "subject_2_data_hof, subject_2_data_IMU_Kin=data_loader('Subject_2')\n",
        "gc.collect()\n",
        "subject_3_data_hof, subject_3_data_IMU_Kin=data_loader('Subject_3')\n",
        "gc.collect()\n",
        "subject_4_data_hof, subject_4_data_IMU_Kin=data_loader('Subject_4')\n",
        "gc.collect()\n",
        "subject_5_data_hof, subject_5_data_IMU_Kin=data_loader('Subject_5')\n",
        "gc.collect()\n",
        "subject_6_data_hof, subject_6_data_IMU_Kin=data_loader('Subject_6')\n",
        "gc.collect()\n",
        "subject_7_data_hof, subject_7_data_IMU_Kin=data_loader('Subject_7')\n",
        "gc.collect()\n",
        "subject_8_data_hof, subject_8_data_IMU_Kin=data_loader('Subject_8')\n",
        "gc.collect()\n",
        "subject_9_data_hof, subject_9_data_IMU_Kin=data_loader('Subject_9')\n",
        "gc.collect()\n",
        "subject_10_data_hof, subject_10_data_IMU_Kin=data_loader('Subject_10')\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A4Eoxgop1qX"
      },
      "source": [
        "#Subject Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUDPyqjRoDoZ"
      },
      "outputs": [],
      "source": [
        "main_dir = \"/content/drive/My Drive/public dataset/Public_dataset_2/Subject01\"\n",
        "# os.mkdir(main_dir)\n",
        "path=\"/content/\"\n",
        "subject='Subject_01'\n",
        "encoder='lstm'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iMx41lxp4Jp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "3bd4df5f-d1bd-48b6-e01c-cb55b53e3163"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-91bb1435e4ad>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_data_hof=np.concatenate((subject_1_data_hof,subject_2_data_hof,subject_4_data_hof,subject_5_data_hof,subject_6_data_hof,\n\u001b[0m\u001b[1;32m      2\u001b[0m                                subject_7_data_hof,subject_8_data_hof,subject_9_data_hof,subject_10_data_hof),axis=0)\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m train_data_IMU_Kin=np.concatenate((subject_1_data_IMU_Kin,subject_2_data_IMU_Kin,subject_4_data_IMU_Kin,subject_5_data_IMU_Kin,subject_6_data_IMU_Kin,\n\u001b[1;32m      5\u001b[0m                                subject_7_data_IMU_Kin,subject_8_data_IMU_Kin,subject_9_data_IMU_Kin,subject_10_data_IMU_Kin),axis=0)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'subject_1_data_hof' is not defined"
          ]
        }
      ],
      "source": [
        "train_data_hof=np.concatenate((subject_1_data_hof,subject_2_data_hof,subject_4_data_hof,subject_5_data_hof,subject_6_data_hof,\n",
        "                               subject_7_data_hof,subject_8_data_hof,subject_9_data_hof,subject_10_data_hof),axis=0)\n",
        "\n",
        "train_data_IMU_Kin=np.concatenate((subject_1_data_IMU_Kin,subject_2_data_IMU_Kin,subject_4_data_IMU_Kin,subject_5_data_IMU_Kin,subject_6_data_IMU_Kin,\n",
        "                               subject_7_data_IMU_Kin,subject_8_data_IMU_Kin,subject_9_data_IMU_Kin,subject_10_data_IMU_Kin),axis=0)\n",
        "\n",
        "# train_data_hof=np.concatenate((subject_2_data_hof,subject_3_data_hof,subject_4_data_hof,subject_5_data_hof),axis=0)\n",
        "\n",
        "# train_data_IMU_Kin=np.concatenate((subject_2_data_IMU_Kin,subject_3_data_IMU_Kin,subject_4_data_IMU_Kin,subject_5_data_IMU_Kin),axis=0)\n",
        "\n",
        "test_data_hof=subject_3_data_hof\n",
        "test_data_IMU_Kin=subject_3_data_IMU_Kin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qj2C3WmxqC4r"
      },
      "source": [
        "# Data processing and preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGYu4d7pqFe3",
        "outputId": "d56c889f-af03-485c-fb0b-54e12c2b9f47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1602477, 48)\n",
            "(534159, 36)\n",
            "(1602477, 6)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "train_dataset_IMU=train_data_IMU_Kin[:,0:48]\n",
        "train_dataset_hof=train_data_hof\n",
        "train_dataset_target=np.concatenate((train_data_IMU_Kin[:,55:56],train_data_IMU_Kin[:,58:60],train_data_IMU_Kin[:,62:63],train_data_IMU_Kin[:,65:67]),axis=1)\n",
        "\n",
        "\n",
        "test_dataset_IMU=test_data_IMU_Kin[:,0:48]\n",
        "test_dataset_hof=test_data_hof\n",
        "test_dataset_target=np.concatenate((test_data_IMU_Kin[:,55:56],test_data_IMU_Kin[:,58:60],test_data_IMU_Kin[:,62:63],test_data_IMU_Kin[:,65:67]),axis=1)\n",
        "\n",
        "print(train_dataset_IMU.shape)\n",
        "print(train_dataset_hof.shape)\n",
        "print(train_dataset_target.shape)\n",
        "\n",
        "\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bkc9S0Ls7n3"
      },
      "outputs": [],
      "source": [
        "# # convert an array of values into a dataset matrix\n",
        "def create_dataset_IMU(dataset_1, window=45):\n",
        "  dataX= []\n",
        "  k=0\n",
        "  shift=9\n",
        "  for i in range(int(len(dataset_1)/shift-8)):\n",
        "    j=shift*k\n",
        "    a = dataset_1[j:j+window,:]\n",
        "    # print(a.shape)\n",
        "    dataX.append(a)\n",
        "    k=k+1\n",
        "  return np.array(dataX)\n",
        "\n",
        "# # convert an array of values into a dataset matrix\n",
        "def create_dataset_hof(dataset_1, window=15):\n",
        "  dataX= []\n",
        "  k=0\n",
        "  shift=3\n",
        "  for i in range(int(len(dataset_1)/shift-8)):\n",
        "    j=shift*k\n",
        "    a = dataset_1[j:j+window,:]\n",
        "    # print(a.shape)\n",
        "    dataX.append(a)\n",
        "    k=k+1\n",
        "  return np.array(dataX)\n",
        "\n",
        "# # convert an array of values into a dataset matrix\n",
        "def create_dataset_Kinematics(dataset_1, window=45):\n",
        "  dataX= []\n",
        "  k=0\n",
        "  shift=9\n",
        "  for i in range(int(len(dataset_1)/shift-8)):\n",
        "      j=shift*k\n",
        "      a = dataset_1[j+window:j+window+shift,:]\n",
        "      dataX.append(a)\n",
        "      k=k+1\n",
        "  return np.array(dataX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quLo3RKSuMtS",
        "outputId": "89b4cd2e-2dce-44c5-cafb-21b7a4a079b3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "window=45\n",
        "window_hof=15\n",
        "\n",
        "train_IMU=create_dataset_IMU(train_dataset_IMU)\n",
        "train_hof=create_dataset_hof(train_dataset_hof)\n",
        "train_target_future=create_dataset_Kinematics(train_dataset_target)\n",
        "train_target_present=create_dataset_IMU(train_dataset_target)\n",
        "\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "\n",
        "test_IMU=create_dataset_IMU(test_dataset_IMU)\n",
        "test_hof=create_dataset_hof(test_dataset_hof)\n",
        "test_target_future=create_dataset_Kinematics(test_dataset_target)\n",
        "test_target_present=create_dataset_IMU(test_dataset_target)\n",
        "\n",
        "\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Omp6f5XueRP",
        "outputId": "d2cdf028-95c6-4588-f507-797e322f3972"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(178045, 45, 48)\n",
            "(178045, 15, 36)\n",
            "(178045, 45, 6)\n",
            "(178045, 9, 6)\n"
          ]
        }
      ],
      "source": [
        "print(train_IMU.shape)\n",
        "print(train_hof.shape)\n",
        "print(train_target_present.shape)\n",
        "print(train_target_future.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X06_a5vPh4iO"
      },
      "outputs": [],
      "source": [
        "train_X_IMU, X_validation_IMU,train_X_hof, X_validation_hof, train_y_5_present, Y_validation_present , train_y_5_future, Y_validation_future=train_test_split(train_IMU,train_hof,train_target_present, train_target_future,test_size=0.20, random_state=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xokCCiskxYZ3"
      },
      "outputs": [],
      "source": [
        "\n",
        "### Data Processing\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "## all Modality Features\n",
        "\n",
        "train_features = torch.Tensor(train_X_IMU)\n",
        "train_features_hof = torch.Tensor(train_X_hof)\n",
        "train_targets_present = torch.Tensor(train_y_5_present)\n",
        "train_targets_future= torch.Tensor(train_y_5_future)\n",
        "\n",
        "\n",
        "val_features = torch.Tensor(X_validation_IMU)\n",
        "val_features_hof = torch.Tensor(X_validation_hof)\n",
        "val_targets_present = torch.Tensor(Y_validation_present)\n",
        "val_targets_future = torch.Tensor(Y_validation_future)\n",
        "\n",
        "\n",
        "test_features = torch.Tensor(test_IMU)\n",
        "test_features_hof = torch.Tensor(test_hof)\n",
        "test_targets_present = torch.Tensor(test_target_present)\n",
        "test_targets_future = torch.Tensor(test_target_future)\n",
        "\n",
        "\n",
        "train_features_acc_8=torch.cat((train_features[:,:,0:3],train_features[:,:,6:9],train_features[:,:,12:15],train_features[:,:,18:21],train_features[:,:,24:27]\\\n",
        "                             ,train_features[:,:,30:33],train_features[:,:,36:39],train_features[:,:,42:45]),axis=-1)\n",
        "test_features_acc_8=torch.cat((test_features[:,:,0:3],test_features[:,:,6:9],test_features[:,:,12:15],test_features[:,:,18:21],test_features[:,:,24:27]\\\n",
        "                             ,test_features[:,:,30:33],test_features[:,:,36:39],test_features[:,:,42:45]),axis=-1)\n",
        "val_features_acc_8=torch.cat((val_features[:,:,0:3],val_features[:,:,6:9],val_features[:,:,12:15],val_features[:,:,18:21],val_features[:,:,24:27]\\\n",
        "                             ,val_features[:,:,30:33],val_features[:,:,36:39],val_features[:,:,42:45]),axis=-1)\n",
        "\n",
        "\n",
        "train_features_gyr_8=torch.cat((train_features[:,:,3:6],train_features[:,:,9:12],train_features[:,:,15:18],train_features[:,:,21:24],train_features[:,:,27:30]\\\n",
        "                             ,train_features[:,:,33:36],train_features[:,:,39:42],train_features[:,:,45:48]),axis=-1)\n",
        "test_features_gyr_8=torch.cat((test_features[:,:,3:6],test_features[:,:,9:12],test_features[:,:,15:18],test_features[:,:,21:24],test_features[:,:,27:30]\\\n",
        "                             ,test_features[:,:,33:36],test_features[:,:,39:42],test_features[:,:,45:48]),axis=-1)\n",
        "val_features_gyr_8=torch.cat((val_features[:,:,3:6],val_features[:,:,9:12],val_features[:,:,15:18],val_features[:,:,21:24],val_features[:,:,27:30]\\\n",
        "                             ,val_features[:,:,33:36],val_features[:,:,39:42],val_features[:,:,45:48]),axis=-1)\n",
        "\n",
        "\n",
        "\n",
        "train = TensorDataset(train_features, train_features_acc_8,train_features_gyr_8,train_features_hof, train_targets_present, train_targets_future)\n",
        "val = TensorDataset(val_features, val_features_acc_8, val_features_gyr_8,val_features_hof,val_targets_present, val_targets_future)\n",
        "test = TensorDataset(test_features, test_features_acc_8, test_features_gyr_8,test_features_hof, test_targets_present, test_targets_future)\n",
        "\n",
        "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "val_loader = DataLoader(val, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "test_loader = DataLoader(test, batch_size=batch_size, shuffle=True, drop_last=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfmjjDCuoNUj",
        "outputId": "6a1cd7f0-f600-4573-bdd5-acf49961cf9e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jl71qLP9hK54"
      },
      "source": [
        "# Important Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYfzxjnmhNj3"
      },
      "outputs": [],
      "source": [
        "import scipy.stats as stats\n",
        "\n",
        "def RMSE_prediction(yhat_4,test_y):\n",
        "\n",
        "  s1=yhat_4.shape[0]*yhat_4.shape[1]\n",
        "\n",
        "  test_o=test_y.reshape((s1,6))\n",
        "  yhat=yhat_4.reshape((s1,6))\n",
        "\n",
        "\n",
        "  y_1_no=yhat[:,0]\n",
        "  y_2_no=yhat[:,1]\n",
        "  y_3_no=yhat[:,2]\n",
        "  y_4_no=yhat[:,3]\n",
        "  y_5_no=yhat[:,4]\n",
        "  y_6_no=yhat[:,5]\n",
        "\n",
        "\n",
        "  y_1=y_1_no\n",
        "  y_2=y_2_no\n",
        "  y_3=y_3_no\n",
        "  y_4=y_4_no\n",
        "  y_5=y_5_no\n",
        "  y_6=y_6_no\n",
        "\n",
        "\n",
        "  y_test_1=test_o[:,0]\n",
        "  y_test_2=test_o[:,1]\n",
        "  y_test_3=test_o[:,2]\n",
        "  y_test_4=test_o[:,3]\n",
        "  y_test_5=test_o[:,4]\n",
        "  y_test_6=test_o[:,5]\n",
        "\n",
        "\n",
        "  ###calculate RMSE\n",
        "\n",
        "  rmse_1 =np.sqrt(mean_squared_error(y_test_1,y_1))\n",
        "  rmse_2 =np.sqrt(mean_squared_error(y_test_2,y_2))\n",
        "  rmse_3 =np.sqrt(mean_squared_error(y_test_3,y_3))\n",
        "  rmse_4 =np.sqrt(mean_squared_error(y_test_4,y_4))\n",
        "  rmse_5 =np.sqrt(mean_squared_error(y_test_5,y_5))\n",
        "  rmse_6 =np.sqrt(mean_squared_error(y_test_6,y_6))\n",
        "\n",
        "\n",
        "  print(rmse_1)\n",
        "  print(rmse_2)\n",
        "  print(rmse_3)\n",
        "  print(rmse_4)\n",
        "  print(rmse_5)\n",
        "  print(rmse_6)\n",
        "\n",
        "\n",
        "  p_1=np.corrcoef(y_1, y_test_1)[0, 1]\n",
        "  p_2=np.corrcoef(y_2, y_test_2)[0, 1]\n",
        "  p_3=np.corrcoef(y_3, y_test_3)[0, 1]\n",
        "  p_4=np.corrcoef(y_4, y_test_4)[0, 1]\n",
        "  p_5=np.corrcoef(y_5, y_test_5)[0, 1]\n",
        "  p_6=np.corrcoef(y_6, y_test_6)[0, 1]\n",
        "\n",
        "\n",
        "\n",
        "  print(\"\\n\")\n",
        "  print(p_1)\n",
        "  print(p_2)\n",
        "  print(p_3)\n",
        "  print(p_4)\n",
        "  print(p_5)\n",
        "  print(p_6)\n",
        "\n",
        "\n",
        "\n",
        "              ### Correlation ###\n",
        "  p=np.array([(p_1+p_4)/2,(p_2+p_5)/2,(p_3+p_6)/2])\n",
        "\n",
        "\n",
        "      #### Mean and standard deviation ####\n",
        "\n",
        "  rmse=np.array([(rmse_1+rmse_4)/2,(rmse_2+rmse_5)/2,(rmse_3+rmse_6)/2])\n",
        "\n",
        "      #### Mean and standard deviation ####\n",
        "  m=statistics.mean(rmse)\n",
        "  SD=statistics.stdev(rmse)\n",
        "  print('Mean: %.3f' % m,'+/- %.3f' %SD)\n",
        "\n",
        "  m_c=statistics.mean(p)\n",
        "  SD_c=statistics.stdev(p)\n",
        "  print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)\n",
        "\n",
        "\n",
        "\n",
        "  return rmse, p\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HqWemuj8qNm"
      },
      "outputs": [],
      "source": [
        "# Define custom RMSE loss function\n",
        "class RMSELoss(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RMSELoss, self).__init__()\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        mse_loss = torch.nn.MSELoss()(pred, target)\n",
        "        rmse_loss = torch.sqrt(mse_loss)\n",
        "        return rmse_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXk4MvglhQy1"
      },
      "outputs": [],
      "source": [
        "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwLfKzwAu0kj"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class PearsonCorrCoefLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PearsonCorrCoefLoss, self).__init__()\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        x = y_pred - torch.mean(y_pred)\n",
        "        y = y_true - torch.mean(y_true)\n",
        "        loss = torch.sum(x * y) / (torch.sqrt(torch.sum(x ** 2)) * torch.sqrt(torch.sum(y ** 2)))\n",
        "        return 1- loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOb4ZMy9mfku"
      },
      "source": [
        "# Model Training-- 8 IMUs+ Hof"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m58C4hoLmw__"
      },
      "source": [
        "## Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EsrBvKyf6mH"
      },
      "outputs": [],
      "source": [
        "def train_mm_m(train_loader, learn_rate, EPOCHS, model,filename):\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      model.cuda()\n",
        "    # Defining loss function and optimizer\n",
        "    # criterion =nn.MSELoss()\n",
        "    criterion =RMSELoss()\n",
        "    # criterion =PearsonCorrCoefLoss()\n",
        "\n",
        "\n",
        "    # criterion=PearsonCorrLoss()\n",
        "    # optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "\n",
        "    running_loss=0\n",
        "    # Train the model\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Train the model with early stopping\n",
        "    best_val_loss = float('inf')\n",
        "    patience = 10\n",
        "\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        epoch_start_time = time.time()\n",
        "        model.train()\n",
        "        for i, (data, data_acc, data_gyr, data_hof, target_present, target_future) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            output= model(data_acc.to(device).float(),data_gyr.to(device).float(),data_hof.to(device).float())\n",
        "\n",
        "            loss = criterion(output, target_future.to(device).float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        train_loss=running_loss/len(train_loader)\n",
        "\n",
        "       # Validate\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for data, data_acc, data_gyr, data_hof, target_present, target_future in val_loader:\n",
        "                output= model(data_acc.to(device).float(),data_gyr.to(device).float(),data_hof.to(device).float())\n",
        "                val_loss += criterion(output, target_future.to(device).float())\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        epoch_end_time = time.time()\n",
        "        epoch_training_time = epoch_end_time - epoch_start_time\n",
        "\n",
        "        torch.set_printoptions(precision=4)\n",
        "\n",
        "        print(f\"Epoch: {epoch+1}, time: {epoch_training_time:.4f}, Training Loss: {train_loss:.4f},  Validation loss: {val_loss:.4f}\")\n",
        "\n",
        "        running_loss=0\n",
        "\n",
        "        epoch_end_time = time.time()\n",
        "\n",
        "                # Check if the validation loss has improved\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), filename)\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        # Early stopping if the validation loss hasn't improved for `patience` epochs\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Stopping early after {epoch+1} epochs\")\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    training_time = end_time - start_time\n",
        "    print(f\"Training time: {training_time} seconds\")\n",
        "\n",
        "\n",
        "    # # Save the trained model\n",
        "    # torch.save(model.state_dict(), \"model.pth\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. LSTM"
      ],
      "metadata": {
        "id": "ImUHYy9dtnZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.lstm_1 = nn.LSTM(input_dim, 128, bidirectional=False, batch_first=True, dropout=0.0)\n",
        "        self.lstm_2 = nn.LSTM(128, 64, bidirectional=False, batch_first=True, dropout=0.0)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_1, _ = self.lstm_1(x)\n",
        "        out_1=self.dropout(out_1)\n",
        "        out_2, (h_n, c_n) = self.lstm_2(out_1)\n",
        "        out_2=self.dropout(out_2)\n",
        "\n",
        "        return out_2, (h_n, c_n)"
      ],
      "metadata": {
        "id": "oJxiWy3otp8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCj7fHxDt3rw"
      },
      "outputs": [],
      "source": [
        "class MM_LSTM(nn.Module):\n",
        "    def __init__(self, input_acc, input_gyr,input_hof, drop_prob=0.0):\n",
        "        super(MM_LSTM, self).__init__()\n",
        "\n",
        "        self.encoder_acc=Encoder(input_acc, drop_prob)\n",
        "        self.encoder_gyr=Encoder(input_gyr, drop_prob)\n",
        "        self.encoder_hof=Encoder(input_hof, drop_prob)\n",
        "\n",
        "        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)\n",
        "        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)\n",
        "        self.BN_hof= nn.BatchNorm1d(input_hof, affine=False)\n",
        "\n",
        "        self.fc = nn.Linear(3*64, 6*9)\n",
        "\n",
        "    def forward(self, x_acc, x_gyr, x_hof):\n",
        "\n",
        "        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))\n",
        "        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))\n",
        "        x_hof_1=x_hof.view(x_hof.size(0)*x_hof.size(1),x_hof.size(-1))\n",
        "\n",
        "        x_acc_1=self.BN_acc(x_acc_1)\n",
        "        x_gyr_1=self.BN_gyr(x_gyr_1)\n",
        "        x_hof_1=self.BN_hof(x_hof_1)\n",
        "\n",
        "        x_acc_2=x_acc_1.view(-1, window, x_acc_1.size(-1))\n",
        "        x_gyr_2=x_gyr_1.view(-1, window, x_gyr_1.size(-1))\n",
        "        x_hof_2=x_hof_1.view(-1, window_hof, x_hof_1.size(-1))\n",
        "\n",
        "        x_acc,(h_acc,c_acc)=self.encoder_acc(x_acc_2)\n",
        "        x_gyr,(h_gyr,c_gyr)=self.encoder_gyr(x_gyr_2)\n",
        "        x_hof,(h_hof,c_hof)=self.encoder_hof(x_hof_2)\n",
        "\n",
        "        x=torch.cat((x_acc[:,-1,:],x_gyr[:,-1,:],x_hof[:,-1,:]),dim=-1).squeeze(0)\n",
        "\n",
        "        out=self.fc(x).view(x.shape[0],9,6)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "460a3c4c-fbb9-4981-c217-5d2af2c253c6",
        "id": "sGz7e5Sst3rx"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, time: 19.0688, Training Loss: 8.5178,  Validation loss: 4.3971\n",
            "Epoch: 2, time: 18.4343, Training Loss: 3.7879,  Validation loss: 3.2354\n",
            "Epoch: 3, time: 21.4182, Training Loss: 3.0854,  Validation loss: 2.8710\n",
            "Epoch: 4, time: 22.7827, Training Loss: 2.7398,  Validation loss: 2.5075\n",
            "Epoch: 5, time: 19.2723, Training Loss: 2.5484,  Validation loss: 2.4016\n",
            "Epoch: 6, time: 18.4387, Training Loss: 2.4026,  Validation loss: 2.2563\n",
            "Epoch: 7, time: 18.1398, Training Loss: 2.3039,  Validation loss: 2.2061\n",
            "Epoch: 8, time: 18.2609, Training Loss: 2.2265,  Validation loss: 2.1467\n",
            "Epoch: 9, time: 18.1727, Training Loss: 2.1658,  Validation loss: 2.1068\n",
            "Epoch: 10, time: 18.3358, Training Loss: 2.1094,  Validation loss: 2.0551\n",
            "Epoch: 11, time: 18.1022, Training Loss: 2.0567,  Validation loss: 2.0282\n",
            "Epoch: 12, time: 18.2473, Training Loss: 2.0208,  Validation loss: 2.0161\n",
            "Epoch: 13, time: 18.1760, Training Loss: 1.9851,  Validation loss: 1.9675\n",
            "Epoch: 14, time: 18.2178, Training Loss: 1.9498,  Validation loss: 1.9510\n",
            "Epoch: 15, time: 18.1014, Training Loss: 1.9272,  Validation loss: 1.9410\n",
            "Training time: 283.2610363960266 seconds\n"
          ]
        }
      ],
      "source": [
        "lr = 0.001\n",
        "model = MM_LSTM(24,24,36)\n",
        "\n",
        "mm_lstm = train_mm_m(train_loader, lr,15,model,path+subject+'_mm_lstm_IMU8_hof.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64004662-59eb-4d22-9ace-fedecf01dea7",
        "id": "C6I0ZVVXt3rx"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2420322f-5376-49af-f20a-a24e3942b48f",
        "id": "YnIvkhoDt3ry"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(18718, 9, 6)\n",
            "5.1902432\n",
            "6.57961\n",
            "4.289622\n",
            "3.6547093\n",
            "4.440761\n",
            "3.7377357\n",
            "\n",
            "\n",
            "0.9737981103675337\n",
            "0.9741165323098353\n",
            "0.9348505551670928\n",
            "0.9787774703574736\n",
            "0.9829825381140588\n",
            "0.9434981510814646\n",
            "Mean: 4.649 +/- 0.773\n",
            "Mean: 0.965 +/- 0.022\n"
          ]
        }
      ],
      "source": [
        "mm_lstm= MM_LSTM(24,24,36)\n",
        "mm_lstm.load_state_dict(torch.load(path+subject+'_mm_lstm_IMU8_hof.pth'))\n",
        "mm_lstm.to(device)\n",
        "\n",
        "mm_lstm.eval()\n",
        "\n",
        "# iterate through batches of test data\n",
        "with torch.no_grad():\n",
        "    for i, (data, data_acc, data_gyr, data_hof, target_present, target_future) in enumerate(test_loader):\n",
        "        output = mm_lstm(data_acc.to(device).float(),data_gyr.to(device).float(),data_hof.to(device).float())\n",
        "        if i==0:\n",
        "          yhat_5=output\n",
        "          test_target=target_future\n",
        "\n",
        "        yhat_5=torch.cat((yhat_5,output),dim=0)\n",
        "        test_target=torch.cat((test_target,target_future),dim=0)\n",
        "\n",
        "        # clear memory\n",
        "        del data, target_future,output\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "\n",
        "yhat_4 = yhat_5.detach().cpu().numpy()\n",
        "test_target = test_target.detach().cpu().numpy()\n",
        "print(yhat_4.shape)\n",
        "\n",
        "rmse, p=RMSE_prediction(yhat_4,test_target)\n",
        "\n",
        "ablation_1=np.hstack([rmse,p])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. GRU"
      ],
      "metadata": {
        "id": "xAYgwE-G4pwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.lstm_1 = nn.GRU(input_dim, 128, bidirectional=False, batch_first=True, dropout=0.0)\n",
        "        self.lstm_2 = nn.GRU(128, 64, bidirectional=False, batch_first=True, dropout=0.0)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_1, _ = self.lstm_1(x)\n",
        "        out_1=self.dropout(out_1)\n",
        "        out_2, h_n = self.lstm_2(out_1)\n",
        "        out_2=self.dropout(out_2)\n",
        "\n",
        "        return out_2, h_n"
      ],
      "metadata": {
        "id": "R02T9REs4pwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHnDcEJl4pwH"
      },
      "outputs": [],
      "source": [
        "class MM_GRU(nn.Module):\n",
        "    def __init__(self, input_acc, input_gyr,input_hof, drop_prob=0.0):\n",
        "        super(MM_GRU, self).__init__()\n",
        "\n",
        "        self.encoder_acc=Encoder(input_acc, drop_prob)\n",
        "        self.encoder_gyr=Encoder(input_gyr, drop_prob)\n",
        "        self.encoder_hof=Encoder(input_hof, drop_prob)\n",
        "\n",
        "        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)\n",
        "        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)\n",
        "        self.BN_hof= nn.BatchNorm1d(input_hof, affine=False)\n",
        "\n",
        "        self.fc = nn.Linear(3*64, 6*9)\n",
        "\n",
        "        self.flatten=nn.Flatten()\n",
        "\n",
        "    def forward(self, x_acc, x_gyr, x_hof):\n",
        "\n",
        "        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))\n",
        "        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))\n",
        "        x_hof_1=x_hof.view(x_hof.size(0)*x_hof.size(1),x_hof.size(-1))\n",
        "\n",
        "        x_acc_1=self.BN_acc(x_acc_1)\n",
        "        x_gyr_1=self.BN_gyr(x_gyr_1)\n",
        "        x_hof_1=self.BN_hof(x_hof_1)\n",
        "\n",
        "        x_acc_2=x_acc_1.view(-1, window, x_acc_1.size(-1))\n",
        "        x_gyr_2=x_gyr_1.view(-1, window, x_gyr_1.size(-1))\n",
        "        x_hof_2=x_hof_1.view(-1, window_hof, x_hof_1.size(-1))\n",
        "\n",
        "        x_acc,h_acc=self.encoder_acc(x_acc_2)\n",
        "        x_gyr,h_gyr=self.encoder_gyr(x_gyr_2)\n",
        "        x_hof,h_hof=self.encoder_hof(x_hof_2)\n",
        "\n",
        "        x=torch.cat((x_acc[:,-1,:],x_gyr[:,-1,:],x_hof[:,-1,:]),dim=-1)\n",
        "\n",
        "        out=self.fc(x).view(x.shape[0],9,6)\n",
        "\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdcad166-0f25-4f5b-c263-61368425597c",
        "id": "NHatxCuB4pwH"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, time: 18.7374, Training Loss: 8.3639,  Validation loss: 4.9778\n",
            "Epoch: 2, time: 16.8893, Training Loss: 4.4554,  Validation loss: 3.7790\n",
            "Epoch: 3, time: 17.1380, Training Loss: 3.7112,  Validation loss: 3.3356\n",
            "Epoch: 4, time: 16.8431, Training Loss: 3.3311,  Validation loss: 3.1439\n",
            "Epoch: 5, time: 16.9289, Training Loss: 3.1322,  Validation loss: 2.9025\n",
            "Epoch: 6, time: 16.9657, Training Loss: 2.9179,  Validation loss: 2.8187\n",
            "Epoch: 7, time: 17.0324, Training Loss: 2.7529,  Validation loss: 2.5950\n",
            "Epoch: 8, time: 17.0199, Training Loss: 2.6363,  Validation loss: 2.4863\n",
            "Epoch: 9, time: 16.7831, Training Loss: 2.5139,  Validation loss: 2.4581\n",
            "Epoch: 10, time: 16.9310, Training Loss: 2.4417,  Validation loss: 2.3836\n",
            "Epoch: 11, time: 17.2408, Training Loss: 2.3688,  Validation loss: 2.3028\n",
            "Epoch: 12, time: 16.8381, Training Loss: 2.3066,  Validation loss: 2.3054\n",
            "Epoch: 13, time: 17.0488, Training Loss: 2.2553,  Validation loss: 2.2739\n",
            "Epoch: 14, time: 17.1343, Training Loss: 2.2177,  Validation loss: 2.2346\n",
            "Epoch: 15, time: 16.8232, Training Loss: 2.1833,  Validation loss: 2.1895\n",
            "Training time: 256.4294638633728 seconds\n"
          ]
        }
      ],
      "source": [
        "lr = 0.001\n",
        "model = MM_GRU(24,24,36)\n",
        "\n",
        "mm_gru = train_mm_m(train_loader, lr,15,model,path+subject+'_mm_gru_IMU8_hof.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a559cc79-c42f-43c0-9cfc-ca0e0c85b7a1",
        "id": "yKSy3-1h4pwH"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4bd47e4-087d-4dfb-dbef-c760a53bb48a",
        "id": "OBWPrEaM4pwI"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(18718, 9, 6)\n",
            "4.6317863\n",
            "5.864687\n",
            "4.144042\n",
            "3.539304\n",
            "4.5084457\n",
            "3.828418\n",
            "\n",
            "\n",
            "0.9759180886013017\n",
            "0.9769881241317538\n",
            "0.9254356590385756\n",
            "0.9792478858279693\n",
            "0.984461993892357\n",
            "0.9328141801648295\n",
            "Mean: 4.419 +/- 0.666\n",
            "Mean: 0.962 +/- 0.029\n"
          ]
        }
      ],
      "source": [
        "mm_gru= MM_GRU(24,24,36)\n",
        "mm_gru.load_state_dict(torch.load(path+subject+'_mm_gru_IMU8_hof.pth'))\n",
        "mm_gru.to(device)\n",
        "\n",
        "mm_gru.eval()\n",
        "\n",
        "# iterate through batches of test data\n",
        "with torch.no_grad():\n",
        "    for i, (data, data_acc, data_gyr, data_hof, target_present, target_future) in enumerate(test_loader):\n",
        "        output = mm_gru(data_acc.to(device).float(),data_gyr.to(device).float(),data_hof.to(device).float())\n",
        "        if i==0:\n",
        "          yhat_5=output\n",
        "          test_target=target_future\n",
        "\n",
        "        yhat_5=torch.cat((yhat_5,output),dim=0)\n",
        "        test_target=torch.cat((test_target,target_future),dim=0)\n",
        "\n",
        "        # clear memory\n",
        "        del data, target_future,output\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "\n",
        "yhat_4 = yhat_5.detach().cpu().numpy()\n",
        "test_target = test_target.detach().cpu().numpy()\n",
        "print(yhat_4.shape)\n",
        "\n",
        "rmse, p=RMSE_prediction(yhat_4,test_target)\n",
        "\n",
        "ablation_2=np.hstack([rmse,p])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Bi-LSTM"
      ],
      "metadata": {
        "id": "uiMMbyo1ygzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.lstm_1 = nn.LSTM(input_dim, 128, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.lstm_2 = nn.LSTM(256, 64, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_1, _ = self.lstm_1(x)\n",
        "        out_1=self.dropout(out_1)\n",
        "        out_2, (h_n, c_n) = self.lstm_2(out_1)\n",
        "        out_2=self.dropout(out_2)\n",
        "\n",
        "        return out_2, (h_n, c_n)"
      ],
      "metadata": {
        "id": "ytYuQKGdygzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOpZ_XeNygzh"
      },
      "outputs": [],
      "source": [
        "class MM_Bi_LSTM(nn.Module):\n",
        "    def __init__(self, input_acc, input_gyr,input_hof, drop_prob=0.0):\n",
        "        super(MM_Bi_LSTM, self).__init__()\n",
        "\n",
        "        self.encoder_acc=Encoder(input_acc, drop_prob)\n",
        "        self.encoder_gyr=Encoder(input_gyr, drop_prob)\n",
        "        self.encoder_hof=Encoder(input_hof, drop_prob)\n",
        "\n",
        "        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)\n",
        "        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)\n",
        "        self.BN_hof= nn.BatchNorm1d(input_hof, affine=False)\n",
        "\n",
        "        self.fc = nn.Linear(3*128, 6*9)\n",
        "\n",
        "        self.flatten=nn.Flatten()\n",
        "\n",
        "    def forward(self, x_acc, x_gyr, x_hof):\n",
        "\n",
        "        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))\n",
        "        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))\n",
        "        x_hof_1=x_hof.view(x_hof.size(0)*x_hof.size(1),x_hof.size(-1))\n",
        "\n",
        "        x_acc_1=self.BN_acc(x_acc_1)\n",
        "        x_gyr_1=self.BN_gyr(x_gyr_1)\n",
        "        x_hof_1=self.BN_hof(x_hof_1)\n",
        "\n",
        "        x_acc_2=x_acc_1.view(-1, window, x_acc_1.size(-1))\n",
        "        x_gyr_2=x_gyr_1.view(-1, window, x_gyr_1.size(-1))\n",
        "        x_hof_2=x_hof_1.view(-1, window_hof, x_hof_1.size(-1))\n",
        "\n",
        "        x_acc,(h_acc,c_acc)=self.encoder_acc(x_acc_2)\n",
        "        x_gyr,(h_gyr,c_gyr)=self.encoder_gyr(x_gyr_2)\n",
        "        x_hof,(h_hof,c_hof)=self.encoder_hof(x_hof_2)\n",
        "\n",
        "        x=torch.cat((x_acc[:,-1,:],x_gyr[:,-1,:],x_hof[:,-1,:]),dim=-1)\n",
        "\n",
        "        out=self.fc(x).view(x.shape[0],9,6)\n",
        "\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab4e5346-1cbd-4038-8041-511b048ad9d2",
        "id": "WQmlgCdTygzh"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, time: 30.9840, Training Loss: 6.9727,  Validation loss: 3.6831\n",
            "Epoch: 2, time: 29.0591, Training Loss: 3.3372,  Validation loss: 2.8154\n",
            "Epoch: 3, time: 28.9778, Training Loss: 2.7889,  Validation loss: 2.5197\n",
            "Epoch: 4, time: 29.1942, Training Loss: 2.5271,  Validation loss: 2.3490\n",
            "Epoch: 5, time: 29.1298, Training Loss: 2.3556,  Validation loss: 2.2454\n",
            "Epoch: 6, time: 29.0324, Training Loss: 2.2375,  Validation loss: 2.1352\n",
            "Epoch: 7, time: 29.0631, Training Loss: 2.1414,  Validation loss: 2.0826\n",
            "Epoch: 8, time: 29.1694, Training Loss: 2.0599,  Validation loss: 2.0261\n",
            "Epoch: 9, time: 28.9813, Training Loss: 1.9978,  Validation loss: 1.9885\n",
            "Epoch: 10, time: 29.1162, Training Loss: 1.9461,  Validation loss: 1.9516\n",
            "Epoch: 11, time: 29.0330, Training Loss: 1.8999,  Validation loss: 1.9694\n",
            "Epoch: 12, time: 29.1151, Training Loss: 1.8601,  Validation loss: 1.8926\n",
            "Training time: 350.9727168083191 seconds\n"
          ]
        }
      ],
      "source": [
        "lr = 0.001\n",
        "model = MM_Bi_LSTM(24,24,36)\n",
        "\n",
        "mm_bi_lstm = train_mm_m(train_loader, lr,12,model,path+subject+'_mm_bi_lstm_IMU8_hof.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b19bf12d-9a60-41f4-bdcf-9663905c3802",
        "id": "bxXZBTXbygzh"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0832c9a9-c41f-426a-9d3f-ba15ba02f509",
        "id": "ubWHUzh_ygzh"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(18718, 9, 6)\n",
            "4.8443847\n",
            "5.9810424\n",
            "3.5630105\n",
            "3.0108778\n",
            "4.21733\n",
            "3.633172\n",
            "\n",
            "\n",
            "0.977920207924719\n",
            "0.9782849351717356\n",
            "0.9465890751991459\n",
            "0.9844866164915999\n",
            "0.9852919148245437\n",
            "0.9418887430110484\n",
            "Mean: 4.208 +/- 0.789\n",
            "Mean: 0.969 +/- 0.022\n"
          ]
        }
      ],
      "source": [
        "mm_bi_lstm= MM_Bi_LSTM(24,24,36)\n",
        "mm_bi_lstm.load_state_dict(torch.load(path+subject+'_mm_bi_lstm_IMU8_hof.pth'))\n",
        "mm_bi_lstm.to(device)\n",
        "\n",
        "mm_bi_lstm.eval()\n",
        "\n",
        "# iterate through batches of test data\n",
        "with torch.no_grad():\n",
        "    for i, (data, data_acc, data_gyr, data_hof, target_present, target_future) in enumerate(test_loader):\n",
        "        output = mm_bi_lstm(data_acc.to(device).float(),data_gyr.to(device).float(),data_hof.to(device).float())\n",
        "        if i==0:\n",
        "          yhat_5=output\n",
        "          test_target=target_future\n",
        "\n",
        "        yhat_5=torch.cat((yhat_5,output),dim=0)\n",
        "        test_target=torch.cat((test_target,target_future),dim=0)\n",
        "\n",
        "        # clear memory\n",
        "        del data, target_future,output\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "\n",
        "yhat_4 = yhat_5.detach().cpu().numpy()\n",
        "test_target = test_target.detach().cpu().numpy()\n",
        "print(yhat_4.shape)\n",
        "\n",
        "rmse, p=RMSE_prediction(yhat_4,test_target)\n",
        "\n",
        "ablation_3=np.hstack([rmse,p])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Bi-GRU"
      ],
      "metadata": {
        "id": "iYZarb34rns7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.lstm_1 = nn.GRU(input_dim, 128, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.lstm_2 = nn.GRU(256, 64, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_1, _ = self.lstm_1(x)\n",
        "        out_1=self.dropout(out_1)\n",
        "        out_2, h_n = self.lstm_2(out_1)\n",
        "        out_2=self.dropout(out_2)\n",
        "\n",
        "        return out_2, h_n"
      ],
      "metadata": {
        "id": "aOMl72hIrntA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMabUqaCrntA"
      },
      "outputs": [],
      "source": [
        "class MM_Bi_GRU(nn.Module):\n",
        "    def __init__(self, input_acc, input_gyr,input_hof, drop_prob=0.0):\n",
        "        super(MM_Bi_GRU, self).__init__()\n",
        "\n",
        "        self.encoder_acc=Encoder(input_acc, drop_prob)\n",
        "        self.encoder_gyr=Encoder(input_gyr, drop_prob)\n",
        "        self.encoder_hof=Encoder(input_hof, drop_prob)\n",
        "\n",
        "        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)\n",
        "        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)\n",
        "        self.BN_hof= nn.BatchNorm1d(input_hof, affine=False)\n",
        "\n",
        "        self.fc = nn.Linear(3*128, 6*9)\n",
        "\n",
        "        self.flatten=nn.Flatten()\n",
        "\n",
        "    def forward(self, x_acc, x_gyr, x_hof):\n",
        "\n",
        "        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))\n",
        "        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))\n",
        "        x_hof_1=x_hof.view(x_hof.size(0)*x_hof.size(1),x_hof.size(-1))\n",
        "\n",
        "        x_acc_1=self.BN_acc(x_acc_1)\n",
        "        x_gyr_1=self.BN_gyr(x_gyr_1)\n",
        "        x_hof_1=self.BN_hof(x_hof_1)\n",
        "\n",
        "        x_acc_2=x_acc_1.view(-1, window, x_acc_1.size(-1))\n",
        "        x_gyr_2=x_gyr_1.view(-1, window, x_gyr_1.size(-1))\n",
        "        x_hof_2=x_hof_1.view(-1, window_hof, x_hof_1.size(-1))\n",
        "\n",
        "        x_acc,h_acc=self.encoder_acc(x_acc_2)\n",
        "        x_gyr,h_gyr=self.encoder_gyr(x_gyr_2)\n",
        "        x_hof,h_hof=self.encoder_hof(x_hof_2)\n",
        "\n",
        "        x=torch.cat((x_acc[:,-1,:],x_gyr[:,-1,:],x_hof[:,-1,:]),dim=-1)\n",
        "\n",
        "        out=self.fc(x).view(x.shape[0],9,6)\n",
        "\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08f34f26-8482-4936-ff5a-f8b40dae185c",
        "id": "qHaDaa02rntB"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, time: 23.9924, Training Loss: 5.6788,  Validation loss: 3.1912\n",
            "Epoch: 2, time: 22.5060, Training Loss: 2.9226,  Validation loss: 2.5430\n",
            "Epoch: 3, time: 22.6864, Training Loss: 2.5792,  Validation loss: 2.3597\n",
            "Epoch: 4, time: 22.7562, Training Loss: 2.3757,  Validation loss: 2.2116\n",
            "Epoch: 5, time: 22.6982, Training Loss: 2.2557,  Validation loss: 2.1460\n",
            "Epoch: 6, time: 22.6695, Training Loss: 2.1648,  Validation loss: 2.0737\n",
            "Epoch: 7, time: 22.5604, Training Loss: 2.0849,  Validation loss: 2.1021\n",
            "Epoch: 8, time: 22.5847, Training Loss: 2.0276,  Validation loss: 2.0514\n",
            "Epoch: 9, time: 22.8611, Training Loss: 1.9762,  Validation loss: 1.9691\n",
            "Epoch: 10, time: 22.7899, Training Loss: 1.9327,  Validation loss: 1.9398\n",
            "Training time: 228.18454957008362 seconds\n"
          ]
        }
      ],
      "source": [
        "lr = 0.001\n",
        "model = MM_Bi_GRU(24,24,36)\n",
        "\n",
        "mm_bi_gru = train_mm_m(train_loader, lr,10,model,path+subject+'_mm_bi_gru_IMU8_hof.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87d16482-dbaf-4656-8a4c-c3244ba997a5",
        "id": "IcolF1QnrntB"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29c9a889-6780-4ccd-b1f1-e270438fb88e",
        "id": "YPnStdNmrntB"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(18718, 9, 6)\n",
            "4.642194\n",
            "5.6626153\n",
            "3.3564138\n",
            "3.097201\n",
            "4.1566443\n",
            "3.3383148\n",
            "\n",
            "\n",
            "0.9778621035714846\n",
            "0.9801487658817363\n",
            "0.949088799411783\n",
            "0.9837706335131909\n",
            "0.9853157837263249\n",
            "0.9496255315652773\n",
            "Mean: 4.042 +/- 0.795\n",
            "Mean: 0.971 +/- 0.019\n"
          ]
        }
      ],
      "source": [
        "mm_bi_gru= MM_Bi_GRU(24,24,36)\n",
        "mm_bi_gru.load_state_dict(torch.load(path+subject+'_mm_bi_gru_IMU8_hof.pth'))\n",
        "mm_bi_gru.to(device)\n",
        "\n",
        "mm_bi_gru.eval()\n",
        "\n",
        "# iterate through batches of test data\n",
        "with torch.no_grad():\n",
        "    for i, (data, data_acc, data_gyr, data_hof, target_present, target_future) in enumerate(test_loader):\n",
        "        output = mm_bi_gru(data_acc.to(device).float(),data_gyr.to(device).float(),data_hof.to(device).float())\n",
        "        if i==0:\n",
        "          yhat_5=output\n",
        "          test_target=target_future\n",
        "\n",
        "        yhat_5=torch.cat((yhat_5,output),dim=0)\n",
        "        test_target=torch.cat((test_target,target_future),dim=0)\n",
        "\n",
        "        # clear memory\n",
        "        del data, target_future,output\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "\n",
        "yhat_4 = yhat_5.detach().cpu().numpy()\n",
        "test_target = test_target.detach().cpu().numpy()\n",
        "print(yhat_4.shape)\n",
        "\n",
        "rmse, p=RMSE_prediction(yhat_4,test_target)\n",
        "\n",
        "ablation_4=np.hstack([rmse,p])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGMs0-M9kEHS"
      },
      "source": [
        "## 5. LSTM --Encoder+decoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.lstm_1 = nn.LSTM(input_dim, 128, bidirectional=False, batch_first=True, dropout=0.0)\n",
        "        self.lstm_2 = nn.LSTM(128, 64, bidirectional=False, batch_first=True, dropout=0.0)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_1, _ = self.lstm_1(x)\n",
        "        out_1=self.dropout(out_1)\n",
        "        out_2, (h_n, c_n) = self.lstm_2(out_1)\n",
        "        out_2=self.dropout(out_2)\n",
        "\n",
        "        return out_2, (h_n, c_n)"
      ],
      "metadata": {
        "id": "zwbt1uoXkEHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMDecoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout):\n",
        "        super(LSTMDecoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, bidirectional=False, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, encoder_hidden, encoder_cell,  max_len):\n",
        "        batch_size = encoder_hidden.shape[1]\n",
        "        hidden = encoder_hidden\n",
        "        cell = encoder_cell\n",
        "        outputs = []\n",
        "\n",
        "        # Use the last time step of target as the initial input\n",
        "        input = torch.zeros(batch_size,1,6).to(device)\n",
        "\n",
        "\n",
        "        for i in range(max_len):\n",
        "            # Run one time step of LSTM\n",
        "            output, (hidden, cell) = self.lstm(input, (hidden, cell))\n",
        "            output=self.dropout(output)\n",
        "\n",
        "            # Use the output for prediction\n",
        "            output = self.fc(output.squeeze(1))\n",
        "            outputs.append(output.unsqueeze(1))\n",
        "\n",
        "            # Use the predicted output as the next input\n",
        "            input = output.unsqueeze(1)\n",
        "\n",
        "\n",
        "        # Concatenate all the outputs along the time dimension\n",
        "        outputs = torch.cat(outputs, dim=1)\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "tccintVxkEHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnqXxB-zkEHa"
      },
      "outputs": [],
      "source": [
        "class MM_ED_LSTM(nn.Module):\n",
        "    def __init__(self, input_acc, input_gyr,input_hof, drop_prob=0.0):\n",
        "        super(MM_ED_LSTM, self).__init__()\n",
        "\n",
        "        self.encoder_acc=Encoder(input_acc, drop_prob)\n",
        "        self.encoder_gyr=Encoder(input_gyr, drop_prob)\n",
        "        self.encoder_hof=Encoder(input_hof, drop_prob)\n",
        "\n",
        "        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)\n",
        "        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)\n",
        "        self.BN_hof= nn.BatchNorm1d(input_hof, affine=False)\n",
        "\n",
        "        self.decoder=LSTMDecoder(6, 3*64, 6, 1, 0.0)\n",
        "\n",
        "    def forward(self, x_acc, x_gyr, x_hof):\n",
        "\n",
        "        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))\n",
        "        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))\n",
        "        x_hof_1=x_hof.view(x_hof.size(0)*x_hof.size(1),x_hof.size(-1))\n",
        "\n",
        "        x_acc_1=self.BN_acc(x_acc_1)\n",
        "        x_gyr_1=self.BN_gyr(x_gyr_1)\n",
        "        x_hof_1=self.BN_hof(x_hof_1)\n",
        "\n",
        "        x_acc_2=x_acc_1.view(-1, window, x_acc_1.size(-1))\n",
        "        x_gyr_2=x_gyr_1.view(-1, window, x_gyr_1.size(-1))\n",
        "        x_hof_2=x_hof_1.view(-1, window_hof, x_hof_1.size(-1))\n",
        "\n",
        "        x_acc,(h_acc,c_acc)=self.encoder_acc(x_acc_2)\n",
        "        x_gyr,(h_gyr,c_gyr)=self.encoder_gyr(x_gyr_2)\n",
        "        x_hof,(h_hof,c_hof)=self.encoder_hof(x_hof_2)\n",
        "\n",
        "\n",
        "        h=torch.cat((h_acc,h_gyr,h_hof),dim=-1)\n",
        "        c=torch.cat((c_acc,c_gyr,c_hof),dim=-1)\n",
        "\n",
        "        out=self.decoder(h, c, 9)\n",
        "\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40c349a3-2853-4c33-b7b9-1e51ffa9314a",
        "id": "oB7-l4QwkEHa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, time: 41.6636, Training Loss: 7.6758,  Validation loss: 3.5887\n",
            "Epoch: 2, time: 40.8271, Training Loss: 3.3004,  Validation loss: 2.8860\n",
            "Epoch: 3, time: 41.1452, Training Loss: 2.8077,  Validation loss: 2.5557\n",
            "Epoch: 4, time: 40.8174, Training Loss: 2.5739,  Validation loss: 2.3965\n",
            "Epoch: 5, time: 40.5989, Training Loss: 2.4142,  Validation loss: 2.2856\n",
            "Epoch: 6, time: 40.6483, Training Loss: 2.3010,  Validation loss: 2.2106\n",
            "Epoch: 7, time: 40.5187, Training Loss: 2.2085,  Validation loss: 2.1343\n",
            "Epoch: 8, time: 40.6856, Training Loss: 2.1447,  Validation loss: 2.1172\n",
            "Epoch: 9, time: 40.4555, Training Loss: 2.0847,  Validation loss: 2.0177\n",
            "Epoch: 10, time: 40.4180, Training Loss: 2.0307,  Validation loss: 2.0378\n",
            "Training time: 407.8421688079834 seconds\n"
          ]
        }
      ],
      "source": [
        "lr = 0.001\n",
        "model = MM_ED_LSTM(24,24,36)\n",
        "\n",
        "mm_ed_lstm = train_mm_m(train_loader, lr,10,model,path+subject+'_mm_ed_lstm_IMU8_hof.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c86e56f-5811-4d26-d14b-4bd19ef59f14",
        "id": "j4v_EkCikEHa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e3cebaa-8afc-4676-97d0-8add47e3992a",
        "id": "oOVlEm6vkEHb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(18718, 9, 6)\n",
            "5.1162634\n",
            "6.459582\n",
            "3.8278992\n",
            "3.3230064\n",
            "4.372437\n",
            "3.6004786\n",
            "\n",
            "\n",
            "0.9758939066494967\n",
            "0.9770489417703624\n",
            "0.9420461373440077\n",
            "0.9826160915265246\n",
            "0.9846254291212083\n",
            "0.9446652131553908\n",
            "Mean: 4.450 +/- 0.874\n",
            "Mean: 0.968 +/- 0.021\n"
          ]
        }
      ],
      "source": [
        "mm_ed_lstm= MM_ED_LSTM(24,24,36)\n",
        "mm_ed_lstm.load_state_dict(torch.load(path+subject+'_mm_ed_lstm_IMU8_hof.pth'))\n",
        "mm_ed_lstm.to(device)\n",
        "\n",
        "mm_ed_lstm.eval()\n",
        "\n",
        "# iterate through batches of test data\n",
        "with torch.no_grad():\n",
        "    for i, (data, data_acc, data_gyr, data_hof, target_present, target_future) in enumerate(test_loader):\n",
        "        output = mm_ed_lstm(data_acc.to(device).float(),data_gyr.to(device).float(),data_hof.to(device).float())\n",
        "        if i==0:\n",
        "          yhat_5=output\n",
        "          test_target=target_future\n",
        "\n",
        "        yhat_5=torch.cat((yhat_5,output),dim=0)\n",
        "        test_target=torch.cat((test_target,target_future),dim=0)\n",
        "\n",
        "        # clear memory\n",
        "        del data, target_future,output\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "\n",
        "yhat_4 = yhat_5.detach().cpu().numpy()\n",
        "test_target = test_target.detach().cpu().numpy()\n",
        "print(yhat_4.shape)\n",
        "\n",
        "rmse, p=RMSE_prediction(yhat_4,test_target)\n",
        "\n",
        "ablation_5=np.hstack([rmse,p])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVK0-v61kEHb"
      },
      "source": [
        "## 6. GRU --Encoder+decoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.lstm_1 = nn.GRU(input_dim, 128, bidirectional=False, batch_first=True, dropout=0.0)\n",
        "        self.lstm_2 = nn.GRU(128, 64, bidirectional=False, batch_first=True, dropout=0.0)\n",
        "        self.flatten=nn.Flatten()\n",
        "        self.fc = nn.Linear(128, 32)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_1, _ = self.lstm_1(x)\n",
        "        out_1=self.dropout(out_1)\n",
        "        out_2, h_n = self.lstm_2(out_1)\n",
        "        out_2=self.dropout(out_2)\n",
        "\n",
        "        return out_2, h_n"
      ],
      "metadata": {
        "id": "JduVa-8xkEHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMDecoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers,dropout):\n",
        "        super(LSTMDecoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.GRU(input_size, hidden_size, num_layers, bidirectional=False, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, encoder_hidden, max_len):\n",
        "        batch_size = encoder_hidden.shape[1]\n",
        "        hidden = encoder_hidden\n",
        "        # cell = encoder_cell\n",
        "        outputs = []\n",
        "\n",
        "        # Use the last time step of target as the initial input\n",
        "\n",
        "        input = torch.zeros(batch_size,1,6).to(device)\n",
        "\n",
        "\n",
        "        for i in range(max_len):\n",
        "\n",
        "\n",
        "            # Run one time step of LSTM\n",
        "            output, hidden = self.lstm(input, hidden)\n",
        "            output=self.dropout(output)\n",
        "\n",
        "            # Use the output for prediction\n",
        "            output = self.fc(output.squeeze(1))\n",
        "            outputs.append(output.unsqueeze(1))\n",
        "\n",
        "            # Use the predicted output as the next input\n",
        "            input = output.unsqueeze(1)\n",
        "\n",
        "\n",
        "        # Concatenate all the outputs along the time dimension\n",
        "        outputs = torch.cat(outputs, dim=1)\n",
        "\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "t_Yhlk_okEHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1zmD8_AkEHb"
      },
      "outputs": [],
      "source": [
        "class MM_ED_GRU(nn.Module):\n",
        "    def __init__(self, input_acc, input_gyr,input_hof, drop_prob=0.0):\n",
        "        super(MM_ED_GRU, self).__init__()\n",
        "\n",
        "        self.encoder_acc=Encoder(input_acc, drop_prob)\n",
        "        self.encoder_gyr=Encoder(input_gyr, drop_prob)\n",
        "        self.encoder_hof=Encoder(input_hof, drop_prob)\n",
        "\n",
        "        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)\n",
        "        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)\n",
        "        self.BN_hof= nn.BatchNorm1d(input_hof, affine=False)\n",
        "\n",
        "        self.decoder=LSTMDecoder(6, 3*64, 6, 1, 0.0)\n",
        "\n",
        "    def forward(self, x_acc, x_gyr, x_hof):\n",
        "\n",
        "        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))\n",
        "        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))\n",
        "        x_hof_1=x_hof.view(x_hof.size(0)*x_hof.size(1),x_hof.size(-1))\n",
        "\n",
        "        x_acc_1=self.BN_acc(x_acc_1)\n",
        "        x_gyr_1=self.BN_gyr(x_gyr_1)\n",
        "        x_hof_1=self.BN_hof(x_hof_1)\n",
        "\n",
        "        x_acc_2=x_acc_1.view(-1, window, x_acc_1.size(-1))\n",
        "        x_gyr_2=x_gyr_1.view(-1, window, x_gyr_1.size(-1))\n",
        "        x_hof_2=x_hof_1.view(-1, window_hof, x_hof_1.size(-1))\n",
        "\n",
        "        x_acc, h_acc=self.encoder_acc(x_acc_2)\n",
        "        x_gyr, h_gyr=self.encoder_gyr(x_gyr_2)\n",
        "        x_hof, h_hof=self.encoder_hof(x_hof_2)\n",
        "\n",
        "\n",
        "        h=torch.cat((h_acc,h_gyr,h_hof),dim=-1)\n",
        "\n",
        "        out=self.decoder(h,9)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bf6a20a-e3ec-4553-dd2b-8e44f6ff03b0",
        "id": "ZZmotX0EkEHc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, time: 35.0138, Training Loss: 6.7537,  Validation loss: 3.3549\n",
            "Epoch: 2, time: 34.9506, Training Loss: 3.1230,  Validation loss: 2.8159\n",
            "Epoch: 3, time: 34.2821, Training Loss: 2.7214,  Validation loss: 2.5213\n",
            "Epoch: 4, time: 34.1092, Training Loss: 2.5263,  Validation loss: 2.3910\n",
            "Epoch: 5, time: 34.2710, Training Loss: 2.3841,  Validation loss: 2.2959\n",
            "Epoch: 6, time: 34.1570, Training Loss: 2.2854,  Validation loss: 2.2062\n",
            "Epoch: 7, time: 34.6883, Training Loss: 2.2072,  Validation loss: 2.1498\n",
            "Epoch: 8, time: 35.5890, Training Loss: 2.1521,  Validation loss: 2.1328\n",
            "Epoch: 9, time: 34.8111, Training Loss: 2.0987,  Validation loss: 2.0723\n",
            "Epoch: 10, time: 34.0966, Training Loss: 2.0558,  Validation loss: 2.0892\n",
            "Training time: 346.02756094932556 seconds\n"
          ]
        }
      ],
      "source": [
        "lr = 0.001\n",
        "model = MM_ED_GRU(24,24,36)\n",
        "\n",
        "mm_ed_gru= train_mm_m(train_loader, lr,10,model,path+subject+'_mm_ed_gru_IMU8_hof.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e9d52e3-9fb8-4ec0-c8d9-563414539ffd",
        "id": "StQGODpWkEHc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdc99f95-a638-493a-8f31-d99d07188f20",
        "id": "izB61v07kEHc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(18718, 9, 6)\n",
            "5.2309847\n",
            "5.8615837\n",
            "3.5573494\n",
            "3.5058208\n",
            "4.462206\n",
            "3.539792\n",
            "\n",
            "\n",
            "0.97865514485718\n",
            "0.9781869934087258\n",
            "0.9406623147634415\n",
            "0.9817604472984334\n",
            "0.9831794091243103\n",
            "0.9420985174373916\n",
            "Mean: 4.360 +/- 0.807\n",
            "Mean: 0.967 +/- 0.023\n"
          ]
        }
      ],
      "source": [
        "mm_ed_gru= MM_ED_GRU(24,24,36)\n",
        "mm_ed_gru.load_state_dict(torch.load(path+subject+'_mm_ed_gru_IMU8_hof.pth'))\n",
        "mm_ed_gru.to(device)\n",
        "\n",
        "mm_ed_gru.eval()\n",
        "\n",
        "# iterate through batches of test data\n",
        "with torch.no_grad():\n",
        "    for i, (data, data_acc, data_gyr, data_hof, target_present, target_future) in enumerate(test_loader):\n",
        "        output = mm_ed_gru(data_acc.to(device).float(),data_gyr.to(device).float(),data_hof.to(device).float())\n",
        "        if i==0:\n",
        "          yhat_5=output\n",
        "          test_target=target_future\n",
        "\n",
        "        yhat_5=torch.cat((yhat_5,output),dim=0)\n",
        "        test_target=torch.cat((test_target,target_future),dim=0)\n",
        "\n",
        "        # clear memory\n",
        "        del data, target_future,output\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "\n",
        "yhat_4 = yhat_5.detach().cpu().numpy()\n",
        "test_target = test_target.detach().cpu().numpy()\n",
        "print(yhat_4.shape)\n",
        "\n",
        "rmse, p=RMSE_prediction(yhat_4,test_target)\n",
        "\n",
        "ablation_6=np.hstack([rmse,p])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7ZL1Z8aa_jC"
      },
      "source": [
        "## 7. Bi-LSTM --Encoder+decoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.lstm_1 = nn.LSTM(input_dim, 128, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.lstm_2 = nn.LSTM(256, 64, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_1, _ = self.lstm_1(x)\n",
        "        out_1=self.dropout(out_1)\n",
        "        out_2, (h_n, c_n) = self.lstm_2(out_1)\n",
        "        out_2=self.dropout(out_2)\n",
        "\n",
        "        return out_2, (h_n, c_n)"
      ],
      "metadata": {
        "id": "7jwGQZegOuRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMDecoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout):\n",
        "        super(LSTMDecoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, bidirectional=True, batch_first=True)\n",
        "        self.fc = nn.Linear(2*hidden_size, output_size)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, encoder_hidden, encoder_cell,  max_len):\n",
        "        batch_size = encoder_hidden.shape[1]\n",
        "        hidden = encoder_hidden\n",
        "        cell = encoder_cell\n",
        "        outputs = []\n",
        "\n",
        "        # Use the last time step of target as the initial input\n",
        "        input = torch.zeros(batch_size,1,6).to(device)\n",
        "\n",
        "\n",
        "        for i in range(max_len):\n",
        "            # Run one time step of LSTM\n",
        "            output, (hidden, cell) = self.lstm(input, (hidden, cell))\n",
        "            output=self.dropout(output)\n",
        "\n",
        "            # Use the output for prediction\n",
        "            output = self.fc(output.squeeze(1))\n",
        "            outputs.append(output.unsqueeze(1))\n",
        "\n",
        "            # Use the predicted output as the next input\n",
        "            input = output.unsqueeze(1)\n",
        "\n",
        "\n",
        "        # Concatenate all the outputs along the time dimension\n",
        "        outputs = torch.cat(outputs, dim=1)\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "2FJk-SYbPbw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGhiGzUHa_jC"
      },
      "outputs": [],
      "source": [
        "class MM_ED_Bi_LSTM(nn.Module):\n",
        "    def __init__(self, input_acc, input_gyr,input_hof, drop_prob=0.0):\n",
        "        super(MM_ED_Bi_LSTM, self).__init__()\n",
        "\n",
        "        self.encoder_acc=Encoder(input_acc, drop_prob)\n",
        "        self.encoder_gyr=Encoder(input_gyr, drop_prob)\n",
        "        self.encoder_hof=Encoder(input_hof, drop_prob)\n",
        "\n",
        "        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)\n",
        "        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)\n",
        "        self.BN_hof= nn.BatchNorm1d(input_hof, affine=False)\n",
        "\n",
        "        self.decoder=LSTMDecoder(6, 3*64, 6, 1, 0.05)\n",
        "\n",
        "    def forward(self, x_acc, x_gyr, x_hof):\n",
        "\n",
        "        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))\n",
        "        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))\n",
        "        x_hof_1=x_hof.view(x_hof.size(0)*x_hof.size(1),x_hof.size(-1))\n",
        "\n",
        "        x_acc_1=self.BN_acc(x_acc_1)\n",
        "        x_gyr_1=self.BN_gyr(x_gyr_1)\n",
        "        x_hof_1=self.BN_hof(x_hof_1)\n",
        "\n",
        "        x_acc_2=x_acc_1.view(-1, window, x_acc_1.size(-1))\n",
        "        x_gyr_2=x_gyr_1.view(-1, window, x_gyr_1.size(-1))\n",
        "        x_hof_2=x_hof_1.view(-1, window_hof, x_hof_1.size(-1))\n",
        "\n",
        "        x_acc,(h_acc,c_acc)=self.encoder_acc(x_acc_2)\n",
        "        x_gyr,(h_gyr,c_gyr)=self.encoder_gyr(x_gyr_2)\n",
        "        x_hof,(h_hof,c_hof)=self.encoder_hof(x_hof_2)\n",
        "\n",
        "        h=torch.cat((h_acc,h_gyr,h_hof),dim=-1)\n",
        "        c=torch.cat((c_acc,c_gyr,c_hof),dim=-1)\n",
        "\n",
        "        out=self.decoder(h, c, 9)\n",
        "\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UErpmSwYa_jC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da16c0dc-baec-4cb4-d70e-8b957a4f2333"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, time: 72.1871, Training Loss: 6.7307,  Validation loss: 3.4572\n",
            "Epoch: 2, time: 70.8813, Training Loss: 3.1513,  Validation loss: 2.7578\n",
            "Epoch: 3, time: 69.8256, Training Loss: 2.6978,  Validation loss: 2.4989\n",
            "Epoch: 4, time: 69.8679, Training Loss: 2.4633,  Validation loss: 2.3153\n",
            "Epoch: 5, time: 69.8595, Training Loss: 2.3188,  Validation loss: 2.1946\n",
            "Epoch: 6, time: 70.5558, Training Loss: 2.2007,  Validation loss: 2.1120\n",
            "Epoch: 7, time: 69.9600, Training Loss: 2.1058,  Validation loss: 2.0911\n",
            "Epoch: 8, time: 70.0477, Training Loss: 2.0249,  Validation loss: 2.0531\n",
            "Epoch: 9, time: 72.2298, Training Loss: 1.9696,  Validation loss: 2.1073\n",
            "Epoch: 10, time: 69.7110, Training Loss: 1.9150,  Validation loss: 1.9471\n",
            "Training time: 705.2566227912903 seconds\n"
          ]
        }
      ],
      "source": [
        "lr = 0.001\n",
        "model = MM_ED_Bi_LSTM(24,24,36)\n",
        "\n",
        "mm_ed_bi_lstm = train_mm_m(train_loader, lr,12,model,path+subject+'_mm_ed_bi_lstm_IMU8_hof.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmUpsOzDa_jC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "761f99cf-b337-484a-defa-91b414302029"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ],
      "source": [
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pip3LZWLa_jC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8367c8b-11b6-48c0-f618-eb3f9bcd8b05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(18718, 9, 6)\n",
            "4.806944\n",
            "6.133658\n",
            "4.131059\n",
            "3.3874657\n",
            "4.893384\n",
            "3.7000504\n",
            "\n",
            "\n",
            "0.9768196443922915\n",
            "0.9750246933300443\n",
            "0.9328980251432529\n",
            "0.9817475526061958\n",
            "0.9829083685154462\n",
            "0.9402483865487552\n",
            "Mean: 4.509 +/- 0.875\n",
            "Mean: 0.965 +/- 0.025\n"
          ]
        }
      ],
      "source": [
        "mm_ed_bi_lstm= MM_ED_Bi_LSTM(24,24,36)\n",
        "mm_ed_bi_lstm.load_state_dict(torch.load(path+subject+'_mm_ed_bi_lstm_IMU8_hof.pth'))\n",
        "mm_ed_bi_lstm.to(device)\n",
        "\n",
        "mm_ed_bi_lstm.eval()\n",
        "\n",
        "# iterate through batches of test data\n",
        "with torch.no_grad():\n",
        "    for i, (data, data_acc, data_gyr, data_hof, target_present, target_future) in enumerate(test_loader):\n",
        "        output = mm_ed_bi_lstm(data_acc.to(device).float(),data_gyr.to(device).float(),data_hof.to(device).float())\n",
        "        if i==0:\n",
        "          yhat_5=output\n",
        "          test_target=target_future\n",
        "\n",
        "        yhat_5=torch.cat((yhat_5,output),dim=0)\n",
        "        test_target=torch.cat((test_target,target_future),dim=0)\n",
        "\n",
        "        # clear memory\n",
        "        del data, target_future,output\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "\n",
        "yhat_4 = yhat_5.detach().cpu().numpy()\n",
        "test_target = test_target.detach().cpu().numpy()\n",
        "print(yhat_4.shape)\n",
        "\n",
        "rmse, p=RMSE_prediction(yhat_4,test_target)\n",
        "\n",
        "ablation_7=np.hstack([rmse,p])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNsrYRfheX_b"
      },
      "source": [
        "## 8. Bi-GRU --Encoder+decoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.lstm_1 = nn.GRU(input_dim, 128, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.lstm_2 = nn.GRU(256, 64, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.flatten=nn.Flatten()\n",
        "        self.fc = nn.Linear(128, 32)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_1, _ = self.lstm_1(x)\n",
        "        out_1=self.dropout(out_1)\n",
        "        out_2, h_n = self.lstm_2(out_1)\n",
        "        out_2=self.dropout(out_2)\n",
        "\n",
        "        return out_2, h_n"
      ],
      "metadata": {
        "id": "ckWdyyEMeX_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMDecoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers,dropout):\n",
        "        super(LSTMDecoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.GRU(input_size, hidden_size, num_layers, bidirectional=True, batch_first=True)\n",
        "        self.fc = nn.Linear(2*hidden_size, output_size)\n",
        "\n",
        "\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, encoder_hidden, max_len):\n",
        "        batch_size = encoder_hidden.shape[1]\n",
        "        hidden = encoder_hidden\n",
        "        # cell = encoder_cell\n",
        "        outputs = []\n",
        "\n",
        "        # Use the last time step of target as the initial input\n",
        "\n",
        "        input = torch.zeros(batch_size,1,6).to(device)\n",
        "\n",
        "        for i in range(max_len):\n",
        "\n",
        "            # Run one time step of LSTM\n",
        "            output, hidden = self.lstm(input, hidden)\n",
        "            output=self.dropout(output)\n",
        "\n",
        "            # Use the output for prediction\n",
        "            output = self.fc(output.squeeze(1))\n",
        "            outputs.append(output.unsqueeze(1))\n",
        "\n",
        "            # Use the predicted output as the next input\n",
        "            input = output.unsqueeze(1)\n",
        "\n",
        "\n",
        "        # Concatenate all the outputs along the time dimension\n",
        "        outputs = torch.cat(outputs, dim=1)\n",
        "\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "kXChMh8reX_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lj61xc5ueX_c"
      },
      "outputs": [],
      "source": [
        "class MM_ED_Bi_GRU(nn.Module):\n",
        "    def __init__(self, input_acc, input_gyr,input_hof, drop_prob=0.0):\n",
        "        super(MM_ED_Bi_GRU, self).__init__()\n",
        "\n",
        "        self.encoder_acc=Encoder(input_acc, drop_prob)\n",
        "        self.encoder_gyr=Encoder(input_gyr, drop_prob)\n",
        "        self.encoder_hof=Encoder(input_hof, drop_prob)\n",
        "\n",
        "        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)\n",
        "        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)\n",
        "        self.BN_hof= nn.BatchNorm1d(input_hof, affine=False)\n",
        "\n",
        "        self.decoder=LSTMDecoder(6, 3*64, 6, 1, 0.05)\n",
        "\n",
        "    def forward(self, x_acc, x_gyr, x_hof):\n",
        "\n",
        "        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))\n",
        "        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))\n",
        "        x_hof_1=x_hof.view(x_hof.size(0)*x_hof.size(1),x_hof.size(-1))\n",
        "\n",
        "        x_acc_1=self.BN_acc(x_acc_1)\n",
        "        x_gyr_1=self.BN_gyr(x_gyr_1)\n",
        "        x_hof_1=self.BN_hof(x_hof_1)\n",
        "\n",
        "        x_acc_2=x_acc_1.view(-1, window, x_acc_1.size(-1))\n",
        "        x_gyr_2=x_gyr_1.view(-1, window, x_gyr_1.size(-1))\n",
        "        x_hof_2=x_hof_1.view(-1, window_hof, x_hof_1.size(-1))\n",
        "\n",
        "        x_acc, h_acc=self.encoder_acc(x_acc_2)\n",
        "        x_gyr, h_gyr=self.encoder_gyr(x_gyr_2)\n",
        "        x_hof, h_hof=self.encoder_hof(x_hof_2)\n",
        "\n",
        "\n",
        "        h=torch.cat((h_acc,h_gyr,h_hof),dim=-1)\n",
        "\n",
        "        out=self.decoder(h,9)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "noye4vzJeX_c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6e1cf6c-00a9-476f-e8ec-96da05b41974"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, time: 53.9054, Training Loss: 5.3444,  Validation loss: 2.9863\n",
            "Epoch: 2, time: 53.8258, Training Loss: 2.9820,  Validation loss: 2.5356\n",
            "Epoch: 3, time: 53.6109, Training Loss: 2.6613,  Validation loss: 2.3078\n",
            "Epoch: 4, time: 53.6693, Training Loss: 2.4830,  Validation loss: 2.2123\n",
            "Epoch: 5, time: 53.8039, Training Loss: 2.3602,  Validation loss: 2.1462\n",
            "Epoch: 6, time: 53.7538, Training Loss: 2.2677,  Validation loss: 2.0599\n",
            "Epoch: 7, time: 53.7611, Training Loss: 2.1954,  Validation loss: 1.9845\n",
            "Epoch: 8, time: 53.8659, Training Loss: 2.1355,  Validation loss: 1.9595\n",
            "Epoch: 9, time: 53.6809, Training Loss: 2.0834,  Validation loss: 1.9949\n",
            "Epoch: 10, time: 53.6727, Training Loss: 2.0407,  Validation loss: 1.9282\n",
            "Training time: 537.6453227996826 seconds\n"
          ]
        }
      ],
      "source": [
        "lr = 0.001\n",
        "model = MM_ED_Bi_GRU(24,24,36)\n",
        "\n",
        "mm_ed_Bi_GRU = train_mm_m(train_loader, lr,15,model,path+subject+'_mm_bi_ed_gru_IMU8_hof.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnXGwLADeX_c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc6a6082-7214-43ed-d050-f5ff03ee8131"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ],
      "source": [
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_78a0wCdeX_c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfeb571b-b191-42fa-f795-9502bb133b35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(18718, 9, 6)\n",
            "4.8143625\n",
            "5.9292045\n",
            "3.7431536\n",
            "3.2184105\n",
            "4.197531\n",
            "3.76761\n",
            "\n",
            "\n",
            "0.9776577985039994\n",
            "0.977868478832106\n",
            "0.9389786630757148\n",
            "0.9834864900307752\n",
            "0.9849636033939677\n",
            "0.9496530840642535\n",
            "Mean: 4.278 +/- 0.692\n",
            "Mean: 0.969 +/- 0.021\n"
          ]
        }
      ],
      "source": [
        "mm_ed_Bi_GRU= MM_ED_Bi_GRU(24,24,36)\n",
        "mm_ed_Bi_GRU.load_state_dict(torch.load(path+subject+'_mm_bi_ed_gru_IMU8_hof.pth'))\n",
        "mm_ed_Bi_GRU.to(device)\n",
        "\n",
        "mm_ed_Bi_GRU.eval()\n",
        "\n",
        "# iterate through batches of test data\n",
        "with torch.no_grad():\n",
        "    for i, (data, data_acc, data_gyr, data_hof, target_present, target_future) in enumerate(test_loader):\n",
        "        output = mm_ed_Bi_GRU(data_acc.to(device).float(),data_gyr.to(device).float(),data_hof.to(device).float())\n",
        "        if i==0:\n",
        "          yhat_5=output\n",
        "          test_target=target_future\n",
        "\n",
        "        yhat_5=torch.cat((yhat_5,output),dim=0)\n",
        "        test_target=torch.cat((test_target,target_future),dim=0)\n",
        "\n",
        "        # clear memory\n",
        "        del data, target_future,output\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "\n",
        "yhat_4 = yhat_5.detach().cpu().numpy()\n",
        "test_target = test_target.detach().cpu().numpy()\n",
        "print(yhat_4.shape)\n",
        "\n",
        "rmse, p=RMSE_prediction(yhat_4,test_target)\n",
        "\n",
        "ablation_8=np.hstack([rmse,p])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2N-DsNDQkR6P"
      },
      "source": [
        "## 9. Attention Without gating+ Bi-GRU --Encoder+decoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.lstm_1 = nn.GRU(input_dim, 128, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.lstm_2 = nn.GRU(256, 64, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.flatten=nn.Flatten()\n",
        "        self.fc = nn.Linear(128, 32)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_1, _ = self.lstm_1(x)\n",
        "        out_1=self.dropout(out_1)\n",
        "        out_2, h_n = self.lstm_2(out_1)\n",
        "        out_2=self.dropout(out_2)\n",
        "\n",
        "        return out_2, h_n"
      ],
      "metadata": {
        "id": "P2jNFJjCkR6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMDecoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers,dropout):\n",
        "        super(LSTMDecoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.GRU(input_size, hidden_size, num_layers, bidirectional=True, batch_first=True)\n",
        "        self.fc = nn.Linear(2*hidden_size, output_size)\n",
        "        self.gating_net = nn.Sequential(\n",
        "            nn.Linear(1542, 1542),\n",
        "            nn.Sigmoid())\n",
        "\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, encoder_hidden, h_att, max_len):\n",
        "        batch_size = encoder_hidden.shape[1]\n",
        "        hidden = encoder_hidden\n",
        "        # cell = encoder_cell\n",
        "        outputs = []\n",
        "\n",
        "        # Use the last time step of target as the initial input\n",
        "        # input = target_estimation[:, -1, :].unsqueeze(1)\n",
        "\n",
        "        input_1 = torch.zeros(batch_size,1,6).to(device)\n",
        "        input = torch.cat((input_1, h_att), dim=-1)\n",
        "\n",
        "        # print(input.shape)\n",
        "\n",
        "\n",
        "        for i in range(max_len):\n",
        "\n",
        "            # gating_weight=self.gating_net(input)\n",
        "            # input=input*gating_weight\n",
        "\n",
        "            # Run one time step of LSTM\n",
        "            output, hidden = self.lstm(input, hidden)\n",
        "            output=self.dropout(output)\n",
        "\n",
        "            # Use the output for prediction\n",
        "            output = self.fc(output.squeeze(1))\n",
        "            outputs.append(output.unsqueeze(1))\n",
        "\n",
        "\n",
        "            input = torch.cat((output.unsqueeze(1), input[:,:,6:390]), dim=-1)\n",
        "\n",
        "\n",
        "\n",
        "        # Concatenate all the outputs along the time dimension\n",
        "        outputs = torch.cat(outputs, dim=1)\n",
        "\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "IUQFfA14kR6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TemporalAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(TemporalAttention, self).__init__()\n",
        "\n",
        "        self.W = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, sequence_length, hidden_size)\n",
        "\n",
        "        # Calculate attention weights.\n",
        "        attn = self.V(torch.tanh(self.W(x)))\n",
        "        attn = attn.squeeze(-1)\n",
        "        attn = torch.softmax(attn, dim=1)\n",
        "\n",
        "        # Calculate weighted average of hidden states.\n",
        "        context = attn.unsqueeze(-1) * x\n",
        "        context = context.sum(dim=1)\n",
        "\n",
        "        return context"
      ],
      "metadata": {
        "id": "XybURM-ZkR6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9CQqI4JkR6i"
      },
      "outputs": [],
      "source": [
        "class MM_ED_Bi_GRU_attention_WFW(nn.Module):\n",
        "    def __init__(self, input_acc, input_gyr,input_hof, drop_prob=0.0):\n",
        "        super(MM_ED_Bi_GRU_attention_WFW, self).__init__()\n",
        "\n",
        "        self.encoder_acc=Encoder(input_acc, drop_prob)\n",
        "        self.encoder_gyr=Encoder(input_gyr, drop_prob)\n",
        "        self.encoder_hof=Encoder(input_hof, drop_prob)\n",
        "\n",
        "        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)\n",
        "        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)\n",
        "        self.BN_hof= nn.BatchNorm1d(input_hof, affine=False)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = nn.Linear(2*window*128+window_hof*128, 54)\n",
        "        # self.fc1 = nn.Linear(3*128, 54)\n",
        "\n",
        "        self.attention_acc=nn.MultiheadAttention(24,4,batch_first=True)\n",
        "        self.attention_gyr=nn.MultiheadAttention(24,4,batch_first=True)\n",
        "        self.attention_hof=nn.MultiheadAttention(36,4,batch_first=True)\n",
        "\n",
        "        self.temporal_attn_acc = TemporalAttention(128)\n",
        "        self.temporal_attn_gyr = TemporalAttention(128)\n",
        "        self.temporal_attn_hof = TemporalAttention(128)\n",
        "\n",
        "        self.decoder=LSTMDecoder(390, 3*64, 6, 1,0.05)\n",
        "\n",
        "    def forward(self, x_acc, x_gyr, x_hof):\n",
        "\n",
        "        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))\n",
        "        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))\n",
        "        x_hof_1=x_hof.view(x_hof.size(0)*x_hof.size(1),x_hof.size(-1))\n",
        "\n",
        "        x_acc_1=self.BN_acc(x_acc_1)\n",
        "        x_gyr_1=self.BN_gyr(x_gyr_1)\n",
        "        x_hof_1=self.BN_hof(x_hof_1)\n",
        "\n",
        "        x_acc_2=x_acc_1.view(-1, window, x_acc_1.size(-1))\n",
        "        x_gyr_2=x_gyr_1.view(-1, window, x_gyr_1.size(-1))\n",
        "        x_hof_2=x_hof_1.view(-1, window_hof, x_hof_1.size(-1))\n",
        "\n",
        "        x_acc, h_acc=self.encoder_acc(x_acc_2)\n",
        "        x_gyr, h_gyr=self.encoder_gyr(x_gyr_2)\n",
        "        x_hof, h_hof=self.encoder_hof(x_hof_2)\n",
        "\n",
        "        x_hof=x_hof.transpose(1,2)\n",
        "        x_hof= F.interpolate(x_hof, size=(45))\n",
        "        x_hof=x_hof.transpose(1,2)\n",
        "\n",
        "        x=torch.cat((x_acc,x_gyr,x_hof),dim=-1)\n",
        "        h=torch.cat((h_acc,h_gyr,h_hof),dim=-1)\n",
        "\n",
        "        h_acc_att=self.temporal_attn_acc(x_acc)\n",
        "        h_gyr_att=self.temporal_attn_gyr(x_gyr)\n",
        "        h_hof_att=self.temporal_attn_hof(x_hof)\n",
        "\n",
        "        h_att=torch.cat((h_acc_att,h_gyr_att,h_hof_att),dim=-1)\n",
        "\n",
        "        #Do it separately\n",
        "        h_att=h_att.unsqueeze(0)\n",
        "        h_att=h_att.transpose(1,0)\n",
        "        out=self.decoder(h,h_att, 9)\n",
        "\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb3ab3aa-4be7-48ca-a356-ef14ed43777d",
        "id": "SxQn8sMHkR6j"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, time: 66.6932, Training Loss: 5.0547,  Validation loss: 2.8943\n",
            "Epoch: 2, time: 63.6063, Training Loss: 2.8825,  Validation loss: 2.4506\n",
            "Epoch: 3, time: 63.2191, Training Loss: 2.5853,  Validation loss: 2.2443\n",
            "Epoch: 4, time: 62.6104, Training Loss: 2.4120,  Validation loss: 2.1291\n",
            "Epoch: 5, time: 62.4913, Training Loss: 2.3006,  Validation loss: 2.0425\n",
            "Epoch: 6, time: 62.2930, Training Loss: 2.2130,  Validation loss: 2.0062\n",
            "Epoch: 7, time: 62.6166, Training Loss: 2.1375,  Validation loss: 1.9266\n",
            "Epoch: 8, time: 62.2862, Training Loss: 2.0811,  Validation loss: 1.9272\n",
            "Epoch: 9, time: 62.3208, Training Loss: 2.0267,  Validation loss: 1.9560\n",
            "Epoch: 10, time: 62.1912, Training Loss: 1.9848,  Validation loss: 1.8779\n",
            "Epoch: 11, time: 61.8684, Training Loss: 1.9434,  Validation loss: 1.8577\n",
            "Epoch: 12, time: 62.0262, Training Loss: 1.9142,  Validation loss: 1.8199\n",
            "Epoch: 13, time: 63.7259, Training Loss: 1.8812,  Validation loss: 1.8169\n",
            "Epoch: 14, time: 62.0013, Training Loss: 1.8535,  Validation loss: 1.8294\n",
            "Epoch: 15, time: 62.1781, Training Loss: 1.8256,  Validation loss: 1.7972\n",
            "Training time: 942.6570234298706 seconds\n"
          ]
        }
      ],
      "source": [
        "lr = 0.001\n",
        "model= MM_ED_Bi_GRU_attention_WFW(24,24,36)\n",
        "\n",
        "mm_ed_bi_gru_attention_wfw = train_mm_m(train_loader, lr,15,model,path+subject+'_mm_ed_bi_gru_attention_wfw_IMU8_hof.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63cff1f4-529b-4601-f176-4a3334b2bb09",
        "id": "-C34BrGUkR6j"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 159
        }
      ],
      "source": [
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d4f17fd-c238-46a5-a7e6-79d7971fedd8",
        "id": "6yTDRS7ukR6k"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(18718, 9, 6)\n",
            "4.398792\n",
            "5.3281293\n",
            "3.7114031\n",
            "3.331985\n",
            "4.203511\n",
            "3.6220999\n",
            "\n",
            "\n",
            "0.9796230198533733\n",
            "0.9808556670699692\n",
            "0.9469112998136312\n",
            "0.9830127250904673\n",
            "0.9862692882535078\n",
            "0.9531164720804496\n",
            "Mean: 4.099 +/- 0.586\n",
            "Mean: 0.972 +/- 0.019\n"
          ]
        }
      ],
      "source": [
        "mm_ed_bi_gru_attention_wfw= MM_ED_Bi_GRU_attention_WFW(24,24,36)\n",
        "mm_ed_bi_gru_attention_wfw.load_state_dict(torch.load(path+subject+'_mm_ed_bi_gru_attention_wfw_IMU8_hof.pth'))\n",
        "mm_ed_bi_gru_attention_wfw.to(device)\n",
        "\n",
        "mm_ed_bi_gru_attention_wfw.eval()\n",
        "\n",
        "# iterate through batches of test data\n",
        "with torch.no_grad():\n",
        "    for i, (data, data_acc, data_gyr, data_hof, target_present, target_future) in enumerate(test_loader):\n",
        "        output = mm_ed_bi_gru_attention_wfw(data_acc.to(device).float(),data_gyr.to(device).float(),data_hof.to(device).float())\n",
        "        if i==0:\n",
        "          yhat_5=output\n",
        "          test_target=target_future\n",
        "\n",
        "        yhat_5=torch.cat((yhat_5,output),dim=0)\n",
        "        test_target=torch.cat((test_target,target_future),dim=0)\n",
        "\n",
        "        # clear memory\n",
        "        del data, target_future,output\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "\n",
        "yhat_4 = yhat_5.detach().cpu().numpy()\n",
        "test_target = test_target.detach().cpu().numpy()\n",
        "print(yhat_4.shape)\n",
        "\n",
        "rmse, p=RMSE_prediction(yhat_4,test_target)\n",
        "\n",
        "ablation_9=np.hstack([rmse,p])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CQeFCGAAD8p"
      },
      "source": [
        "## 10. Attention With gating+ Bi-GRU --Encoder+decoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.lstm_1 = nn.GRU(input_dim, 128, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.lstm_2 = nn.GRU(256, 64, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_1, _ = self.lstm_1(x)\n",
        "        out_1=self.dropout(out_1)\n",
        "        out_2, h_n = self.lstm_2(out_1)\n",
        "        out_2=self.dropout(out_2)\n",
        "\n",
        "        return out_2, h_n"
      ],
      "metadata": {
        "id": "C_xLDOG0AD8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMDecoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers,dropout):\n",
        "        super(LSTMDecoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.GRU(input_size, hidden_size, num_layers, bidirectional=True, batch_first=True)\n",
        "        self.fc = nn.Linear(2*hidden_size, output_size)\n",
        "        self.gating_net = nn.Sequential(\n",
        "            nn.Linear(390, 390),\n",
        "            nn.Sigmoid())\n",
        "\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, encoder_hidden, h_att, max_len):\n",
        "        batch_size = encoder_hidden.shape[1]\n",
        "        hidden = encoder_hidden\n",
        "        # cell = encoder_cell\n",
        "        outputs = []\n",
        "\n",
        "        # Use the last time step of target as the initial input\n",
        "\n",
        "        input_1 = torch.zeros(batch_size,1,6).to(device)\n",
        "        input = torch.cat((input_1, h_att), dim=-1)\n",
        "\n",
        "\n",
        "        for i in range(max_len):\n",
        "\n",
        "            gating_weight=self.gating_net(input)\n",
        "            input=input*gating_weight\n",
        "\n",
        "            # Run one time step of LSTM\n",
        "            output, hidden = self.lstm(input, hidden)\n",
        "            output=self.dropout(output)\n",
        "\n",
        "            # Use the output for prediction\n",
        "            output = self.fc(output.squeeze(1))\n",
        "            outputs.append(output.unsqueeze(1))\n",
        "\n",
        "\n",
        "            input = torch.cat((output.unsqueeze(1), input[:,:,6:390]), dim=-1)\n",
        "\n",
        "\n",
        "\n",
        "        # Concatenate all the outputs along the time dimension\n",
        "        outputs = torch.cat(outputs, dim=1)\n",
        "\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "KQ-vF5H4AD8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TemporalAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(TemporalAttention, self).__init__()\n",
        "\n",
        "        self.W = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, sequence_length, hidden_size)\n",
        "\n",
        "        # Calculate attention weights.\n",
        "        attn = self.V(torch.tanh(self.W(x)))\n",
        "        attn = attn.squeeze(-1)\n",
        "        attn = torch.softmax(attn, dim=1)\n",
        "\n",
        "        # Calculate weighted average of hidden states.\n",
        "        context = attn.unsqueeze(-1) * x\n",
        "        context = context.sum(dim=1)\n",
        "\n",
        "        return context"
      ],
      "metadata": {
        "id": "mEqtiwNiAD8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IPytOzeAD8w"
      },
      "outputs": [],
      "source": [
        "class MM_ED_Bi_GRU_attention_FW(nn.Module):\n",
        "    def __init__(self, input_acc, input_gyr,input_hof, drop_prob=0.0):\n",
        "        super(MM_ED_Bi_GRU_attention_FW, self).__init__()\n",
        "\n",
        "        self.encoder_acc=Encoder(input_acc, drop_prob)\n",
        "        self.encoder_gyr=Encoder(input_gyr, drop_prob)\n",
        "        self.encoder_hof=Encoder(input_hof, drop_prob)\n",
        "\n",
        "        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)\n",
        "        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)\n",
        "        self.BN_hof= nn.BatchNorm1d(input_hof, affine=False)\n",
        "\n",
        "\n",
        "        self.temporal_attn_acc = TemporalAttention(128)\n",
        "        self.temporal_attn_gyr = TemporalAttention(128)\n",
        "        self.temporal_attn_hof = TemporalAttention(128)\n",
        "\n",
        "\n",
        "        self.decoder=LSTMDecoder(390, 3*64, 6, 1, 0.05)\n",
        "\n",
        "    def forward(self, x_acc, x_gyr, x_hof):\n",
        "\n",
        "        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))\n",
        "        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))\n",
        "        x_hof_1=x_hof.view(x_hof.size(0)*x_hof.size(1),x_hof.size(-1))\n",
        "\n",
        "        x_acc_1=self.BN_acc(x_acc_1)\n",
        "        x_gyr_1=self.BN_gyr(x_gyr_1)\n",
        "        x_hof_1=self.BN_hof(x_hof_1)\n",
        "\n",
        "        x_acc_2=x_acc_1.view(-1, window, x_acc_1.size(-1))\n",
        "        x_gyr_2=x_gyr_1.view(-1, window, x_gyr_1.size(-1))\n",
        "        x_hof_2=x_hof_1.view(-1, window_hof, x_hof_1.size(-1))\n",
        "\n",
        "        x_acc, h_acc=self.encoder_acc(x_acc_2)\n",
        "        x_gyr, h_gyr=self.encoder_gyr(x_gyr_2)\n",
        "        x_hof, h_hof=self.encoder_hof(x_hof_2)\n",
        "\n",
        "        x_hof=x_hof.transpose(1,2)\n",
        "        x_hof= F.interpolate(x_hof, size=(45))\n",
        "        x_hof=x_hof.transpose(1,2)\n",
        "\n",
        "\n",
        "        x=torch.cat((x_acc,x_gyr,x_hof),dim=-1)\n",
        "        h=torch.cat((h_acc,h_gyr,h_hof),dim=-1)\n",
        "\n",
        "\n",
        "        h_acc_att=self.temporal_attn_acc(x_acc)\n",
        "        h_gyr_att=self.temporal_attn_gyr(x_gyr)\n",
        "        h_hof_att=self.temporal_attn_hof(x_hof)\n",
        "\n",
        "\n",
        "        h_att=torch.cat((h_acc_att,h_gyr_att,h_hof_att),dim=-1)\n",
        "\n",
        "        #Do it separately\n",
        "\n",
        "        h_att=h_att.unsqueeze(0)\n",
        "        h_att=h_att.transpose(1,0)\n",
        "        out=self.decoder(h,h_att, 9)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "d403205c-5337-4198-f1ee-0bebe9fb709b",
        "id": "_IzP3CnyAD8w"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, time: 67.3476, Training Loss: 5.0805,  Validation loss: 2.9329\n",
            "Epoch: 2, time: 66.2477, Training Loss: 2.8410,  Validation loss: 2.3974\n",
            "Epoch: 3, time: 66.3993, Training Loss: 2.5178,  Validation loss: 2.1982\n",
            "Epoch: 4, time: 65.9367, Training Loss: 2.3519,  Validation loss: 2.0725\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-165-dc716b89b58b>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mMM_ED_Bi_GRU_attention_FW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m36\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmm_ed_bi_gru_attention_fw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_mm_m\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_mm_ed_bi_gru_attention_fw_IMU8_hof.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-153-045009fc72e3>\u001b[0m in \u001b[0;36mtrain_mm_m\u001b[0;34m(train_loader, learn_rate, EPOCHS, model, filename)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_gyr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_hof\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_present\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_future\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_gyr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_hof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-164-c2e87b0a6958>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_acc, x_gyr, x_hof)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mh_att\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mh_att\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mh_att\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mh_att\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh_att\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-162-5c236e30ba43>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, encoder_hidden, h_att, max_len)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;31m# Run one time step of LSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    996\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 998\u001b[0;31m             result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    999\u001b[0m                              self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m   1000\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "lr = 0.001\n",
        "model= MM_ED_Bi_GRU_attention_FW(24,24,36)\n",
        "\n",
        "mm_ed_bi_gru_attention_fw = train_mm_m(train_loader, lr,15,model,path+subject+'_mm_ed_bi_gru_attention_fw_IMU8_hof.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cebdf6fa-2d8a-4bd7-d7f7-0ae74568312a",
        "id": "26LaoYRfAD8w"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 166
        }
      ],
      "source": [
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abd3121e-45e9-4235-b732-4f753b1fe678",
        "id": "mNTHYqKrAD8w"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(18718, 9, 6)\n",
            "4.89441\n",
            "6.019002\n",
            "3.9768891\n",
            "3.5445812\n",
            "5.1437\n",
            "3.966602\n",
            "\n",
            "\n",
            "0.9747551304987435\n",
            "0.9787777076079676\n",
            "0.9396227357232203\n",
            "0.981943668557083\n",
            "0.9811282381768003\n",
            "0.9464309246746228\n",
            "Mean: 4.591 +/- 0.867\n",
            "Mean: 0.967 +/- 0.021\n"
          ]
        }
      ],
      "source": [
        "mm_ed_bi_gru_attention_fw= MM_ED_Bi_GRU_attention_FW(24,24,36)\n",
        "mm_ed_bi_gru_attention_fw.load_state_dict(torch.load(path+subject+'_mm_ed_bi_gru_attention_fw_IMU8_hof.pth'))\n",
        "mm_ed_bi_gru_attention_fw.to(device)\n",
        "\n",
        "mm_ed_bi_gru_attention_fw.eval()\n",
        "\n",
        "# iterate through batches of test data\n",
        "with torch.no_grad():\n",
        "    for i, (data, data_acc, data_gyr, data_hof, target_present, target_future) in enumerate(test_loader):\n",
        "        output = mm_ed_bi_gru_attention_fw(data_acc.to(device).float(),data_gyr.to(device).float(),data_hof.to(device).float())\n",
        "        if i==0:\n",
        "          yhat_5=output\n",
        "          test_target=target_future\n",
        "\n",
        "        yhat_5=torch.cat((yhat_5,output),dim=0)\n",
        "        test_target=torch.cat((test_target,target_future),dim=0)\n",
        "\n",
        "        # clear memory\n",
        "        del data, target_future,output\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "\n",
        "yhat_4 = yhat_5.detach().cpu().numpy()\n",
        "test_target = test_target.detach().cpu().numpy()\n",
        "print(yhat_4.shape)\n",
        "\n",
        "rmse, p=RMSE_prediction(yhat_4,test_target)\n",
        "\n",
        "ablation_10=np.hstack([rmse,p])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUlkjirxQ2z6"
      },
      "source": [
        "## 11. Attention Without gating+ Bi-LSTM --Encoder+decoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.lstm_1 = nn.LSTM(input_dim, 128, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.lstm_2 = nn.LSTM(256, 64, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_1, _ = self.lstm_1(x)\n",
        "        out_1=self.dropout(out_1)\n",
        "        out_2, (h_n, c_n) = self.lstm_2(out_1)\n",
        "        out_2=self.dropout(out_2)\n",
        "\n",
        "        return out_2, (h_n, c_n)"
      ],
      "metadata": {
        "id": "ZBOLr2I-Rcgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMDecoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout):\n",
        "        super(LSTMDecoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, bidirectional=True, batch_first=True)\n",
        "        self.fc = nn.Linear(2*hidden_size, output_size)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "        self.gating_net = nn.Sequential(nn.Linear(390, 390),nn.Sigmoid())\n",
        "\n",
        "\n",
        "    def forward(self, encoder_hidden, encoder_cell, h_att,  max_len):\n",
        "        batch_size = encoder_hidden.shape[1]\n",
        "        hidden = encoder_hidden\n",
        "        cell = encoder_cell\n",
        "        outputs = []\n",
        "\n",
        "        # Use the last time step of target as the initial input\n",
        "        input_1 = torch.zeros(batch_size,1,6).to(device)\n",
        "        input = torch.cat((input_1, h_att), dim=-1)\n",
        "\n",
        "\n",
        "\n",
        "        for i in range(max_len):\n",
        "\n",
        "            # gating_weight=self.gating_net(input)\n",
        "            # input=input*gating_weight\n",
        "\n",
        "            # Run one time step of LSTM\n",
        "            output, (hidden, cell) = self.lstm(input, (hidden, cell))\n",
        "            output=self.dropout(output)\n",
        "\n",
        "            # Use the output for prediction\n",
        "            output = self.fc(output.squeeze(1))\n",
        "            outputs.append(output.unsqueeze(1))\n",
        "\n",
        "            # Use the predicted output as the next input\n",
        "            input = torch.cat(( output.unsqueeze(1), input[:,:,6:390]), dim=-1)\n",
        "\n",
        "\n",
        "\n",
        "        # Concatenate all the outputs along the time dimension\n",
        "        outputs = torch.cat(outputs, dim=1)\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "kuEwv0kgRcg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TemporalAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(TemporalAttention, self).__init__()\n",
        "\n",
        "        self.W = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, sequence_length, hidden_size)\n",
        "\n",
        "        # Calculate attention weights.\n",
        "        attn = self.V(torch.tanh(self.W(x)))\n",
        "        attn = attn.squeeze(-1)\n",
        "        attn = torch.softmax(attn, dim=1)\n",
        "\n",
        "        # Calculate weighted average of hidden states.\n",
        "        context = attn.unsqueeze(-1) * x\n",
        "        context = context.sum(dim=1)\n",
        "\n",
        "        return context"
      ],
      "metadata": {
        "id": "8JDBnRvnnkTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cg8Aeya0Rcg5"
      },
      "outputs": [],
      "source": [
        "class MM_ED_Bi_LSTM_WFW(nn.Module):\n",
        "    def __init__(self, input_acc, input_gyr,input_hof, drop_prob=0.05):\n",
        "        super(MM_ED_Bi_LSTM_WFW, self).__init__()\n",
        "\n",
        "        self.encoder_acc=Encoder(input_acc, drop_prob)\n",
        "        self.encoder_gyr=Encoder(input_gyr, drop_prob)\n",
        "        self.encoder_hof=Encoder(input_hof, drop_prob)\n",
        "\n",
        "        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)\n",
        "        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)\n",
        "        self.BN_hof= nn.BatchNorm1d(input_hof, affine=False)\n",
        "\n",
        "        self.temporal_attn_acc = TemporalAttention(128)\n",
        "        self.temporal_attn_gyr = TemporalAttention(128)\n",
        "        self.temporal_attn_hof = TemporalAttention(128)\n",
        "\n",
        "\n",
        "        self.decoder=LSTMDecoder(3*128+6, 3*64, 6, 1, 0.05)\n",
        "\n",
        "    def forward(self, x_acc, x_gyr, x_hof):\n",
        "\n",
        "        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))\n",
        "        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))\n",
        "        x_hof_1=x_hof.view(x_hof.size(0)*x_hof.size(1),x_hof.size(-1))\n",
        "\n",
        "        x_acc_1=self.BN_acc(x_acc_1)\n",
        "        x_gyr_1=self.BN_gyr(x_gyr_1)\n",
        "        x_hof_1=self.BN_hof(x_hof_1)\n",
        "\n",
        "        x_acc_2=x_acc_1.view(-1, window, x_acc_1.size(-1))\n",
        "        x_gyr_2=x_gyr_1.view(-1, window, x_gyr_1.size(-1))\n",
        "        x_hof_2=x_hof_1.view(-1, window_hof, x_hof_1.size(-1))\n",
        "\n",
        "        x_acc,(h_acc,c_acc)=self.encoder_acc(x_acc_2)\n",
        "        x_gyr,(h_gyr,c_gyr)=self.encoder_gyr(x_gyr_2)\n",
        "        x_hof,(h_hof,c_hof)=self.encoder_hof(x_hof_2)\n",
        "\n",
        "        h=torch.cat((h_acc,h_gyr,h_hof),dim=-1)\n",
        "        c=torch.cat((c_acc,c_gyr,c_hof),dim=-1)\n",
        "\n",
        "        h_acc_att=self.temporal_attn_acc(x_acc)\n",
        "        h_gyr_att=self.temporal_attn_gyr(x_gyr)\n",
        "        h_hof_att=self.temporal_attn_hof(x_hof)\n",
        "\n",
        "\n",
        "        h_att=torch.cat((h_acc_att,h_gyr_att,h_hof_att),dim=-1)\n",
        "\n",
        "        #Do it separately\n",
        "\n",
        "        h_att=h_att.unsqueeze(0)\n",
        "        h_att=h_att.transpose(1,0)\n",
        "\n",
        "        out=self.decoder(h, c, h_att, 9)\n",
        "\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "95a1d53a-0654-4a00-c0ac-1fba5639d33a",
        "id": "kcDkHa0FRcg5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, time: 76.7736, Training Loss: 5.5185,  Validation loss: 3.0833\n",
            "Epoch: 2, time: 75.8279, Training Loss: 3.0199,  Validation loss: 2.5674\n",
            "Epoch: 3, time: 76.0158, Training Loss: 2.6697,  Validation loss: 2.3778\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-18e66a128b73>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMM_ED_Bi_LSTM_WFW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m36\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmm_ed_bi_lstm_WFW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_mm_m\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_mm_ed_bi_lstm_WFW_IMU8_hof.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-26-045009fc72e3>\u001b[0m in \u001b[0;36mtrain_mm_m\u001b[0;34m(train_loader, learn_rate, EPOCHS, model, filename)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "lr = 0.001\n",
        "model = MM_ED_Bi_LSTM_WFW(24,24,36)\n",
        "\n",
        "mm_ed_bi_lstm_WFW = train_mm_m(train_loader, lr,12,model,path+subject+'_mm_ed_bi_lstm_WFW_IMU8_hof.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce3a0274-822d-4c7a-f66d-4963e255781c",
        "id": "Xm-EjT6BRcg5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e60b097-b6bf-4575-a526-aea10b7932f5",
        "id": "9FLZAUrxRcg6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(18718, 9, 6)\n",
            "4.65375\n",
            "5.9890943\n",
            "3.741672\n",
            "3.1439338\n",
            "4.5193315\n",
            "3.9471853\n",
            "\n",
            "\n",
            "0.9756094612726343\n",
            "0.9742531753551379\n",
            "0.9274537355258423\n",
            "0.9828250227233615\n",
            "0.9812887904405857\n",
            "0.9293976588407373\n",
            "Mean: 4.332 +/- 0.799\n",
            "Mean: 0.962 +/- 0.029\n"
          ]
        }
      ],
      "source": [
        "mm_ed_bi_lstm_WFW= MM_ED_Bi_LSTM_WFW(24,24,36)\n",
        "mm_ed_bi_lstm_WFW.load_state_dict(torch.load(path+subject+'_mm_ed_bi_lstm_WFW_IMU8_hof.pth'))\n",
        "mm_ed_bi_lstm_WFW.to(device)\n",
        "\n",
        "mm_ed_bi_lstm_WFW.eval()\n",
        "\n",
        "# iterate through batches of test data\n",
        "with torch.no_grad():\n",
        "    for i, (data, data_acc, data_gyr, data_hof, target_present, target_future) in enumerate(test_loader):\n",
        "        output = mm_ed_bi_lstm_WFW(data_acc.to(device).float(),data_gyr.to(device).float(),data_hof.to(device).float())\n",
        "        if i==0:\n",
        "          yhat_5=output\n",
        "          test_target=target_future\n",
        "\n",
        "        yhat_5=torch.cat((yhat_5,output),dim=0)\n",
        "        test_target=torch.cat((test_target,target_future),dim=0)\n",
        "\n",
        "        # clear memory\n",
        "        del data, target_future,output\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "\n",
        "yhat_4 = yhat_5.detach().cpu().numpy()\n",
        "test_target = test_target.detach().cpu().numpy()\n",
        "print(yhat_4.shape)\n",
        "\n",
        "rmse, p=RMSE_prediction(yhat_4,test_target)\n",
        "\n",
        "ablation_11=np.hstack([rmse,p])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iJ-wk0eQ20B"
      },
      "source": [
        "## 12. Attention With gating+ Bi-LSTM --Encoder+decoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.lstm_1 = nn.LSTM(input_dim, 128, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.lstm_2 = nn.LSTM(256, 64, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_1, _ = self.lstm_1(x)\n",
        "        out_1=self.dropout(out_1)\n",
        "        out_2, (h_n, c_n) = self.lstm_2(out_1)\n",
        "        out_2=self.dropout(out_2)\n",
        "\n",
        "        return out_2, (h_n, c_n)"
      ],
      "metadata": {
        "id": "Xm7HcoC7YwYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMDecoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout):\n",
        "        super(LSTMDecoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, bidirectional=True, batch_first=True)\n",
        "        self.fc = nn.Linear(2*hidden_size, output_size)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "        self.gating_net = nn.Sequential(nn.Linear(390, 390),nn.Sigmoid())\n",
        "\n",
        "\n",
        "    def forward(self, encoder_hidden, encoder_cell, h_att,  max_len):\n",
        "        batch_size = encoder_hidden.shape[1]\n",
        "        hidden = encoder_hidden\n",
        "        cell = encoder_cell\n",
        "        outputs = []\n",
        "\n",
        "        # Use the last time step of target as the initial input\n",
        "        input_1 = torch.zeros(batch_size,1,6).to(device)\n",
        "        input = torch.cat((input_1, h_att), dim=-1)\n",
        "\n",
        "\n",
        "\n",
        "        for i in range(max_len):\n",
        "\n",
        "            gating_weight=self.gating_net(input)\n",
        "            input=input*gating_weight\n",
        "\n",
        "            # Run one time step of LSTM\n",
        "            output, (hidden, cell) = self.lstm(input, (hidden, cell))\n",
        "            output=self.dropout(output)\n",
        "\n",
        "            # Use the output for prediction\n",
        "            output = self.fc(output.squeeze(1))\n",
        "            outputs.append(output.unsqueeze(1))\n",
        "\n",
        "            # Use the predicted output as the next input\n",
        "            input = torch.cat(( output.unsqueeze(1), input[:,:,6:390]), dim=-1)\n",
        "\n",
        "\n",
        "\n",
        "        # Concatenate all the outputs along the time dimension\n",
        "        outputs = torch.cat(outputs, dim=1)\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "TjS7GVZgYwYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TemporalAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(TemporalAttention, self).__init__()\n",
        "\n",
        "        self.W = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, sequence_length, hidden_size)\n",
        "\n",
        "        # Calculate attention weights.\n",
        "        attn = self.V(torch.tanh(self.W(x)))\n",
        "        attn = attn.squeeze(-1)\n",
        "        attn = torch.softmax(attn, dim=1)\n",
        "\n",
        "        # Calculate weighted average of hidden states.\n",
        "        context = attn.unsqueeze(-1) * x\n",
        "        context = context.sum(dim=1)\n",
        "\n",
        "        return context"
      ],
      "metadata": {
        "id": "7UYmBf7TYwYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgJuOApkYwYv"
      },
      "outputs": [],
      "source": [
        "class MM_ED_Bi_LSTM_WF(nn.Module):\n",
        "    def __init__(self, input_acc, input_gyr,input_hof, drop_prob=0.05):\n",
        "        super(MM_ED_Bi_LSTM_WF, self).__init__()\n",
        "\n",
        "        self.encoder_acc=Encoder(input_acc, drop_prob)\n",
        "        self.encoder_gyr=Encoder(input_gyr, drop_prob)\n",
        "        self.encoder_hof=Encoder(input_hof, drop_prob)\n",
        "\n",
        "        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)\n",
        "        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)\n",
        "        self.BN_hof= nn.BatchNorm1d(input_hof, affine=False)\n",
        "\n",
        "        self.temporal_attn_acc = TemporalAttention(128)\n",
        "        self.temporal_attn_gyr = TemporalAttention(128)\n",
        "        self.temporal_attn_hof = TemporalAttention(128)\n",
        "\n",
        "\n",
        "        self.decoder=LSTMDecoder(3*128+6, 3*64, 6, 1, 0.05)\n",
        "\n",
        "    def forward(self, x_acc, x_gyr, x_hof):\n",
        "\n",
        "        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))\n",
        "        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))\n",
        "        x_hof_1=x_hof.view(x_hof.size(0)*x_hof.size(1),x_hof.size(-1))\n",
        "\n",
        "        x_acc_1=self.BN_acc(x_acc_1)\n",
        "        x_gyr_1=self.BN_gyr(x_gyr_1)\n",
        "        x_hof_1=self.BN_hof(x_hof_1)\n",
        "\n",
        "        x_acc_2=x_acc_1.view(-1, window, x_acc_1.size(-1))\n",
        "        x_gyr_2=x_gyr_1.view(-1, window, x_gyr_1.size(-1))\n",
        "        x_hof_2=x_hof_1.view(-1, window_hof, x_hof_1.size(-1))\n",
        "\n",
        "        x_acc,(h_acc,c_acc)=self.encoder_acc(x_acc_2)\n",
        "        x_gyr,(h_gyr,c_gyr)=self.encoder_gyr(x_gyr_2)\n",
        "        x_hof,(h_hof,c_hof)=self.encoder_hof(x_hof_2)\n",
        "\n",
        "        h=torch.cat((h_acc,h_gyr,h_hof),dim=-1)\n",
        "        c=torch.cat((c_acc,c_gyr,c_hof),dim=-1)\n",
        "\n",
        "        h_acc_att=self.temporal_attn_acc(x_acc)\n",
        "        h_gyr_att=self.temporal_attn_gyr(x_gyr)\n",
        "        h_hof_att=self.temporal_attn_hof(x_hof)\n",
        "\n",
        "\n",
        "        h_att=torch.cat((h_acc_att,h_gyr_att,h_hof_att),dim=-1)\n",
        "\n",
        "        #Do it separately\n",
        "\n",
        "        h_att=h_att.unsqueeze(0)\n",
        "        h_att=h_att.transpose(1,0)\n",
        "\n",
        "        out=self.decoder(h, c, h_att, 9)\n",
        "\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "0be42d9c-05d5-40eb-c6c2-b745b48c2e8e",
        "id": "YyH5fEbRYwYw"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, time: 79.6012, Training Loss: 5.9792,  Validation loss: 3.1301\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-e3a96c67e372>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMM_ED_Bi_LSTM_WF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m36\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmm_ed_bi_lstm_WF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_mm_m\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_mm_ed_bi_lstm_WF_IMU8_hof.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-26-045009fc72e3>\u001b[0m in \u001b[0;36mtrain_mm_m\u001b[0;34m(train_loader, learn_rate, EPOCHS, model, filename)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_gyr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_hof\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_present\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_future\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_gyr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_hof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-50-be169cbda184>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_acc, x_gyr, x_hof)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mh_att\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mh_att\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_att\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-9663310e9a51>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, encoder_hidden, encoder_cell, h_att, max_len)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;31m# Run one time step of LSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    810\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    813\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    814\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "lr = 0.001\n",
        "model = MM_ED_Bi_LSTM_WF(24,24,36)\n",
        "\n",
        "mm_ed_bi_lstm_WF = train_mm_m(train_loader, lr,12,model,path+subject+'_mm_ed_bi_lstm_WF_IMU8_hof.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e5a9fc5-1c68-421b-b3c7-7a6c5932af07",
        "id": "5X9AWDSqYwYw"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99c8a5dd-b7d7-4f89-f974-327581e93073",
        "id": "hfocdjY0YwYw"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(18718, 9, 6)\n",
            "5.052948\n",
            "6.628182\n",
            "3.978851\n",
            "3.5742335\n",
            "4.899725\n",
            "4.148736\n",
            "\n",
            "\n",
            "0.9699024346282157\n",
            "0.9679877709769519\n",
            "0.9046638891283868\n",
            "0.9778484383285247\n",
            "0.9781158213951986\n",
            "0.9061621230280406\n",
            "Mean: 4.714 +/- 0.918\n",
            "Mean: 0.951 +/- 0.039\n"
          ]
        }
      ],
      "source": [
        "mm_ed_bi_lstm_WF= MM_ED_Bi_LSTM_WF(24,24,36)\n",
        "mm_ed_bi_lstm_WF.load_state_dict(torch.load(path+subject+'_mm_ed_bi_lstm_WF_IMU8_hof.pth'))\n",
        "mm_ed_bi_lstm_WF.to(device)\n",
        "\n",
        "mm_ed_bi_lstm_WF.eval()\n",
        "\n",
        "# iterate through batches of test data\n",
        "with torch.no_grad():\n",
        "    for i, (data, data_acc, data_gyr, data_hof, target_present, target_future) in enumerate(test_loader):\n",
        "        output = mm_ed_bi_lstm_WF(data_acc.to(device).float(),data_gyr.to(device).float(),data_hof.to(device).float())\n",
        "        if i==0:\n",
        "          yhat_5=output\n",
        "          test_target=target_future\n",
        "\n",
        "        yhat_5=torch.cat((yhat_5,output),dim=0)\n",
        "        test_target=torch.cat((test_target,target_future),dim=0)\n",
        "\n",
        "        # clear memory\n",
        "        del data, target_future,output\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "\n",
        "yhat_4 = yhat_5.detach().cpu().numpy()\n",
        "test_target = test_target.detach().cpu().numpy()\n",
        "print(yhat_4.shape)\n",
        "\n",
        "rmse, p=RMSE_prediction(yhat_4,test_target)\n",
        "\n",
        "ablation_12=np.hstack([rmse,p])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qBNEnsKe5Gw"
      },
      "source": [
        "## 13. Attention Without gating+ Bi-LSTM+ Bi-GRU --Encoder+decoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder_lstm(nn.Module):\n",
        "    def __init__(self, input_dim, dropout):\n",
        "        super(Encoder_lstm, self).__init__()\n",
        "        self.lstm_1 = nn.LSTM(input_dim, 128, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.lstm_2 = nn.LSTM(256, 64, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_1, _ = self.lstm_1(x)\n",
        "        out_1=self.dropout(out_1)\n",
        "        out_2, (h_n, c_n) = self.lstm_2(out_1)\n",
        "        out_2=self.dropout(out_2)\n",
        "\n",
        "        return out_2, (h_n, c_n)"
      ],
      "metadata": {
        "id": "MxBSUq3ge5G2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder_gru(nn.Module):\n",
        "    def __init__(self, input_dim, dropout):\n",
        "        super(Encoder_gru, self).__init__()\n",
        "        self.lstm_1 = nn.GRU(input_dim, 128, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.lstm_2 = nn.GRU(256, 64, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_1, _ = self.lstm_1(x)\n",
        "        out_1=self.dropout(out_1)\n",
        "        out_2, h_n = self.lstm_2(out_1)\n",
        "        out_2=self.dropout(out_2)\n",
        "\n",
        "        return out_2, h_n"
      ],
      "metadata": {
        "id": "TB0myGHrfkKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMDecoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout):\n",
        "        super(LSTMDecoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, bidirectional=True, batch_first=True)\n",
        "        self.gru = nn.GRU(input_size, hidden_size, num_layers, bidirectional=True, batch_first=True)\n",
        "        self.fc = nn.Linear(2*2*hidden_size, output_size)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "        self.gating_net = nn.Sequential(nn.Linear(390, 390),nn.Sigmoid())\n",
        "\n",
        "\n",
        "    def forward(self, encoder_hidden, encoder_cell, h_gru, h_att_lstm, h_att_gru,  max_len):\n",
        "        batch_size = encoder_hidden.shape[1]\n",
        "        hidden = encoder_hidden\n",
        "        hidden_gru=h_gru\n",
        "        cell = encoder_cell\n",
        "        outputs = []\n",
        "\n",
        "        # Use the last time step of target as the initial input\n",
        "        input_1 = torch.zeros(batch_size,1,6).to(device)\n",
        "        input = torch.cat((input_1, h_att_lstm, h_att_gru), dim=-1)\n",
        "\n",
        "\n",
        "        for i in range(max_len):\n",
        "\n",
        "            # Run one time step of LSTM\n",
        "            output_1, (hidden, cell) = self.lstm(input, (hidden, cell))\n",
        "            output_2, hidden_gru = self.gru(input, hidden_gru)\n",
        "\n",
        "            output=torch.cat((output_1,output_2),dim=-1)\n",
        "\n",
        "            output=self.dropout(output)\n",
        "\n",
        "            # Use the output for prediction\n",
        "            output = self.fc(output.squeeze(1))\n",
        "            outputs.append(output.unsqueeze(1))\n",
        "\n",
        "            # Use the predicted output as the next input\n",
        "            input = torch.cat(( output.unsqueeze(1), input[:,:,6:774]), dim=-1)\n",
        "\n",
        "\n",
        "\n",
        "        # Concatenate all the outputs along the time dimension\n",
        "        outputs = torch.cat(outputs, dim=1)\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "33ppBLLJe5G2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TemporalAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(TemporalAttention, self).__init__()\n",
        "\n",
        "        self.W = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, sequence_length, hidden_size)\n",
        "\n",
        "        # Calculate attention weights.\n",
        "        attn = self.V(torch.tanh(self.W(x)))\n",
        "        attn = attn.squeeze(-1)\n",
        "        attn = torch.softmax(attn, dim=1)\n",
        "\n",
        "        # Calculate weighted average of hidden states.\n",
        "        context = attn.unsqueeze(-1) * x\n",
        "        context = context.sum(dim=1)\n",
        "\n",
        "        return context"
      ],
      "metadata": {
        "id": "aCqG2XLDe5G3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpUHxmPee5G3"
      },
      "outputs": [],
      "source": [
        "class MM_ED_Bi_LSTM_GRU_WFW(nn.Module):\n",
        "    def __init__(self, input_acc, input_gyr,input_hof, drop_prob=0.05):\n",
        "        super(MM_ED_Bi_LSTM_GRU_WFW, self).__init__()\n",
        "\n",
        "        self.encoder_acc_lstm=Encoder_lstm(input_acc, drop_prob)\n",
        "        self.encoder_gyr_lstm=Encoder_lstm(input_gyr, drop_prob)\n",
        "        self.encoder_hof_lstm=Encoder_lstm(input_hof, drop_prob)\n",
        "\n",
        "        self.encoder_acc_gru=Encoder_gru(input_acc, drop_prob)\n",
        "        self.encoder_gyr_gru=Encoder_gru(input_gyr, drop_prob)\n",
        "        self.encoder_hof_gru=Encoder_gru(input_hof, drop_prob)\n",
        "\n",
        "        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)\n",
        "        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)\n",
        "        self.BN_hof= nn.BatchNorm1d(input_hof, affine=False)\n",
        "\n",
        "        self.temporal_attn_acc = TemporalAttention(128)\n",
        "        self.temporal_attn_gyr = TemporalAttention(128)\n",
        "        self.temporal_attn_hof = TemporalAttention(128)\n",
        "\n",
        "\n",
        "        self.decoder=LSTMDecoder(2*3*128+6, 3*64, 6, 1, 0.05)\n",
        "\n",
        "    def forward(self, x_acc, x_gyr, x_hof):\n",
        "\n",
        "        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))\n",
        "        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))\n",
        "        x_hof_1=x_hof.view(x_hof.size(0)*x_hof.size(1),x_hof.size(-1))\n",
        "\n",
        "        x_acc_1=self.BN_acc(x_acc_1)\n",
        "        x_gyr_1=self.BN_gyr(x_gyr_1)\n",
        "        x_hof_1=self.BN_hof(x_hof_1)\n",
        "\n",
        "        x_acc_2=x_acc_1.view(-1, window, x_acc_1.size(-1))\n",
        "        x_gyr_2=x_gyr_1.view(-1, window, x_gyr_1.size(-1))\n",
        "        x_hof_2=x_hof_1.view(-1, window_hof, x_hof_1.size(-1))\n",
        "\n",
        "        x_acc_lstm,(h_acc_lstm, c_acc_lstm)=self.encoder_acc_lstm(x_acc_2)\n",
        "        x_gyr_lstm,(h_gyr_lstm, c_gyr_lstm)=self.encoder_gyr_lstm(x_gyr_2)\n",
        "        x_hof_lstm,(h_hof_lstm, c_hof_lstm)=self.encoder_hof_lstm(x_hof_2)\n",
        "\n",
        "\n",
        "        x_acc_gru,h_acc_gru=self.encoder_acc_gru(x_acc_2)\n",
        "        x_gyr_gru,h_gyr_gru=self.encoder_gyr_gru(x_gyr_2)\n",
        "        x_hof_gru,h_hof_gru=self.encoder_hof_gru(x_hof_2)\n",
        "\n",
        "\n",
        "        h_lstm=torch.cat((h_acc_lstm,h_gyr_lstm,h_hof_lstm),dim=-1)\n",
        "        c_lstm=torch.cat((c_acc_lstm,c_gyr_lstm,c_hof_lstm),dim=-1)\n",
        "\n",
        "        h_gru=torch.cat((h_acc_gru,h_gyr_gru,h_hof_gru),dim=-1)\n",
        "\n",
        "\n",
        "        h_acc_att_lstm=self.temporal_attn_acc(x_acc_lstm)\n",
        "        h_gyr_att_lstm=self.temporal_attn_gyr(x_gyr_lstm)\n",
        "        h_hof_att_lstm=self.temporal_attn_hof(x_hof_lstm)\n",
        "\n",
        "\n",
        "        h_acc_att_gru=self.temporal_attn_acc(x_acc_gru)\n",
        "        h_gyr_att_gru=self.temporal_attn_gyr(x_gyr_gru)\n",
        "        h_hof_att_gru=self.temporal_attn_hof(x_hof_gru)\n",
        "\n",
        "\n",
        "        h_att_lstm=torch.cat((h_acc_att_lstm,h_gyr_att_lstm,h_hof_att_lstm),dim=-1)\n",
        "        h_att_gru=torch.cat((h_acc_att_gru,h_gyr_att_gru,h_hof_att_gru),dim=-1)\n",
        "\n",
        "        #Do it separately\n",
        "\n",
        "        h_att_lstm=h_att_lstm.unsqueeze(0)\n",
        "        h_att_lstm=h_att_lstm.transpose(1,0)\n",
        "\n",
        "        h_att_gru=h_att_gru.unsqueeze(0)\n",
        "        h_att_gru=h_att_gru.transpose(1,0)\n",
        "\n",
        "        out=self.decoder(h_lstm, c_lstm, h_gru, h_att_lstm, h_att_gru, 9)\n",
        "\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a6ccd8f-2036-4bdd-93dc-d96d8a75eac8",
        "id": "OTIgrfzGe5G3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, time: 124.3969, Training Loss: 4.3842,  Validation loss: 2.7420\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-8c32a517f244>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMM_ED_Bi_LSTM_GRU_WFW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m36\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmm_ed_bi_lstm_gru_WFW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_mm_m\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_mm_ed_bi_lstm_gru_WFW_IMU8_hof.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-045009fc72e3>\u001b[0m in \u001b[0;36mtrain_mm_m\u001b[0;34m(train_loader, learn_rate, EPOCHS, model, filename)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_gyr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_hof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "lr = 0.001\n",
        "model = MM_ED_Bi_LSTM_GRU_WFW(24,24,36)\n",
        "\n",
        "mm_ed_bi_lstm_gru_WFW = train_mm_m(train_loader, lr,12,model,path+subject+'_mm_ed_bi_lstm_gru_WFW_IMU8_hof.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42dfb4a3-e372-4d25-d87a-0f75e466cfb2",
        "id": "Q1itVnRle5G3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9571cb02-b1ed-45ff-dfe5-806b49af3635",
        "id": "pEoYJLsxe5G3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(18718, 9, 6)\n",
            "4.7527905\n",
            "6.1163077\n",
            "3.857654\n",
            "3.6282103\n",
            "5.10625\n",
            "4.243827\n",
            "\n",
            "\n",
            "0.9742496952249654\n",
            "0.9764147958650746\n",
            "0.9263430769912734\n",
            "0.9813187864939696\n",
            "0.9772392479927535\n",
            "0.9192215342353331\n",
            "Mean: 4.618 +/- 0.863\n",
            "Mean: 0.959 +/- 0.031\n"
          ]
        }
      ],
      "source": [
        "mm_ed_bi_lstm_gru_WFW= MM_ED_Bi_LSTM_GRU_WFW(24,24,36)\n",
        "mm_ed_bi_lstm_gru_WFW.load_state_dict(torch.load(path+subject+'_mm_ed_bi_lstm_gru_WFW_IMU8_hof.pth'))\n",
        "mm_ed_bi_lstm_gru_WFW.to(device)\n",
        "\n",
        "mm_ed_bi_lstm_gru_WFW.eval()\n",
        "\n",
        "# iterate through batches of test data\n",
        "with torch.no_grad():\n",
        "    for i, (data, data_acc, data_gyr, data_hof, target_present, target_future) in enumerate(test_loader):\n",
        "        output = mm_ed_bi_lstm_gru_WFW(data_acc.to(device).float(),data_gyr.to(device).float(),data_hof.to(device).float())\n",
        "        if i==0:\n",
        "          yhat_5=output\n",
        "          test_target=target_future\n",
        "\n",
        "        yhat_5=torch.cat((yhat_5,output),dim=0)\n",
        "        test_target=torch.cat((test_target,target_future),dim=0)\n",
        "\n",
        "        # clear memory\n",
        "        del data, target_future,output\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "\n",
        "yhat_4 = yhat_5.detach().cpu().numpy()\n",
        "test_target = test_target.detach().cpu().numpy()\n",
        "print(yhat_4.shape)\n",
        "\n",
        "rmse, p=RMSE_prediction(yhat_4,test_target)\n",
        "\n",
        "ablation_13=np.hstack([rmse,p])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzXwU4E6qaWb"
      },
      "source": [
        "## 14. Attention Without gating+ Bi-LSTM + Bi-GRU --Encoder+decoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder_lstm(nn.Module):\n",
        "    def __init__(self, input_dim, dropout):\n",
        "        super(Encoder_lstm, self).__init__()\n",
        "        self.lstm_1 = nn.LSTM(input_dim, 128, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.lstm_2 = nn.LSTM(256, 64, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_1, _ = self.lstm_1(x)\n",
        "        out_1=self.dropout(out_1)\n",
        "        out_2, (h_n, c_n) = self.lstm_2(out_1)\n",
        "        out_2=self.dropout(out_2)\n",
        "\n",
        "        return out_2, (h_n, c_n)"
      ],
      "metadata": {
        "id": "4eGRsNiHqaWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder_gru(nn.Module):\n",
        "    def __init__(self, input_dim, dropout):\n",
        "        super(Encoder_gru, self).__init__()\n",
        "        self.lstm_1 = nn.GRU(input_dim, 128, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.lstm_2 = nn.GRU(256, 64, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_1, _ = self.lstm_1(x)\n",
        "        out_1=self.dropout(out_1)\n",
        "        out_2, h_n = self.lstm_2(out_1)\n",
        "        out_2=self.dropout(out_2)\n",
        "\n",
        "        return out_2, h_n"
      ],
      "metadata": {
        "id": "JVfwZiIfqaWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMDecoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout):\n",
        "        super(LSTMDecoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, bidirectional=True, batch_first=True)\n",
        "        self.gru = nn.GRU(input_size, hidden_size, num_layers, bidirectional=True, batch_first=True)\n",
        "        self.fc = nn.Linear(2*2*hidden_size, output_size)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "        self.gating_net = nn.Sequential(nn.Linear(2*3*128+6, 2*3*128+6),nn.Sigmoid())\n",
        "\n",
        "\n",
        "    def forward(self, encoder_hidden, encoder_cell, h_gru, h_att_lstm, h_att_gru,  max_len):\n",
        "        batch_size = encoder_hidden.shape[1]\n",
        "        hidden = encoder_hidden\n",
        "        hidden_gru=h_gru\n",
        "        cell = encoder_cell\n",
        "        outputs = []\n",
        "\n",
        "        # Use the last time step of target as the initial input\n",
        "        input_1 = torch.zeros(batch_size,1,6).to(device)\n",
        "        input = torch.cat((input_1, h_att_lstm, h_att_gru), dim=-1)\n",
        "\n",
        "\n",
        "        for i in range(max_len):\n",
        "\n",
        "            gating_weight=self.gating_net(input)\n",
        "            input=input*gating_weight\n",
        "\n",
        "            # Run one time step of LSTM\n",
        "            output_1, (hidden, cell) = self.lstm(input, (hidden, cell))\n",
        "            output_2, hidden_gru = self.gru(input, hidden_gru)\n",
        "\n",
        "            output=torch.cat((output_1,output_2),dim=-1)\n",
        "\n",
        "            output=self.dropout(output)\n",
        "\n",
        "            # Use the output for prediction\n",
        "            output = self.fc(output.squeeze(1))\n",
        "            outputs.append(output.unsqueeze(1))\n",
        "\n",
        "            # Use the predicted output as the next input\n",
        "            input = torch.cat(( output.unsqueeze(1), input[:,:,6:774]), dim=-1)\n",
        "\n",
        "\n",
        "\n",
        "        # Concatenate all the outputs along the time dimension\n",
        "        outputs = torch.cat(outputs, dim=1)\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "AEZXg4ixqaWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TemporalAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(TemporalAttention, self).__init__()\n",
        "\n",
        "        self.W = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, sequence_length, hidden_size)\n",
        "\n",
        "        # Calculate attention weights.\n",
        "        attn = self.V(torch.tanh(self.W(x)))\n",
        "        attn = attn.squeeze(-1)\n",
        "        attn = torch.softmax(attn, dim=1)\n",
        "\n",
        "        # Calculate weighted average of hidden states.\n",
        "        context = attn.unsqueeze(-1) * x\n",
        "        context = context.sum(dim=1)\n",
        "\n",
        "        return context"
      ],
      "metadata": {
        "id": "Y4Q74jggqaWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7swUMpmTqaWc"
      },
      "outputs": [],
      "source": [
        "class MM_ED_Bi_LSTM_GRU_WF(nn.Module):\n",
        "    def __init__(self, input_acc, input_gyr,input_hof, drop_prob=0.05):\n",
        "        super(MM_ED_Bi_LSTM_GRU_WF, self).__init__()\n",
        "\n",
        "        self.encoder_acc_lstm=Encoder_lstm(input_acc, drop_prob)\n",
        "        self.encoder_gyr_lstm=Encoder_lstm(input_gyr, drop_prob)\n",
        "        self.encoder_hof_lstm=Encoder_lstm(input_hof, drop_prob)\n",
        "\n",
        "        self.encoder_acc_gru=Encoder_gru(input_acc, drop_prob)\n",
        "        self.encoder_gyr_gru=Encoder_gru(input_gyr, drop_prob)\n",
        "        self.encoder_hof_gru=Encoder_gru(input_hof, drop_prob)\n",
        "\n",
        "        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)\n",
        "        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)\n",
        "        self.BN_hof= nn.BatchNorm1d(input_hof, affine=False)\n",
        "\n",
        "        self.temporal_attn_acc = TemporalAttention(128)\n",
        "        self.temporal_attn_gyr = TemporalAttention(128)\n",
        "        self.temporal_attn_hof = TemporalAttention(128)\n",
        "\n",
        "\n",
        "        self.decoder=LSTMDecoder(2*3*128+6, 3*64, 6, 1, 0.05)\n",
        "\n",
        "    def forward(self, x_acc, x_gyr, x_hof):\n",
        "\n",
        "        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))\n",
        "        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))\n",
        "        x_hof_1=x_hof.view(x_hof.size(0)*x_hof.size(1),x_hof.size(-1))\n",
        "\n",
        "        x_acc_1=self.BN_acc(x_acc_1)\n",
        "        x_gyr_1=self.BN_gyr(x_gyr_1)\n",
        "        x_hof_1=self.BN_hof(x_hof_1)\n",
        "\n",
        "        x_acc_2=x_acc_1.view(-1, window, x_acc_1.size(-1))\n",
        "        x_gyr_2=x_gyr_1.view(-1, window, x_gyr_1.size(-1))\n",
        "        x_hof_2=x_hof_1.view(-1, window_hof, x_hof_1.size(-1))\n",
        "\n",
        "        x_acc_lstm,(h_acc_lstm, c_acc_lstm)=self.encoder_acc_lstm(x_acc_2)\n",
        "        x_gyr_lstm,(h_gyr_lstm, c_gyr_lstm)=self.encoder_gyr_lstm(x_gyr_2)\n",
        "        x_hof_lstm,(h_hof_lstm, c_hof_lstm)=self.encoder_hof_lstm(x_hof_2)\n",
        "\n",
        "\n",
        "        x_acc_gru,h_acc_gru=self.encoder_acc_gru(x_acc_2)\n",
        "        x_gyr_gru,h_gyr_gru=self.encoder_gyr_gru(x_gyr_2)\n",
        "        x_hof_gru,h_hof_gru=self.encoder_hof_gru(x_hof_2)\n",
        "\n",
        "\n",
        "        h_lstm=torch.cat((h_acc_lstm,h_gyr_lstm,h_hof_lstm),dim=-1)\n",
        "        c_lstm=torch.cat((c_acc_lstm,c_gyr_lstm,c_hof_lstm),dim=-1)\n",
        "\n",
        "        h_gru=torch.cat((h_acc_gru,h_gyr_gru,h_hof_gru),dim=-1)\n",
        "\n",
        "\n",
        "        h_acc_att_lstm=self.temporal_attn_acc(x_acc_lstm)\n",
        "        h_gyr_att_lstm=self.temporal_attn_gyr(x_gyr_lstm)\n",
        "        h_hof_att_lstm=self.temporal_attn_hof(x_hof_lstm)\n",
        "\n",
        "\n",
        "        h_acc_att_gru=self.temporal_attn_acc(x_acc_gru)\n",
        "        h_gyr_att_gru=self.temporal_attn_gyr(x_gyr_gru)\n",
        "        h_hof_att_gru=self.temporal_attn_hof(x_hof_gru)\n",
        "\n",
        "\n",
        "        h_att_lstm=torch.cat((h_acc_att_lstm,h_gyr_att_lstm,h_hof_att_lstm),dim=-1)\n",
        "        h_att_gru=torch.cat((h_acc_att_gru,h_gyr_att_gru,h_hof_att_gru),dim=-1)\n",
        "\n",
        "        #Do it separately\n",
        "\n",
        "        h_att_lstm=h_att_lstm.unsqueeze(0)\n",
        "        h_att_lstm=h_att_lstm.transpose(1,0)\n",
        "\n",
        "        h_att_gru=h_att_gru.unsqueeze(0)\n",
        "        h_att_gru=h_att_gru.transpose(1,0)\n",
        "\n",
        "\n",
        "\n",
        "        out=self.decoder(h_lstm, c_lstm, h_gru, h_att_lstm, h_att_gru, 9)\n",
        "\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbbc9b0c-3ad4-4c00-9744-c77b58695df8",
        "id": "58Sr5w7nqaWc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, time: 132.7939, Training Loss: 4.5319,  Validation loss: 2.7744\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-c39468df02a6>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMM_ED_Bi_LSTM_GRU_WF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m36\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmm_ed_bi_lstm_gru_WF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_mm_m\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_mm_ed_bi_lstm_gru_WF_IMU8_hof.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-045009fc72e3>\u001b[0m in \u001b[0;36mtrain_mm_m\u001b[0;34m(train_loader, learn_rate, EPOCHS, model, filename)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "lr = 0.001\n",
        "model = MM_ED_Bi_LSTM_GRU_WF(24,24,36)\n",
        "\n",
        "mm_ed_bi_lstm_gru_WF = train_mm_m(train_loader, lr,12,model,path+subject+'_mm_ed_bi_lstm_gru_WF_IMU8_hof.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f05ef5f-522a-4314-f25f-ba1f1e7bcd58",
        "id": "2eT6OfivqaWc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d30d601-72ad-4105-de84-fd7ffc138baa",
        "id": "S0gLfl01qaWd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(18718, 9, 6)\n",
            "4.7582517\n",
            "5.937454\n",
            "4.208193\n",
            "3.399551\n",
            "5.687384\n",
            "4.337381\n",
            "\n",
            "\n",
            "0.9738523935759464\n",
            "0.975415009291972\n",
            "0.923289597821706\n",
            "0.9801899970350705\n",
            "0.9774105491517506\n",
            "0.9237925254937391\n",
            "Mean: 4.721 +/- 0.950\n",
            "Mean: 0.959 +/- 0.031\n"
          ]
        }
      ],
      "source": [
        "mm_ed_bi_lstm_gru_WF= MM_ED_Bi_LSTM_GRU_WF(24,24,36)\n",
        "mm_ed_bi_lstm_gru_WF.load_state_dict(torch.load(path+subject+'_mm_ed_bi_lstm_gru_WF_IMU8_hof.pth'))\n",
        "mm_ed_bi_lstm_gru_WF.to(device)\n",
        "\n",
        "mm_ed_bi_lstm_gru_WF.eval()\n",
        "\n",
        "# iterate through batches of test data\n",
        "with torch.no_grad():\n",
        "    for i, (data, data_acc, data_gyr, data_hof, target_present, target_future) in enumerate(test_loader):\n",
        "        output = mm_ed_bi_lstm_gru_WF(data_acc.to(device).float(),data_gyr.to(device).float(),data_hof.to(device).float())\n",
        "        if i==0:\n",
        "          yhat_5=output\n",
        "          test_target=target_future\n",
        "\n",
        "        yhat_5=torch.cat((yhat_5,output),dim=0)\n",
        "        test_target=torch.cat((test_target,target_future),dim=0)\n",
        "\n",
        "        # clear memory\n",
        "        del data, target_future,output\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "\n",
        "yhat_4 = yhat_5.detach().cpu().numpy()\n",
        "test_target = test_target.detach().cpu().numpy()\n",
        "print(yhat_4.shape)\n",
        "\n",
        "rmse, p=RMSE_prediction(yhat_4,test_target)\n",
        "\n",
        "ablation_14=np.hstack([rmse,p])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEZxWsCyG0_t"
      },
      "source": [
        "# Model Training-- 8 IMUs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vj45YYMkG0_z"
      },
      "source": [
        "## Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnsba4GXG0_z"
      },
      "outputs": [],
      "source": [
        "def train_mm_m(train_loader, learn_rate, EPOCHS, model,filename):\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      model.cuda()\n",
        "    # Defining loss function and optimizer\n",
        "    # criterion =nn.MSELoss()\n",
        "    criterion =RMSELoss()\n",
        "    # criterion =PearsonCorrCoefLoss()\n",
        "\n",
        "\n",
        "    # criterion=PearsonCorrLoss()\n",
        "    # optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "\n",
        "    running_loss=0\n",
        "    # Train the model\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Train the model with early stopping\n",
        "    best_val_loss = float('inf')\n",
        "    patience = 10\n",
        "\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        epoch_start_time = time.time()\n",
        "        model.train()\n",
        "        for i, (data, data_acc, data_gyr, data_hof, target_present, target_future) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            output= model(data_acc.to(device).float(),data_gyr.to(device).float())\n",
        "\n",
        "            loss = criterion(output, target_future.to(device).float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        train_loss=running_loss/len(train_loader)\n",
        "\n",
        "       # Validate\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for data, data_acc, data_gyr, data_hof, target_present, target_future in val_loader:\n",
        "                output= model(data_acc.to(device).float(),data_gyr.to(device).float())\n",
        "                val_loss += criterion(output, target_future.to(device).float())\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        epoch_end_time = time.time()\n",
        "        epoch_training_time = epoch_end_time - epoch_start_time\n",
        "\n",
        "        torch.set_printoptions(precision=4)\n",
        "\n",
        "        print(f\"Epoch: {epoch+1}, time: {epoch_training_time:.4f}, Training Loss: {train_loss:.4f},  Validation loss: {val_loss:.4f}\")\n",
        "\n",
        "        running_loss=0\n",
        "\n",
        "        epoch_end_time = time.time()\n",
        "\n",
        "                # Check if the validation loss has improved\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), filename)\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        # Early stopping if the validation loss hasn't improved for `patience` epochs\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Stopping early after {epoch+1} epochs\")\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    training_time = end_time - start_time\n",
        "    print(f\"Training time: {training_time} seconds\")\n",
        "\n",
        "\n",
        "    # # Save the trained model\n",
        "    # torch.save(model.state_dict(), \"model.pth\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. LSTM"
      ],
      "metadata": {
        "id": "V__uBw92ANwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.lstm_1 = nn.LSTM(input_dim, 128, bidirectional=False, batch_first=True, dropout=0.0)\n",
        "        self.lstm_2 = nn.LSTM(128, 64, bidirectional=False, batch_first=True, dropout=0.0)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_1, _ = self.lstm_1(x)\n",
        "        out_1=self.dropout(out_1)\n",
        "        out_2, (h_n, c_n) = self.lstm_2(out_1)\n",
        "        out_2=self.dropout(out_2)\n",
        "\n",
        "        return out_2, (h_n, c_n)"
      ],
      "metadata": {
        "id": "YnPzOSKUANwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRtHI3sQANwh"
      },
      "outputs": [],
      "source": [
        "class MM_LSTM(nn.Module):\n",
        "    def __init__(self, input_acc, input_gyr, drop_prob=0.0):\n",
        "        super(MM_LSTM, self).__init__()\n",
        "\n",
        "        self.encoder_acc=Encoder(input_acc, drop_prob)\n",
        "        self.encoder_gyr=Encoder(input_gyr, drop_prob)\n",
        "\n",
        "        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)\n",
        "        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)\n",
        "\n",
        "        self.fc = nn.Linear(2*64, 6*9)\n",
        "\n",
        "    def forward(self, x_acc, x_gyr):\n",
        "\n",
        "        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))\n",
        "        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))\n",
        "\n",
        "        x_acc_1=self.BN_acc(x_acc_1)\n",
        "        x_gyr_1=self.BN_gyr(x_gyr_1)\n",
        "\n",
        "        x_acc_2=x_acc_1.view(-1, window, x_acc_1.size(-1))\n",
        "        x_gyr_2=x_gyr_1.view(-1, window, x_gyr_1.size(-1))\n",
        "\n",
        "        x_acc,(h_acc,c_acc)=self.encoder_acc(x_acc_2)\n",
        "        x_gyr,(h_gyr,c_gyr)=self.encoder_gyr(x_gyr_2)\n",
        "\n",
        "\n",
        "        x=torch.cat((x_acc[:,-1,:],x_gyr[:,-1,:]),dim=-1).squeeze(0)\n",
        "\n",
        "        out=self.fc(x).view(x.shape[0],9,6)\n",
        "\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "f304ec91-ce64-4d30-f100-86a0af8ec934",
        "id": "4tB_qTVzANwh"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, time: 16.6776, Training Loss: 10.5227,  Validation loss: 5.5717\n",
            "Epoch: 2, time: 15.2462, Training Loss: 4.7944,  Validation loss: 4.1137\n",
            "Epoch: 3, time: 15.4241, Training Loss: 3.8770,  Validation loss: 3.4332\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-f04dfe83447e>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMM_LSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmm_lstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_mm_m\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_mm_lstm_IMU8.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-71b1fe5884af>\u001b[0m in \u001b[0;36mtrain_mm_m\u001b[0;34m(train_loader, learn_rate, EPOCHS, model, filename)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "lr = 0.001\n",
        "model = MM_LSTM(24,24)\n",
        "\n",
        "mm_lstm = train_mm_m(train_loader, lr,15,model,path+subject+'_mm_lstm_IMU8.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "590e31e5-45a6-4284-c444-e85c28832299",
        "id": "6O2QOgiVANwh"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2527b41b-efd8-4788-834c-fead2be48ffc",
        "id": "5FUkHCa8ANwi"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(18718, 9, 6)\n",
            "4.949417\n",
            "5.553074\n",
            "4.085481\n",
            "4.861573\n",
            "5.546299\n",
            "3.8134406\n",
            "\n",
            "\n",
            "0.9723253934658082\n",
            "0.9734864189895154\n",
            "0.9013688187204286\n",
            "0.9672491368527629\n",
            "0.9744345396099293\n",
            "0.9180798221345129\n",
            "Mean: 4.802 +/- 0.805\n",
            "Mean: 0.951 +/- 0.036\n"
          ]
        }
      ],
      "source": [
        "mm_lstm= MM_LSTM(24,24)\n",
        "mm_lstm.load_state_dict(torch.load(path+subject+'_mm_lstm_IMU8.pth'))\n",
        "mm_lstm.to(device)\n",
        "\n",
        "mm_lstm.eval()\n",
        "\n",
        "# iterate through batches of test data\n",
        "with torch.no_grad():\n",
        "    for i, (data, data_acc, data_gyr, data_hof, target_present, target_future) in enumerate(test_loader):\n",
        "        output = mm_lstm(data_acc.to(device).float(),data_gyr.to(device).float())\n",
        "        if i==0:\n",
        "          yhat_5=output\n",
        "          test_target=target_future\n",
        "\n",
        "        yhat_5=torch.cat((yhat_5,output),dim=0)\n",
        "        test_target=torch.cat((test_target,target_future),dim=0)\n",
        "\n",
        "        # clear memory\n",
        "        del data, target_future,output\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "\n",
        "yhat_4 = yhat_5.detach().cpu().numpy()\n",
        "test_target = test_target.detach().cpu().numpy()\n",
        "print(yhat_4.shape)\n",
        "\n",
        "rmse, p=RMSE_prediction(yhat_4,test_target)\n",
        "\n",
        "ablation_1=np.hstack([rmse,p])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. GRU"
      ],
      "metadata": {
        "id": "LzFPBtS6ANwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.lstm_1 = nn.GRU(input_dim, 128, bidirectional=False, batch_first=True, dropout=0.0)\n",
        "        self.lstm_2 = nn.GRU(128, 64, bidirectional=False, batch_first=True, dropout=0.0)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_1, _ = self.lstm_1(x)\n",
        "        out_1=self.dropout(out_1)\n",
        "        out_2, h_n = self.lstm_2(out_1)\n",
        "        out_2=self.dropout(out_2)\n",
        "\n",
        "        return out_2, h_n"
      ],
      "metadata": {
        "id": "_psTGXAeANwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQCTfaIoANwi"
      },
      "outputs": [],
      "source": [
        "class MM_GRU(nn.Module):\n",
        "    def __init__(self, input_acc, input_gyr, drop_prob=0.0):\n",
        "        super(MM_GRU, self).__init__()\n",
        "\n",
        "        self.encoder_acc=Encoder(input_acc, drop_prob)\n",
        "        self.encoder_gyr=Encoder(input_gyr, drop_prob)\n",
        "\n",
        "        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)\n",
        "        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)\n",
        "\n",
        "        self.fc = nn.Linear(2*64, 6*9)\n",
        "\n",
        "        self.flatten=nn.Flatten()\n",
        "\n",
        "    def forward(self, x_acc, x_gyr):\n",
        "\n",
        "        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))\n",
        "        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))\n",
        "\n",
        "        x_acc_1=self.BN_acc(x_acc_1)\n",
        "        x_gyr_1=self.BN_gyr(x_gyr_1)\n",
        "\n",
        "        x_acc_2=x_acc_1.view(-1, window, x_acc_1.size(-1))\n",
        "        x_gyr_2=x_gyr_1.view(-1, window, x_gyr_1.size(-1))\n",
        "\n",
        "        x_acc,h_acc=self.encoder_acc(x_acc_2)\n",
        "        x_gyr,h_gyr=self.encoder_gyr(x_gyr_2)\n",
        "\n",
        "        x=torch.cat((x_acc[:,-1,:],x_gyr[:,-1,:]),dim=-1)\n",
        "\n",
        "        out=self.fc(x).view(x.shape[0],9,6)\n",
        "\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "f10e3473-1569-4e56-b426-06fef88fb57f",
        "id": "2WYJz23qANwi"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, time: 16.4847, Training Loss: 8.3214,  Validation loss: 3.9811\n",
            "Epoch: 2, time: 14.1543, Training Loss: 3.5747,  Validation loss: 3.0023\n",
            "Epoch: 3, time: 14.0905, Training Loss: 2.9865,  Validation loss: 2.7240\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-90bb99308f8f>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMM_GRU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmm_gru\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_mm_m\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_mm_gru_IMU8.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-71b1fe5884af>\u001b[0m in \u001b[0;36mtrain_mm_m\u001b[0;34m(train_loader, learn_rate, EPOCHS, model, filename)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mepoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_gyr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_hof\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_present\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_future\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_gyr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Handle `CustomType` automatically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \"\"\"\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_collate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcollate_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/abc.py\u001b[0m in \u001b[0;36m__instancecheck__\u001b[0;34m(cls, instance)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__instancecheck__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;34m\"\"\"Override for isinstance(instance, cls).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_abc_instancecheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__subclasscheck__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubclass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "lr = 0.001\n",
        "model = MM_GRU(24,24)\n",
        "\n",
        "mm_gru = train_mm_m(train_loader, lr,15,model,path+subject+'_mm_gru_IMU8.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbc6af29-50f8-440d-b86d-c0cca396b4a8",
        "id": "yXSpXvV_ANwj"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca040c58-07ab-4466-94d8-20fb2976842a",
        "id": "DilTqBnnANwj"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(18718, 9, 6)\n",
            "5.0360312\n",
            "6.332044\n",
            "3.515925\n",
            "3.2935438\n",
            "4.669665\n",
            "3.93904\n",
            "\n",
            "\n",
            "0.9756019017231119\n",
            "0.9788432166834375\n",
            "0.9357594689988705\n",
            "0.9812021150435383\n",
            "0.9810001182791444\n",
            "0.9373002671333565\n",
            "Mean: 4.464 +/- 0.924\n",
            "Mean: 0.965 +/- 0.025\n"
          ]
        }
      ],
      "source": [
        "mm_gru= MM_GRU(24,24)\n",
        "mm_gru.load_state_dict(torch.load(path+subject+'_mm_gru_IMU8.pth'))\n",
        "mm_gru.to(device)\n",
        "\n",
        "mm_gru.eval()\n",
        "\n",
        "# iterate through batches of test data\n",
        "with torch.no_grad():\n",
        "    for i, (data, data_acc, data_gyr, data_hof, target_present, target_future) in enumerate(test_loader):\n",
        "        output = mm_gru(data_acc.to(device).float(),data_gyr.to(device).float())\n",
        "        if i==0:\n",
        "          yhat_5=output\n",
        "          test_target=target_future\n",
        "\n",
        "        yhat_5=torch.cat((yhat_5,output),dim=0)\n",
        "        test_target=torch.cat((test_target,target_future),dim=0)\n",
        "\n",
        "        # clear memory\n",
        "        del data, target_future,output\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "\n",
        "yhat_4 = yhat_5.detach().cpu().numpy()\n",
        "test_target = test_target.detach().cpu().numpy()\n",
        "print(yhat_4.shape)\n",
        "\n",
        "rmse, p=RMSE_prediction(yhat_4,test_target)\n",
        "\n",
        "ablation_2=np.hstack([rmse,p])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Bi-LSTM"
      ],
      "metadata": {
        "id": "EDyHRbpDANwj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.lstm_1 = nn.LSTM(input_dim, 128, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.lstm_2 = nn.LSTM(256, 64, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_1, _ = self.lstm_1(x)\n",
        "        out_1=self.dropout(out_1)\n",
        "        out_2, (h_n, c_n) = self.lstm_2(out_1)\n",
        "        out_2=self.dropout(out_2)\n",
        "\n",
        "        return out_2, (h_n, c_n)"
      ],
      "metadata": {
        "id": "pUKK8RJuANwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSf_qz4xANwj"
      },
      "outputs": [],
      "source": [
        "class MM_Bi_LSTM(nn.Module):\n",
        "    def __init__(self, input_acc, input_gyr,drop_prob=0.0):\n",
        "        super(MM_Bi_LSTM, self).__init__()\n",
        "\n",
        "        self.encoder_acc=Encoder(input_acc, drop_prob)\n",
        "        self.encoder_gyr=Encoder(input_gyr, drop_prob)\n",
        "\n",
        "        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)\n",
        "        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)\n",
        "\n",
        "        self.fc = nn.Linear(2*128, 6*9)\n",
        "\n",
        "        self.flatten=nn.Flatten()\n",
        "\n",
        "    def forward(self, x_acc, x_gyr):\n",
        "\n",
        "        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))\n",
        "        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))\n",
        "\n",
        "        x_acc_1=self.BN_acc(x_acc_1)\n",
        "        x_gyr_1=self.BN_gyr(x_gyr_1)\n",
        "\n",
        "        x_acc_2=x_acc_1.view(-1, window, x_acc_1.size(-1))\n",
        "        x_gyr_2=x_gyr_1.view(-1, window, x_gyr_1.size(-1))\n",
        "\n",
        "        x_acc,(h_acc,c_acc)=self.encoder_acc(x_acc_2)\n",
        "        x_gyr,(h_gyr,c_gyr)=self.encoder_gyr(x_gyr_2)\n",
        "\n",
        "        x=torch.cat((x_acc[:,-1,:],x_gyr[:,-1,:]),dim=-1)\n",
        "\n",
        "        out=self.fc(x).view(x.shape[0],9,6)\n",
        "\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "764652d0-7440-4958-f839-951a42ee4ddb",
        "id": "NVFmuKCeANwj"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, time: 26.3256, Training Loss: 7.5454,  Validation loss: 3.7170\n",
            "Epoch: 2, time: 24.9096, Training Loss: 3.3699,  Validation loss: 2.8739\n",
            "Epoch: 3, time: 24.2866, Training Loss: 2.8391,  Validation loss: 2.5773\n",
            "Epoch: 4, time: 24.4610, Training Loss: 2.5742,  Validation loss: 2.3981\n",
            "Epoch: 5, time: 24.3517, Training Loss: 2.4087,  Validation loss: 2.2489\n",
            "Epoch: 6, time: 24.2792, Training Loss: 2.2874,  Validation loss: 2.1653\n",
            "Epoch: 7, time: 24.3256, Training Loss: 2.1986,  Validation loss: 2.1251\n",
            "Epoch: 8, time: 24.3111, Training Loss: 2.1258,  Validation loss: 2.0589\n",
            "Epoch: 9, time: 24.4949, Training Loss: 2.0617,  Validation loss: 1.9901\n",
            "Epoch: 10, time: 24.3022, Training Loss: 2.0147,  Validation loss: 2.0025\n",
            "Epoch: 11, time: 24.2897, Training Loss: 1.9680,  Validation loss: 1.9524\n",
            "Epoch: 12, time: 24.3360, Training Loss: 1.9267,  Validation loss: 1.9240\n",
            "Training time: 294.76104855537415 seconds\n"
          ]
        }
      ],
      "source": [
        "lr = 0.001\n",
        "model = MM_Bi_LSTM(24,24)\n",
        "\n",
        "mm_bi_lstm = train_mm_m(train_loader, lr,12,model,path+subject+'_mm_bi_lstm_IMU8.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36625a42-a39b-4b8f-dc2e-66fd649c9386",
        "id": "ouQ-AxBoANwk"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a768166-fb70-4e57-c702-277b09583276",
        "id": "z-NjpR1AANwk"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(18718, 9, 6)\n",
            "4.897123\n",
            "5.4377394\n",
            "3.685829\n",
            "3.207719\n",
            "4.366973\n",
            "3.3581257\n",
            "\n",
            "\n",
            "0.9766756421789765\n",
            "0.9795538391731456\n",
            "0.9493933482693394\n",
            "0.9846847255215011\n",
            "0.9847822000448125\n",
            "0.950157247857943\n",
            "Mean: 4.159 +/- 0.696\n",
            "Mean: 0.971 +/- 0.018\n"
          ]
        }
      ],
      "source": [
        "mm_bi_lstm= MM_Bi_LSTM(24,24)\n",
        "mm_bi_lstm.load_state_dict(torch.load(path+subject+'_mm_bi_lstm_IMU8.pth'))\n",
        "mm_bi_lstm.to(device)\n",
        "\n",
        "mm_bi_lstm.eval()\n",
        "\n",
        "# iterate through batches of test data\n",
        "with torch.no_grad():\n",
        "    for i, (data, data_acc, data_gyr, data_hof, target_present, target_future) in enumerate(test_loader):\n",
        "        output = mm_bi_lstm(data_acc.to(device).float(),data_gyr.to(device).float())\n",
        "        if i==0:\n",
        "          yhat_5=output\n",
        "          test_target=target_future\n",
        "\n",
        "        yhat_5=torch.cat((yhat_5,output),dim=0)\n",
        "        test_target=torch.cat((test_target,target_future),dim=0)\n",
        "\n",
        "        # clear memory\n",
        "        del data, target_future,output\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "\n",
        "yhat_4 = yhat_5.detach().cpu().numpy()\n",
        "test_target = test_target.detach().cpu().numpy()\n",
        "print(yhat_4.shape)\n",
        "\n",
        "rmse, p=RMSE_prediction(yhat_4,test_target)\n",
        "\n",
        "ablation_3=np.hstack([rmse,p])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Bi-GRU"
      ],
      "metadata": {
        "id": "pqBJRVLLANwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.lstm_1 = nn.GRU(input_dim, 128, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.lstm_2 = nn.GRU(256, 64, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_1, _ = self.lstm_1(x)\n",
        "        out_1=self.dropout(out_1)\n",
        "        out_2, h_n = self.lstm_2(out_1)\n",
        "        out_2=self.dropout(out_2)\n",
        "\n",
        "        return out_2, h_n"
      ],
      "metadata": {
        "id": "U4EC8OFWANwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4WSLw70ANwk"
      },
      "outputs": [],
      "source": [
        "class MM_Bi_GRU(nn.Module):\n",
        "    def __init__(self, input_acc, input_gyr, drop_prob=0.0):\n",
        "        super(MM_Bi_GRU, self).__init__()\n",
        "\n",
        "        self.encoder_acc=Encoder(input_acc, drop_prob)\n",
        "        self.encoder_gyr=Encoder(input_gyr, drop_prob)\n",
        "\n",
        "        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)\n",
        "        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)\n",
        "\n",
        "        self.fc = nn.Linear(2*128, 6*9)\n",
        "\n",
        "        self.flatten=nn.Flatten()\n",
        "\n",
        "    def forward(self, x_acc, x_gyr):\n",
        "\n",
        "        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))\n",
        "        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))\n",
        "\n",
        "        x_acc_1=self.BN_acc(x_acc_1)\n",
        "        x_gyr_1=self.BN_gyr(x_gyr_1)\n",
        "\n",
        "        x_acc_2=x_acc_1.view(-1, window, x_acc_1.size(-1))\n",
        "        x_gyr_2=x_gyr_1.view(-1, window, x_gyr_1.size(-1))\n",
        "\n",
        "        x_acc,h_acc=self.encoder_acc(x_acc_2)\n",
        "        x_gyr,h_gyr=self.encoder_gyr(x_gyr_2)\n",
        "\n",
        "        x=torch.cat((x_acc[:,-1,:],x_gyr[:,-1,:]),dim=-1)\n",
        "\n",
        "        out=self.fc(x).view(x.shape[0],9,6)\n",
        "\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8330d84b-d0d9-419c-eb8b-90238ec7a688",
        "id": "aXPxh7m8ANwk"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, time: 21.9325, Training Loss: 6.2825,  Validation loss: 3.3304\n",
            "Epoch: 2, time: 19.1714, Training Loss: 3.0807,  Validation loss: 2.6021\n",
            "Epoch: 3, time: 18.8630, Training Loss: 2.6644,  Validation loss: 2.4436\n",
            "Epoch: 4, time: 18.7997, Training Loss: 2.4580,  Validation loss: 2.2838\n",
            "Epoch: 5, time: 18.8255, Training Loss: 2.3302,  Validation loss: 2.1997\n",
            "Epoch: 6, time: 19.1889, Training Loss: 2.2315,  Validation loss: 2.1062\n",
            "Epoch: 7, time: 18.9042, Training Loss: 2.1579,  Validation loss: 2.0622\n",
            "Epoch: 8, time: 18.8941, Training Loss: 2.0977,  Validation loss: 2.0760\n",
            "Epoch: 9, time: 18.7655, Training Loss: 2.0523,  Validation loss: 2.0801\n",
            "Epoch: 10, time: 18.8809, Training Loss: 1.9990,  Validation loss: 1.9858\n",
            "Training time: 192.2840118408203 seconds\n"
          ]
        }
      ],
      "source": [
        "lr = 0.001\n",
        "model = MM_Bi_GRU(24,24)\n",
        "\n",
        "mm_bi_gru = train_mm_m(train_loader, lr,10,model,path+subject+'_mm_bi_gru_IMU8.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87d16482-dbaf-4656-8a4c-c3244ba997a5",
        "id": "aITcjvvDANwk"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe5cdd02-a4a3-46e9-860e-0ef75ede2652",
        "id": "8xPME_y_ANwl"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(18718, 9, 6)\n",
            "4.7923265\n",
            "5.473757\n",
            "3.7359457\n",
            "3.1008954\n",
            "4.045067\n",
            "3.268089\n",
            "\n",
            "\n",
            "0.9778082203108445\n",
            "0.9797034904015585\n",
            "0.9481096368958415\n",
            "0.9845953764224039\n",
            "0.9855138138144034\n",
            "0.9491898059775877\n",
            "Mean: 4.069 +/- 0.638\n",
            "Mean: 0.971 +/- 0.019\n"
          ]
        }
      ],
      "source": [
        "mm_bi_gru= MM_Bi_GRU(24,24)\n",
        "mm_bi_gru.load_state_dict(torch.load(path+subject+'_mm_bi_gru_IMU8.pth'))\n",
        "mm_bi_gru.to(device)\n",
        "\n",
        "mm_bi_gru.eval()\n",
        "\n",
        "# iterate through batches of test data\n",
        "with torch.no_grad():\n",
        "    for i, (data, data_acc, data_gyr, data_hof, target_present, target_future) in enumerate(test_loader):\n",
        "        output = mm_bi_gru(data_acc.to(device).float(),data_gyr.to(device).float())\n",
        "        if i==0:\n",
        "          yhat_5=output\n",
        "          test_target=target_future\n",
        "\n",
        "        yhat_5=torch.cat((yhat_5,output),dim=0)\n",
        "        test_target=torch.cat((test_target,target_future),dim=0)\n",
        "\n",
        "        # clear memory\n",
        "        del data, target_future,output\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "\n",
        "yhat_4 = yhat_5.detach().cpu().numpy()\n",
        "test_target = test_target.detach().cpu().numpy()\n",
        "print(yhat_4.shape)\n",
        "\n",
        "rmse, p=RMSE_prediction(yhat_4,test_target)\n",
        "\n",
        "ablation_4=np.hstack([rmse,p])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SU8M0091ANwl"
      },
      "source": [
        "## 5. LSTM --Encoder+decoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.lstm_1 = nn.LSTM(input_dim, 128, bidirectional=False, batch_first=True, dropout=0.0)\n",
        "        self.lstm_2 = nn.LSTM(128, 64, bidirectional=False, batch_first=True, dropout=0.0)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_1, _ = self.lstm_1(x)\n",
        "        out_1=self.dropout(out_1)\n",
        "        out_2, (h_n, c_n) = self.lstm_2(out_1)\n",
        "        out_2=self.dropout(out_2)\n",
        "\n",
        "        return out_2, (h_n, c_n)"
      ],
      "metadata": {
        "id": "195qanO1ANwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMDecoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout):\n",
        "        super(LSTMDecoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, bidirectional=False, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, encoder_hidden, encoder_cell,  max_len):\n",
        "        batch_size = encoder_hidden.shape[1]\n",
        "        hidden = encoder_hidden\n",
        "        cell = encoder_cell\n",
        "        outputs = []\n",
        "\n",
        "        # Use the last time step of target as the initial input\n",
        "        input = torch.zeros(batch_size,1,6).to(device)\n",
        "\n",
        "\n",
        "        for i in range(max_len):\n",
        "            # Run one time step of LSTM\n",
        "            output, (hidden, cell) = self.lstm(input, (hidden, cell))\n",
        "            output=self.dropout(output)\n",
        "\n",
        "            # Use the output for prediction\n",
        "            output = self.fc(output.squeeze(1))\n",
        "            outputs.append(output.unsqueeze(1))\n",
        "\n",
        "            # Use the predicted output as the next input\n",
        "            input = output.unsqueeze(1)\n",
        "\n",
        "\n",
        "        # Concatenate all the outputs along the time dimension\n",
        "        outputs = torch.cat(outputs, dim=1)\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "RcLwIbmgANwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vGrbKY7ANwl"
      },
      "outputs": [],
      "source": [
        "class MM_ED_LSTM(nn.Module):\n",
        "    def __init__(self, input_acc, input_gyr, drop_prob=0.0):\n",
        "        super(MM_ED_LSTM, self).__init__()\n",
        "\n",
        "        self.encoder_acc=Encoder(input_acc, drop_prob)\n",
        "        self.encoder_gyr=Encoder(input_gyr, drop_prob)\n",
        "\n",
        "        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)\n",
        "        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)\n",
        "\n",
        "        self.decoder=LSTMDecoder(6, 2*64, 6, 1, 0.0)\n",
        "\n",
        "    def forward(self, x_acc, x_gyr):\n",
        "\n",
        "        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))\n",
        "        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))\n",
        "\n",
        "        x_acc_1=self.BN_acc(x_acc_1)\n",
        "        x_gyr_1=self.BN_gyr(x_gyr_1)\n",
        "\n",
        "        x_acc_2=x_acc_1.view(-1, window, x_acc_1.size(-1))\n",
        "        x_gyr_2=x_gyr_1.view(-1, window, x_gyr_1.size(-1))\n",
        "\n",
        "        x_acc,(h_acc,c_acc)=self.encoder_acc(x_acc_2)\n",
        "        x_gyr,(h_gyr,c_gyr)=self.encoder_gyr(x_gyr_2)\n",
        "\n",
        "\n",
        "        h=torch.cat((h_acc,h_gyr),dim=-1)\n",
        "        c=torch.cat((c_acc,c_gyr),dim=-1)\n",
        "\n",
        "        out=self.decoder(h, c, 9)\n",
        "\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "40ccc7aa-b661-43e9-d730-1caae4149559",
        "id": "rrV_PoLCANwm"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, time: 30.9576, Training Loss: 12.1812,  Validation loss: 5.2825\n",
            "Epoch: 2, time: 28.1177, Training Loss: 4.3589,  Validation loss: 3.5202\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-26b08840145a>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMM_ED_LSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmm_ed_lstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_mm_m\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_mm_ed_lstm_IMU8.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-71b1fe5884af>\u001b[0m in \u001b[0;36mtrain_mm_m\u001b[0;34m(train_loader, learn_rate, EPOCHS, model, filename)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "lr = 0.001\n",
        "model = MM_ED_LSTM(24,24)\n",
        "\n",
        "mm_ed_lstm = train_mm_m(train_loader, lr,10,model,path+subject+'_mm_ed_lstm_IMU8.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ca37e72-ab8a-4770-ebdd-e544fb409f65",
        "id": "GC6-6e4TANwm"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d232bb92-555f-4c7a-af14-151e3bb944c6",
        "id": "x0QILcrgANwm"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(18718, 9, 6)\n",
            "5.477536\n",
            "6.237811\n",
            "3.873354\n",
            "3.906839\n",
            "6.246587\n",
            "4.1573277\n",
            "\n",
            "\n",
            "0.9690679259837184\n",
            "0.97247434547719\n",
            "0.9078764856155249\n",
            "0.975965672818167\n",
            "0.9730347706579593\n",
            "0.9062692013911055\n",
            "Mean: 4.983 +/- 1.142\n",
            "Mean: 0.951 +/- 0.038\n"
          ]
        }
      ],
      "source": [
        "mm_ed_lstm= MM_ED_LSTM(24,24)\n",
        "mm_ed_lstm.load_state_dict(torch.load(path+subject+'_mm_ed_lstm_IMU8.pth'))\n",
        "mm_ed_lstm.to(device)\n",
        "\n",
        "mm_ed_lstm.eval()\n",
        "\n",
        "# iterate through batches of test data\n",
        "with torch.no_grad():\n",
        "    for i, (data, data_acc, data_gyr, data_hof, target_present, target_future) in enumerate(test_loader):\n",
        "        output = mm_ed_lstm(data_acc.to(device).float(),data_gyr.to(device).float())\n",
        "        if i==0:\n",
        "          yhat_5=output\n",
        "          test_target=target_future\n",
        "\n",
        "        yhat_5=torch.cat((yhat_5,output),dim=0)\n",
        "        test_target=torch.cat((test_target,target_future),dim=0)\n",
        "\n",
        "        # clear memory\n",
        "        del data, target_future,output\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "\n",
        "yhat_4 = yhat_5.detach().cpu().numpy()\n",
        "test_target = test_target.detach().cpu().numpy()\n",
        "print(yhat_4.shape)\n",
        "\n",
        "rmse, p=RMSE_prediction(yhat_4,test_target)\n",
        "\n",
        "ablation_5=np.hstack([rmse,p])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bstAWR6xANwm"
      },
      "source": [
        "## 6. GRU --Encoder+decoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.lstm_1 = nn.GRU(input_dim, 128, bidirectional=False, batch_first=True, dropout=0.0)\n",
        "        self.lstm_2 = nn.GRU(128, 64, bidirectional=False, batch_first=True, dropout=0.0)\n",
        "        self.flatten=nn.Flatten()\n",
        "        self.fc = nn.Linear(128, 32)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_1, _ = self.lstm_1(x)\n",
        "        out_1=self.dropout(out_1)\n",
        "        out_2, h_n = self.lstm_2(out_1)\n",
        "        out_2=self.dropout(out_2)\n",
        "\n",
        "        return out_2, h_n"
      ],
      "metadata": {
        "id": "PtkXU8e0ANwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMDecoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers,dropout):\n",
        "        super(LSTMDecoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.GRU(input_size, hidden_size, num_layers, bidirectional=False, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, encoder_hidden, max_len):\n",
        "        batch_size = encoder_hidden.shape[1]\n",
        "        hidden = encoder_hidden\n",
        "        # cell = encoder_cell\n",
        "        outputs = []\n",
        "\n",
        "        # Use the last time step of target as the initial input\n",
        "\n",
        "        input = torch.zeros(batch_size,1,6).to(device)\n",
        "\n",
        "\n",
        "        for i in range(max_len):\n",
        "\n",
        "\n",
        "            # Run one time step of LSTM\n",
        "            output, hidden = self.lstm(input, hidden)\n",
        "            output=self.dropout(output)\n",
        "\n",
        "            # Use the output for prediction\n",
        "            output = self.fc(output.squeeze(1))\n",
        "            outputs.append(output.unsqueeze(1))\n",
        "\n",
        "            # Use the predicted output as the next input\n",
        "            input = output.unsqueeze(1)\n",
        "\n",
        "\n",
        "        # Concatenate all the outputs along the time dimension\n",
        "        outputs = torch.cat(outputs, dim=1)\n",
        "\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "NLexVoM5ANwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCPkz24hANwm"
      },
      "outputs": [],
      "source": [
        "class MM_ED_GRU(nn.Module):\n",
        "    def __init__(self, input_acc, input_gyr, drop_prob=0.0):\n",
        "        super(MM_ED_GRU, self).__init__()\n",
        "\n",
        "        self.encoder_acc=Encoder(input_acc, drop_prob)\n",
        "        self.encoder_gyr=Encoder(input_gyr, drop_prob)\n",
        "\n",
        "        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)\n",
        "        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)\n",
        "\n",
        "        self.decoder=LSTMDecoder(6, 2*64, 6, 1, 0.0)\n",
        "\n",
        "    def forward(self, x_acc, x_gyr):\n",
        "\n",
        "        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))\n",
        "        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))\n",
        "\n",
        "        x_acc_1=self.BN_acc(x_acc_1)\n",
        "        x_gyr_1=self.BN_gyr(x_gyr_1)\n",
        "\n",
        "        x_acc_2=x_acc_1.view(-1, window, x_acc_1.size(-1))\n",
        "        x_gyr_2=x_gyr_1.view(-1, window, x_gyr_1.size(-1))\n",
        "\n",
        "        x_acc, h_acc=self.encoder_acc(x_acc_2)\n",
        "        x_gyr, h_gyr=self.encoder_gyr(x_gyr_2)\n",
        "\n",
        "        h=torch.cat((h_acc,h_gyr),dim=-1)\n",
        "\n",
        "        out=self.decoder(h,9)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "a6041cfa-f943-4c48-d241-a0cb90e7967a",
        "id": "vDpQSRJ-ANwm"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, time: 29.5520, Training Loss: 8.6695,  Validation loss: 4.7768\n",
            "Epoch: 2, time: 27.5437, Training Loss: 4.1428,  Validation loss: 3.3991\n",
            "Epoch: 3, time: 29.0977, Training Loss: 3.2683,  Validation loss: 2.9795\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-85d5c4e9a874>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMM_ED_GRU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmm_ed_gru\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtrain_mm_m\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_mm_ed_gru_IMU8.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-71b1fe5884af>\u001b[0m in \u001b[0;36mtrain_mm_m\u001b[0;34m(train_loader, learn_rate, EPOCHS, model, filename)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "lr = 0.001\n",
        "model = MM_ED_GRU(24,24)\n",
        "\n",
        "mm_ed_gru= train_mm_m(train_loader, lr,10,model,path+subject+'_mm_ed_gru_IMU8.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a47978d0-de5c-4eee-bafb-8840ec5575df",
        "id": "ukWqtbbhANwn"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f8707e4-2ead-49da-b3af-c05ca4830872",
        "id": "5lzu8AI5ANwn"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(18718, 9, 6)\n",
            "5.196075\n",
            "7.3994856\n",
            "4.261845\n",
            "3.4596195\n",
            "4.9493504\n",
            "3.675685\n",
            "\n",
            "\n",
            "0.9742479566970041\n",
            "0.9699994137085569\n",
            "0.920816103201719\n",
            "0.9799577124414494\n",
            "0.9801049101483387\n",
            "0.9290781175562804\n",
            "Mean: 4.824 +/- 1.183\n",
            "Mean: 0.959 +/- 0.030\n"
          ]
        }
      ],
      "source": [
        "mm_ed_gru= MM_ED_GRU(24,24)\n",
        "mm_ed_gru.load_state_dict(torch.load(path+subject+'_mm_ed_gru_IMU8.pth'))\n",
        "mm_ed_gru.to(device)\n",
        "\n",
        "mm_ed_gru.eval()\n",
        "\n",
        "# iterate through batches of test data\n",
        "with torch.no_grad():\n",
        "    for i, (data, data_acc, data_gyr, data_hof, target_present, target_future) in enumerate(test_loader):\n",
        "        output = mm_ed_gru(data_acc.to(device).float(),data_gyr.to(device).float())\n",
        "        if i==0:\n",
        "          yhat_5=output\n",
        "          test_target=target_future\n",
        "\n",
        "        yhat_5=torch.cat((yhat_5,output),dim=0)\n",
        "        test_target=torch.cat((test_target,target_future),dim=0)\n",
        "\n",
        "        # clear memory\n",
        "        del data, target_future,output\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "\n",
        "yhat_4 = yhat_5.detach().cpu().numpy()\n",
        "test_target = test_target.detach().cpu().numpy()\n",
        "print(yhat_4.shape)\n",
        "\n",
        "rmse, p=RMSE_prediction(yhat_4,test_target)\n",
        "\n",
        "ablation_6=np.hstack([rmse,p])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWDohhh1ANwn"
      },
      "source": [
        "## 7. Bi-LSTM --Encoder+decoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.lstm_1 = nn.LSTM(input_dim, 128, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.lstm_2 = nn.LSTM(256, 64, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_1, _ = self.lstm_1(x)\n",
        "        out_1=self.dropout(out_1)\n",
        "        out_2, (h_n, c_n) = self.lstm_2(out_1)\n",
        "        out_2=self.dropout(out_2)\n",
        "\n",
        "        return out_2, (h_n, c_n)"
      ],
      "metadata": {
        "id": "1FJOhKitANwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMDecoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout):\n",
        "        super(LSTMDecoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, bidirectional=True, batch_first=True)\n",
        "        self.fc = nn.Linear(2*hidden_size, output_size)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, encoder_hidden, encoder_cell,  max_len):\n",
        "        batch_size = encoder_hidden.shape[1]\n",
        "        hidden = encoder_hidden\n",
        "        cell = encoder_cell\n",
        "        outputs = []\n",
        "\n",
        "        # Use the last time step of target as the initial input\n",
        "        input = torch.zeros(batch_size,1,6).to(device)\n",
        "\n",
        "\n",
        "        for i in range(max_len):\n",
        "            # Run one time step of LSTM\n",
        "            output, (hidden, cell) = self.lstm(input, (hidden, cell))\n",
        "            output=self.dropout(output)\n",
        "\n",
        "            # Use the output for prediction\n",
        "            output = self.fc(output.squeeze(1))\n",
        "            outputs.append(output.unsqueeze(1))\n",
        "\n",
        "            # Use the predicted output as the next input\n",
        "            input = output.unsqueeze(1)\n",
        "\n",
        "\n",
        "        # Concatenate all the outputs along the time dimension\n",
        "        outputs = torch.cat(outputs, dim=1)\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "LOaxHxPuANwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUF80sZPANwo"
      },
      "outputs": [],
      "source": [
        "class MM_ED_Bi_LSTM(nn.Module):\n",
        "    def __init__(self, input_acc, input_gyr, drop_prob=0.0):\n",
        "        super(MM_ED_Bi_LSTM, self).__init__()\n",
        "\n",
        "        self.encoder_acc=Encoder(input_acc, drop_prob)\n",
        "        self.encoder_gyr=Encoder(input_gyr, drop_prob)\n",
        "\n",
        "        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)\n",
        "        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)\n",
        "\n",
        "        self.decoder=LSTMDecoder(6, 2*64, 6, 1, 0.05)\n",
        "\n",
        "    def forward(self, x_acc, x_gyr):\n",
        "\n",
        "        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))\n",
        "        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))\n",
        "\n",
        "        x_acc_1=self.BN_acc(x_acc_1)\n",
        "        x_gyr_1=self.BN_gyr(x_gyr_1)\n",
        "\n",
        "        x_acc_2=x_acc_1.view(-1, window, x_acc_1.size(-1))\n",
        "        x_gyr_2=x_gyr_1.view(-1, window, x_gyr_1.size(-1))\n",
        "\n",
        "        x_acc,(h_acc,c_acc)=self.encoder_acc(x_acc_2)\n",
        "        x_gyr,(h_gyr,c_gyr)=self.encoder_gyr(x_gyr_2)\n",
        "\n",
        "        h=torch.cat((h_acc,h_gyr),dim=-1)\n",
        "        c=torch.cat((c_acc,c_gyr),dim=-1)\n",
        "\n",
        "        out=self.decoder(h, c, 9)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "cda20444-73db-42b6-fb23-fa816e665b6d",
        "id": "OdN-mCwWANwo"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, time: 44.1782, Training Loss: 7.3852,  Validation loss: 3.5165\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-63fc4572c10b>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMM_ED_Bi_LSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmm_ed_bi_lstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_mm_m\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_mm_ed_bi_lstm_IMU8.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-71b1fe5884af>\u001b[0m in \u001b[0;36mtrain_mm_m\u001b[0;34m(train_loader, learn_rate, EPOCHS, model, filename)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "lr = 0.001\n",
        "model = MM_ED_Bi_LSTM(24,24)\n",
        "\n",
        "mm_ed_bi_lstm = train_mm_m(train_loader, lr,12,model,path+subject+'_mm_ed_bi_lstm_IMU8.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbe05e8b-0475-4a9d-8c18-713acba22796",
        "id": "weQab5CBANwo"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ],
      "source": [
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ef6269e-da05-483e-8fc4-ed1990602f17",
        "id": "fy4XitzWANwo"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(18718, 9, 6)\n",
            "6.475506\n",
            "8.072348\n",
            "4.3942847\n",
            "4.035379\n",
            "5.387628\n",
            "4.1430855\n",
            "\n",
            "\n",
            "0.96477368830328\n",
            "0.9601144357511544\n",
            "0.8996115086202288\n",
            "0.9776036548763187\n",
            "0.9744174088089231\n",
            "0.9029811951522773\n",
            "Mean: 5.418 +/- 1.239\n",
            "Mean: 0.947 +/- 0.039\n"
          ]
        }
      ],
      "source": [
        "mm_ed_bi_lstm= MM_ED_Bi_LSTM(24,24)\n",
        "mm_ed_bi_lstm.load_state_dict(torch.load(path+subject+'_mm_ed_bi_lstm_IMU8.pth'))\n",
        "mm_ed_bi_lstm.to(device)\n",
        "\n",
        "mm_ed_bi_lstm.eval()\n",
        "\n",
        "# iterate through batches of test data\n",
        "with torch.no_grad():\n",
        "    for i, (data, data_acc, data_gyr, data_hof, target_present, target_future) in enumerate(test_loader):\n",
        "        output = mm_ed_bi_lstm(data_acc.to(device).float(),data_gyr.to(device).float())\n",
        "        if i==0:\n",
        "          yhat_5=output\n",
        "          test_target=target_future\n",
        "\n",
        "        yhat_5=torch.cat((yhat_5,output),dim=0)\n",
        "        test_target=torch.cat((test_target,target_future),dim=0)\n",
        "\n",
        "        # clear memory\n",
        "        del data, target_future,output\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "yhat_4 = yhat_5.detach().cpu().numpy()\n",
        "test_target = test_target.detach().cpu().numpy()\n",
        "print(yhat_4.shape)\n",
        "\n",
        "rmse, p=RMSE_prediction(yhat_4,test_target)\n",
        "\n",
        "ablation_7=np.hstack([rmse,p])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBd8FBNKANwo"
      },
      "source": [
        "## 8. Bi-GRU --Encoder+decoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.lstm_1 = nn.GRU(input_dim, 128, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.lstm_2 = nn.GRU(256, 64, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.flatten=nn.Flatten()\n",
        "        self.fc = nn.Linear(128, 32)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_1, _ = self.lstm_1(x)\n",
        "        out_1=self.dropout(out_1)\n",
        "        out_2, h_n = self.lstm_2(out_1)\n",
        "        out_2=self.dropout(out_2)\n",
        "\n",
        "        return out_2, h_n"
      ],
      "metadata": {
        "id": "azSlP9JuANwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMDecoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers,dropout):\n",
        "        super(LSTMDecoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.GRU(input_size, hidden_size, num_layers, bidirectional=True, batch_first=True)\n",
        "        self.fc = nn.Linear(2*hidden_size, output_size)\n",
        "\n",
        "\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, encoder_hidden, max_len):\n",
        "        batch_size = encoder_hidden.shape[1]\n",
        "        hidden = encoder_hidden\n",
        "        # cell = encoder_cell\n",
        "        outputs = []\n",
        "\n",
        "        # Use the last time step of target as the initial input\n",
        "\n",
        "        input = torch.zeros(batch_size,1,6).to(device)\n",
        "\n",
        "        for i in range(max_len):\n",
        "\n",
        "            # Run one time step of LSTM\n",
        "            output, hidden = self.lstm(input, hidden)\n",
        "            output=self.dropout(output)\n",
        "\n",
        "            # Use the output for prediction\n",
        "            output = self.fc(output.squeeze(1))\n",
        "            outputs.append(output.unsqueeze(1))\n",
        "\n",
        "            # Use the predicted output as the next input\n",
        "            input = output.unsqueeze(1)\n",
        "\n",
        "\n",
        "        # Concatenate all the outputs along the time dimension\n",
        "        outputs = torch.cat(outputs, dim=1)\n",
        "\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "yGA0bF7xANwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GBPimETANwp"
      },
      "outputs": [],
      "source": [
        "class MM_ED_Bi_GRU(nn.Module):\n",
        "    def __init__(self, input_acc, input_gyr, drop_prob=0.0):\n",
        "        super(MM_ED_Bi_GRU, self).__init__()\n",
        "\n",
        "        self.encoder_acc=Encoder(input_acc, drop_prob)\n",
        "        self.encoder_gyr=Encoder(input_gyr, drop_prob)\n",
        "\n",
        "        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)\n",
        "        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)\n",
        "\n",
        "        self.decoder=LSTMDecoder(6, 2*64, 6, 1, 0.05)\n",
        "\n",
        "    def forward(self, x_acc, x_gyr):\n",
        "\n",
        "        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))\n",
        "        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))\n",
        "\n",
        "        x_acc_1=self.BN_acc(x_acc_1)\n",
        "        x_gyr_1=self.BN_gyr(x_gyr_1)\n",
        "\n",
        "        x_acc_2=x_acc_1.view(-1, window, x_acc_1.size(-1))\n",
        "        x_gyr_2=x_gyr_1.view(-1, window, x_gyr_1.size(-1))\n",
        "\n",
        "        x_acc, h_acc=self.encoder_acc(x_acc_2)\n",
        "        x_gyr, h_gyr=self.encoder_gyr(x_gyr_2)\n",
        "\n",
        "        h=torch.cat((h_acc,h_gyr),dim=-1)\n",
        "\n",
        "        out=self.decoder(h,9)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "4acc4fd1-06bb-4bac-d839-8c60bbf3f26b",
        "id": "EtjpBXQDANwp"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, time: 39.1136, Training Loss: 6.2370,  Validation loss: 3.1408\n",
            "Epoch: 2, time: 35.2314, Training Loss: 3.1377,  Validation loss: 2.6061\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-47811497556d>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMM_ED_Bi_GRU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmm_ed_Bi_GRU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_mm_m\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_mm_bi_ed_gru_IMU8.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-71b1fe5884af>\u001b[0m in \u001b[0;36mtrain_mm_m\u001b[0;34m(train_loader, learn_rate, EPOCHS, model, filename)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_gyr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_hof\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_present\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_future\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_gyr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-87-30949948e229>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_acc, x_gyr)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mx_gyr_2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_gyr_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_gyr_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mx_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_acc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_acc_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mx_gyr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_gyr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_gyr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_gyr_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-85-6762f01b0f3b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mout_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mout_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mout_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    996\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 998\u001b[0;31m             result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    999\u001b[0m                              self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m   1000\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "lr = 0.001\n",
        "model = MM_ED_Bi_GRU(24,24)\n",
        "\n",
        "mm_ed_Bi_GRU = train_mm_m(train_loader, lr,15,model,path+subject+'_mm_bi_ed_gru_IMU8.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "808f27d6-4947-40d5-85be-925a07ed5040",
        "id": "LyPjYcMTANwp"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ],
      "source": [
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4881f38c-04ad-48f1-c160-0452e842c1d4",
        "id": "uRBXP4o4ANwp"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(18718, 9, 6)\n",
            "5.2798133\n",
            "5.933696\n",
            "3.4630857\n",
            "3.510238\n",
            "4.461637\n",
            "3.871863\n",
            "\n",
            "\n",
            "0.9710445927573607\n",
            "0.9770033015595392\n",
            "0.942428624132805\n",
            "0.9809081665108047\n",
            "0.98220245817604\n",
            "0.9376767941159204\n",
            "Mean: 4.420 +/- 0.765\n",
            "Mean: 0.965 +/- 0.022\n"
          ]
        }
      ],
      "source": [
        "mm_ed_Bi_GRU= MM_ED_Bi_GRU(24,24)\n",
        "mm_ed_Bi_GRU.load_state_dict(torch.load(path+subject+'_mm_bi_ed_gru_IMU8.pth'))\n",
        "mm_ed_Bi_GRU.to(device)\n",
        "\n",
        "mm_ed_Bi_GRU.eval()\n",
        "\n",
        "# iterate through batches of test data\n",
        "with torch.no_grad():\n",
        "    for i, (data, data_acc, data_gyr, data_hof, target_present, target_future) in enumerate(test_loader):\n",
        "        output = mm_ed_Bi_GRU(data_acc.to(device).float(),data_gyr.to(device).float())\n",
        "        if i==0:\n",
        "          yhat_5=output\n",
        "          test_target=target_future\n",
        "\n",
        "        yhat_5=torch.cat((yhat_5,output),dim=0)\n",
        "        test_target=torch.cat((test_target,target_future),dim=0)\n",
        "\n",
        "        # clear memory\n",
        "        del data, target_future,output\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "\n",
        "yhat_4 = yhat_5.detach().cpu().numpy()\n",
        "test_target = test_target.detach().cpu().numpy()\n",
        "print(yhat_4.shape)\n",
        "\n",
        "rmse, p=RMSE_prediction(yhat_4,test_target)\n",
        "\n",
        "ablation_8=np.hstack([rmse,p])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qkc7GwSIANwp"
      },
      "source": [
        "## 9. Attention Without gating+ Bi-GRU --Encoder+decoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.lstm_1 = nn.GRU(input_dim, 128, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.lstm_2 = nn.GRU(256, 64, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.flatten=nn.Flatten()\n",
        "        self.fc = nn.Linear(128, 32)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_1, _ = self.lstm_1(x)\n",
        "        out_1=self.dropout(out_1)\n",
        "        out_2, h_n = self.lstm_2(out_1)\n",
        "        out_2=self.dropout(out_2)\n",
        "\n",
        "        return out_2, h_n"
      ],
      "metadata": {
        "id": "k0e5F-cfANwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMDecoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers,dropout):\n",
        "        super(LSTMDecoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.GRU(input_size, hidden_size, num_layers, bidirectional=True, batch_first=True)\n",
        "        self.fc = nn.Linear(2*hidden_size, output_size)\n",
        "        self.gating_net = nn.Sequential(\n",
        "            nn.Linear(1542, 1542),\n",
        "            nn.Sigmoid())\n",
        "\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, encoder_hidden, h_att, max_len):\n",
        "        batch_size = encoder_hidden.shape[1]\n",
        "        hidden = encoder_hidden\n",
        "        # cell = encoder_cell\n",
        "        outputs = []\n",
        "\n",
        "        # Use the last time step of target as the initial input\n",
        "        # input = target_estimation[:, -1, :].unsqueeze(1)\n",
        "\n",
        "        input_1 = torch.zeros(batch_size,1,6).to(device)\n",
        "        input = torch.cat((input_1, h_att), dim=-1)\n",
        "\n",
        "        # print(input.shape)\n",
        "\n",
        "\n",
        "        for i in range(max_len):\n",
        "\n",
        "            # gating_weight=self.gating_net(input)\n",
        "            # input=input*gating_weight\n",
        "\n",
        "            # Run one time step of LSTM\n",
        "            output, hidden = self.lstm(input, hidden)\n",
        "            output=self.dropout(output)\n",
        "\n",
        "            # Use the output for prediction\n",
        "            output = self.fc(output.squeeze(1))\n",
        "            outputs.append(output.unsqueeze(1))\n",
        "\n",
        "            input = torch.cat(( output.unsqueeze(1),input[:,:,6:262]), dim=-1)\n",
        "\n",
        "\n",
        "\n",
        "        # Concatenate all the outputs along the time dimension\n",
        "        outputs = torch.cat(outputs, dim=1)\n",
        "\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "S1PQzNL3ANwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TemporalAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(TemporalAttention, self).__init__()\n",
        "\n",
        "        self.W = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, sequence_length, hidden_size)\n",
        "\n",
        "        # Calculate attention weights.\n",
        "        attn = self.V(torch.tanh(self.W(x)))\n",
        "        attn = attn.squeeze(-1)\n",
        "        attn = torch.softmax(attn, dim=1)\n",
        "\n",
        "        # Calculate weighted average of hidden states.\n",
        "        context = attn.unsqueeze(-1) * x\n",
        "        context = context.sum(dim=1)\n",
        "\n",
        "        return context"
      ],
      "metadata": {
        "id": "hpOHfc-5ANwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTHvF8AZANwq"
      },
      "outputs": [],
      "source": [
        "class MM_ED_Bi_GRU_attention_WFW(nn.Module):\n",
        "    def __init__(self, input_acc, input_gyr, drop_prob=0.0):\n",
        "        super(MM_ED_Bi_GRU_attention_WFW, self).__init__()\n",
        "\n",
        "        self.encoder_acc=Encoder(input_acc, drop_prob)\n",
        "        self.encoder_gyr=Encoder(input_gyr, drop_prob)\n",
        "\n",
        "        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)\n",
        "        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = nn.Linear(2*window*128, 54)\n",
        "\n",
        "        self.temporal_attn_acc = TemporalAttention(128)\n",
        "        self.temporal_attn_gyr = TemporalAttention(128)\n",
        "\n",
        "        self.decoder=LSTMDecoder(2*128+6, 2*64, 6, 1,0.05)\n",
        "\n",
        "    def forward(self, x_acc, x_gyr):\n",
        "\n",
        "        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))\n",
        "        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))\n",
        "\n",
        "        x_acc_1=self.BN_acc(x_acc_1)\n",
        "        x_gyr_1=self.BN_gyr(x_gyr_1)\n",
        "\n",
        "        x_acc_2=x_acc_1.view(-1, window, x_acc_1.size(-1))\n",
        "        x_gyr_2=x_gyr_1.view(-1, window, x_gyr_1.size(-1))\n",
        "\n",
        "        x_acc, h_acc=self.encoder_acc(x_acc_2)\n",
        "        x_gyr, h_gyr=self.encoder_gyr(x_gyr_2)\n",
        "\n",
        "\n",
        "        x=torch.cat((x_acc,x_gyr),dim=-1)\n",
        "        h=torch.cat((h_acc,h_gyr),dim=-1)\n",
        "\n",
        "        h_acc_att=self.temporal_attn_acc(x_acc)\n",
        "        h_gyr_att=self.temporal_attn_gyr(x_gyr)\n",
        "\n",
        "        h_att=torch.cat((h_acc_att,h_gyr_att),dim=-1)\n",
        "\n",
        "        #Do it separately\n",
        "        h_att=h_att.unsqueeze(0)\n",
        "        h_att=h_att.transpose(1,0)\n",
        "        out=self.decoder(h,h_att, 9)\n",
        "\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "7792fb87-5bdc-4862-a58b-7ff925285b9b",
        "id": "F84zpd2YANwq"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, time: 41.5052, Training Loss: 5.5306,  Validation loss: 3.1365\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-178-58a26da58a2a>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mMM_ED_Bi_GRU_attention_WFW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmm_ed_bi_gru_attention_wfw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_mm_m\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_mm_ed_bi_gru_attention_wfw_IMU8.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-173-71b1fe5884af>\u001b[0m in \u001b[0;36mtrain_mm_m\u001b[0;34m(train_loader, learn_rate, EPOCHS, model, filename)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_gyr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_hof\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_present\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_future\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_gyr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-177-06d2abb3ecd0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_acc, x_gyr)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mh_att\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mh_att\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mh_att\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mh_att\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh_att\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-175-77876b0544fa>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, encoder_hidden, h_att, max_len)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m# Use the output for prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "lr = 0.001\n",
        "model= MM_ED_Bi_GRU_attention_WFW(24,24)\n",
        "\n",
        "mm_ed_bi_gru_attention_wfw = train_mm_m(train_loader, lr,15,model,path+subject+'_mm_ed_bi_gru_attention_wfw_IMU8.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d4d9b0b-9d1d-42ac-e63b-3d391e1fc441",
        "id": "IGPWBPX1ANwr"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 179
        }
      ],
      "source": [
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "574c7765-2c74-416d-bed2-3620c344d28f",
        "id": "8DZ6-KyxANwr"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(18718, 9, 6)\n",
            "4.467006\n",
            "5.454405\n",
            "3.9722931\n",
            "3.5507724\n",
            "4.770136\n",
            "3.788638\n",
            "\n",
            "\n",
            "0.9719973034309487\n",
            "0.976223752301612\n",
            "0.9305533828074444\n",
            "0.979727411659312\n",
            "0.9800109187806403\n",
            "0.9268166364623989\n",
            "Mean: 4.334 +/- 0.677\n",
            "Mean: 0.961 +/- 0.028\n"
          ]
        }
      ],
      "source": [
        "mm_ed_bi_gru_attention_wfw= MM_ED_Bi_GRU_attention_WFW(24,24)\n",
        "mm_ed_bi_gru_attention_wfw.load_state_dict(torch.load(path+subject+'_mm_ed_bi_gru_attention_wfw_IMU8.pth'))\n",
        "mm_ed_bi_gru_attention_wfw.to(device)\n",
        "\n",
        "mm_ed_bi_gru_attention_wfw.eval()\n",
        "\n",
        "# iterate through batches of test data\n",
        "with torch.no_grad():\n",
        "    for i, (data, data_acc, data_gyr, data_hof, target_present, target_future) in enumerate(test_loader):\n",
        "        output = mm_ed_bi_gru_attention_wfw(data_acc.to(device).float(),data_gyr.to(device).float())\n",
        "        if i==0:\n",
        "          yhat_5=output\n",
        "          test_target=target_future\n",
        "\n",
        "        yhat_5=torch.cat((yhat_5,output),dim=0)\n",
        "        test_target=torch.cat((test_target,target_future),dim=0)\n",
        "\n",
        "        # clear memory\n",
        "        del data, target_future,output\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "\n",
        "yhat_4 = yhat_5.detach().cpu().numpy()\n",
        "test_target = test_target.detach().cpu().numpy()\n",
        "print(yhat_4.shape)\n",
        "\n",
        "rmse, p=RMSE_prediction(yhat_4,test_target)\n",
        "\n",
        "ablation_9=np.hstack([rmse,p])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOHKs6gyANwr"
      },
      "source": [
        "## 10. Attention With gating+ Bi-GRU --Encoder+decoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.lstm_1 = nn.GRU(input_dim, 128, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.lstm_2 = nn.GRU(256, 64, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_1, _ = self.lstm_1(x)\n",
        "        out_1=self.dropout(out_1)\n",
        "        out_2, h_n = self.lstm_2(out_1)\n",
        "        out_2=self.dropout(out_2)\n",
        "\n",
        "        return out_2, h_n"
      ],
      "metadata": {
        "id": "fSZtOo2uANwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMDecoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers,dropout):\n",
        "        super(LSTMDecoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.GRU(input_size, hidden_size, num_layers, bidirectional=True, batch_first=True)\n",
        "        self.fc = nn.Linear(2*hidden_size, output_size)\n",
        "        self.gating_net = nn.Sequential(\n",
        "            nn.Linear(262, 262),\n",
        "            nn.Sigmoid())\n",
        "\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, encoder_hidden, h_att, max_len):\n",
        "        batch_size = encoder_hidden.shape[1]\n",
        "        hidden = encoder_hidden\n",
        "        # cell = encoder_cell\n",
        "        outputs = []\n",
        "\n",
        "        # Use the last time step of target as the initial input\n",
        "        input_1 = torch.zeros(batch_size,1,6).to(device)\n",
        "        input = torch.cat((input_1, h_att), dim=-1)\n",
        "\n",
        "        for i in range(max_len):\n",
        "\n",
        "            gating_weight=self.gating_net(input)\n",
        "            input=input*gating_weight\n",
        "\n",
        "            # Run one time step of LSTM\n",
        "            output, hidden = self.lstm(input, hidden)\n",
        "            output=self.dropout(output)\n",
        "\n",
        "            # Use the output for prediction\n",
        "            output = self.fc(output.squeeze(1))\n",
        "            outputs.append(output.unsqueeze(1))\n",
        "\n",
        "            input = torch.cat((output.unsqueeze(1),input[:,:,6:262]), dim=-1)\n",
        "\n",
        "        # Concatenate all the outputs along the time dimension\n",
        "        outputs = torch.cat(outputs, dim=1)\n",
        "\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "iNCULw2LANwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TemporalAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(TemporalAttention, self).__init__()\n",
        "\n",
        "        self.W = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, sequence_length, hidden_size)\n",
        "\n",
        "        # Calculate attention weights.\n",
        "        attn = self.V(torch.tanh(self.W(x)))\n",
        "        attn = attn.squeeze(-1)\n",
        "        attn = torch.softmax(attn, dim=1)\n",
        "\n",
        "        # Calculate weighted average of hidden states.\n",
        "        context = attn.unsqueeze(-1) * x\n",
        "        context = context.sum(dim=1)\n",
        "\n",
        "        return context"
      ],
      "metadata": {
        "id": "6mXuWbsLANws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pn2itl3fANws"
      },
      "outputs": [],
      "source": [
        "class MM_ED_Bi_GRU_attention_FW(nn.Module):\n",
        "    def __init__(self, input_acc, input_gyr, drop_prob=0.0):\n",
        "        super(MM_ED_Bi_GRU_attention_FW, self).__init__()\n",
        "\n",
        "        self.encoder_acc=Encoder(input_acc, drop_prob)\n",
        "        self.encoder_gyr=Encoder(input_gyr, drop_prob)\n",
        "\n",
        "        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)\n",
        "        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)\n",
        "\n",
        "        self.temporal_attn_acc = TemporalAttention(128)\n",
        "        self.temporal_attn_gyr = TemporalAttention(128)\n",
        "\n",
        "        self.decoder=LSTMDecoder(2*128+6, 2*64, 6, 1, 0.05)\n",
        "\n",
        "    def forward(self, x_acc, x_gyr):\n",
        "\n",
        "        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))\n",
        "        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))\n",
        "\n",
        "        x_acc_1=self.BN_acc(x_acc_1)\n",
        "        x_gyr_1=self.BN_gyr(x_gyr_1)\n",
        "\n",
        "        x_acc_2=x_acc_1.view(-1, window, x_acc_1.size(-1))\n",
        "        x_gyr_2=x_gyr_1.view(-1, window, x_gyr_1.size(-1))\n",
        "\n",
        "        x_acc, h_acc=self.encoder_acc(x_acc_2)\n",
        "        x_gyr, h_gyr=self.encoder_gyr(x_gyr_2)\n",
        "\n",
        "        x=torch.cat((x_acc,x_gyr),dim=-1)\n",
        "        h=torch.cat((h_acc,h_gyr),dim=-1)\n",
        "\n",
        "        h_acc_att=self.temporal_attn_acc(x_acc)\n",
        "        h_gyr_att=self.temporal_attn_gyr(x_gyr)\n",
        "\n",
        "        h_att=torch.cat((h_acc_att,h_gyr_att),dim=-1)\n",
        "\n",
        "        #Do it separately\n",
        "\n",
        "        h_att=h_att.unsqueeze(0)\n",
        "        h_att=h_att.transpose(1,0)\n",
        "        out=self.decoder(h,h_att, 9)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "e9f0be11-b3ae-48c6-f749-1860056401cf",
        "id": "rbBIHV1SANws"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, time: 50.4831, Training Loss: 5.8036,  Validation loss: 3.0796\n",
            "Epoch: 2, time: 49.5163, Training Loss: 3.0357,  Validation loss: 2.4816\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-185-e738cc2d13a0>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mMM_ED_Bi_GRU_attention_FW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmm_ed_bi_gru_attention_fw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_mm_m\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_mm_ed_bi_gru_attention_fw_IMU8.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-173-71b1fe5884af>\u001b[0m in \u001b[0;36mtrain_mm_m\u001b[0;34m(train_loader, learn_rate, EPOCHS, model, filename)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_gyr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_hof\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_present\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_future\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_gyr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-184-9a665da83d79>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_acc, x_gyr)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mh_att\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mh_att\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mh_att\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mh_att\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh_att\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-182-17c97e7df220>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, encoder_hidden, h_att, max_len)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;31m# Run one time step of LSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;31m# Use the output for prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1250\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dropout probability has to be between 0 and 1, \"\u001b[0m \u001b[0;34m\"but got {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1252\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_VF.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "lr = 0.001\n",
        "model= MM_ED_Bi_GRU_attention_FW(24,24)\n",
        "\n",
        "mm_ed_bi_gru_attention_fw = train_mm_m(train_loader, lr,15,model,path+subject+'_mm_ed_bi_gru_attention_fw_IMU8.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "438ab97e-b4ff-49ca-a8d1-a08c7b53a68e",
        "id": "qKEC9o53ANws"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 186
        }
      ],
      "source": [
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fda5d26a-02e1-4ef7-dc6d-50c147b02d6c",
        "id": "GfA_Kc-KANws"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(18718, 9, 6)\n",
            "4.799924\n",
            "5.898864\n",
            "3.8519933\n",
            "3.460963\n",
            "4.386693\n",
            "3.8390324\n",
            "\n",
            "\n",
            "0.9797253240938865\n",
            "0.9761120952918895\n",
            "0.9456209163386363\n",
            "0.9821675738748021\n",
            "0.9834574194086709\n",
            "0.9383691373222359\n",
            "Mean: 4.373 +/- 0.682\n",
            "Mean: 0.968 +/- 0.022\n"
          ]
        }
      ],
      "source": [
        "mm_ed_bi_gru_attention_fw= MM_ED_Bi_GRU_attention_FW(24,24)\n",
        "mm_ed_bi_gru_attention_fw.load_state_dict(torch.load(path+subject+'_mm_ed_bi_gru_attention_fw_IMU8.pth'))\n",
        "mm_ed_bi_gru_attention_fw.to(device)\n",
        "\n",
        "mm_ed_bi_gru_attention_fw.eval()\n",
        "\n",
        "# iterate through batches of test data\n",
        "with torch.no_grad():\n",
        "    for i, (data, data_acc, data_gyr, data_hof, target_present, target_future) in enumerate(test_loader):\n",
        "        output = mm_ed_bi_gru_attention_fw(data_acc.to(device).float(),data_gyr.to(device).float())\n",
        "        if i==0:\n",
        "          yhat_5=output\n",
        "          test_target=target_future\n",
        "\n",
        "        yhat_5=torch.cat((yhat_5,output),dim=0)\n",
        "        test_target=torch.cat((test_target,target_future),dim=0)\n",
        "\n",
        "        # clear memory\n",
        "        del data, target_future,output\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "\n",
        "yhat_4 = yhat_5.detach().cpu().numpy()\n",
        "test_target = test_target.detach().cpu().numpy()\n",
        "print(yhat_4.shape)\n",
        "\n",
        "rmse, p=RMSE_prediction(yhat_4,test_target)\n",
        "\n",
        "ablation_10=np.hstack([rmse,p])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UD_o3xNxaC5p"
      },
      "source": [
        "## 11. Attention Without gating+ Bi-LSTM --Encoder+decoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.lstm_1 = nn.LSTM(input_dim, 128, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.lstm_2 = nn.LSTM(256, 64, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_1, _ = self.lstm_1(x)\n",
        "        out_1=self.dropout(out_1)\n",
        "        out_2, (h_n, c_n) = self.lstm_2(out_1)\n",
        "        out_2=self.dropout(out_2)\n",
        "\n",
        "        return out_2, (h_n, c_n)"
      ],
      "metadata": {
        "id": "w57A4GG0aC56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMDecoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout):\n",
        "        super(LSTMDecoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, bidirectional=True, batch_first=True)\n",
        "        self.fc = nn.Linear(2*hidden_size, output_size)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "        self.gating_net = nn.Sequential(nn.Linear(2*128+6, 2*128+6),nn.Sigmoid())\n",
        "\n",
        "\n",
        "    def forward(self, encoder_hidden, encoder_cell, h_att,  max_len):\n",
        "        batch_size = encoder_hidden.shape[1]\n",
        "        hidden = encoder_hidden\n",
        "        cell = encoder_cell\n",
        "        outputs = []\n",
        "\n",
        "        # Use the last time step of target as the initial input\n",
        "        input_1 = torch.zeros(batch_size,1,6).to(device)\n",
        "        input = torch.cat((input_1, h_att), dim=-1)\n",
        "\n",
        "\n",
        "\n",
        "        for i in range(max_len):\n",
        "\n",
        "            # gating_weight=self.gating_net(input)\n",
        "            # input=input*gating_weight\n",
        "\n",
        "            # Run one time step of LSTM\n",
        "            output, (hidden, cell) = self.lstm(input, (hidden, cell))\n",
        "            output=self.dropout(output)\n",
        "\n",
        "            # Use the output for prediction\n",
        "            output = self.fc(output.squeeze(1))\n",
        "            outputs.append(output.unsqueeze(1))\n",
        "\n",
        "            # Use the predicted output as the next input\n",
        "            input = torch.cat(( output.unsqueeze(1), input[:,:,6:262]), dim=-1)\n",
        "\n",
        "\n",
        "\n",
        "        # Concatenate all the outputs along the time dimension\n",
        "        outputs = torch.cat(outputs, dim=1)\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "vBlDjCIZaC57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TemporalAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(TemporalAttention, self).__init__()\n",
        "\n",
        "        self.W = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, sequence_length, hidden_size)\n",
        "\n",
        "        # Calculate attention weights.\n",
        "        attn = self.V(torch.tanh(self.W(x)))\n",
        "        attn = attn.squeeze(-1)\n",
        "        attn = torch.softmax(attn, dim=1)\n",
        "\n",
        "        # Calculate weighted average of hidden states.\n",
        "        context = attn.unsqueeze(-1) * x\n",
        "        context = context.sum(dim=1)\n",
        "\n",
        "        return context"
      ],
      "metadata": {
        "id": "ZytNlQJQaC57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3kFVHWEaC58"
      },
      "outputs": [],
      "source": [
        "class MM_ED_Bi_LSTM_WFW(nn.Module):\n",
        "    def __init__(self, input_acc, input_gyr, drop_prob=0.05):\n",
        "        super(MM_ED_Bi_LSTM_WFW, self).__init__()\n",
        "\n",
        "        self.encoder_acc=Encoder(input_acc, drop_prob)\n",
        "        self.encoder_gyr=Encoder(input_gyr, drop_prob)\n",
        "\n",
        "        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)\n",
        "        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)\n",
        "\n",
        "        self.temporal_attn_acc = TemporalAttention(128)\n",
        "        self.temporal_attn_gyr = TemporalAttention(128)\n",
        "\n",
        "\n",
        "        self.decoder=LSTMDecoder(2*128+6, 2*64, 6, 1, 0.05)\n",
        "\n",
        "    def forward(self, x_acc, x_gyr):\n",
        "\n",
        "        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))\n",
        "        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))\n",
        "\n",
        "        x_acc_1=self.BN_acc(x_acc_1)\n",
        "        x_gyr_1=self.BN_gyr(x_gyr_1)\n",
        "\n",
        "        x_acc_2=x_acc_1.view(-1, window, x_acc_1.size(-1))\n",
        "        x_gyr_2=x_gyr_1.view(-1, window, x_gyr_1.size(-1))\n",
        "\n",
        "        x_acc,(h_acc,c_acc)=self.encoder_acc(x_acc_2)\n",
        "        x_gyr,(h_gyr,c_gyr)=self.encoder_gyr(x_gyr_2)\n",
        "\n",
        "        h=torch.cat((h_acc,h_gyr),dim=-1)\n",
        "        c=torch.cat((c_acc,c_gyr),dim=-1)\n",
        "\n",
        "        h_acc_att=self.temporal_attn_acc(x_acc)\n",
        "        h_gyr_att=self.temporal_attn_gyr(x_gyr)\n",
        "\n",
        "\n",
        "        h_att=torch.cat((h_acc_att,h_gyr_att),dim=-1)\n",
        "\n",
        "        #Do it separately\n",
        "\n",
        "        h_att=h_att.unsqueeze(0)\n",
        "        h_att=h_att.transpose(1,0)\n",
        "\n",
        "        out=self.decoder(h, c, h_att, 9)\n",
        "\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "outputId": "8020da6b-9a50-44cb-afb4-44e23dc24d20",
        "id": "D0vmAb6uaC58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, time: 47.6215, Training Loss: 6.1801,  Validation loss: 3.2773\n",
            "Epoch: 2, time: 48.0524, Training Loss: 3.2080,  Validation loss: 2.6463\n",
            "Epoch: 3, time: 47.6908, Training Loss: 2.8126,  Validation loss: 2.4100\n",
            "Epoch: 4, time: 47.8591, Training Loss: 2.6098,  Validation loss: 2.2671\n",
            "Epoch: 5, time: 48.0067, Training Loss: 2.4750,  Validation loss: 2.1846\n",
            "Epoch: 6, time: 47.7742, Training Loss: 2.3676,  Validation loss: 2.1475\n",
            "Epoch: 7, time: 47.8497, Training Loss: 2.2876,  Validation loss: 2.1026\n",
            "Epoch: 8, time: 47.8870, Training Loss: 2.2188,  Validation loss: 1.9594\n",
            "Epoch: 9, time: 47.5657, Training Loss: 2.1593,  Validation loss: 1.9772\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-fb2ba702bb33>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMM_ED_Bi_LSTM_WFW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmm_ed_bi_lstm_WFW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_mm_m\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_mm_ed_bi_lstm_WFW_IMU8.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-62-71b1fe5884af>\u001b[0m in \u001b[0;36mtrain_mm_m\u001b[0;34m(train_loader, learn_rate, EPOCHS, model, filename)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m                                                f\"but got {result}.\")\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    139\u001b[0m                 state_steps)\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    142\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m     func(params,\n\u001b[0m\u001b[1;32m    282\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m          \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "lr = 0.001\n",
        "model = MM_ED_Bi_LSTM_WFW(24,24)\n",
        "\n",
        "mm_ed_bi_lstm_WFW = train_mm_m(train_loader, lr,12,model,path+subject+'_mm_ed_bi_lstm_WFW_IMU8.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05bc1265-c755-4d2d-a7e7-4ccb9959f7e1",
        "id": "exJ-Gk4MaC58"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efb0d610-4acf-405f-bd4c-54b6b8bff981",
        "id": "K5Cn5Sa6aC59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(18718, 9, 6)\n",
            "5.065517\n",
            "5.538847\n",
            "3.5576572\n",
            "3.138841\n",
            "4.162308\n",
            "3.75366\n",
            "\n",
            "\n",
            "0.9747889402516533\n",
            "0.9800464145894439\n",
            "0.9465982808814303\n",
            "0.9847122791325597\n",
            "0.9852672504097985\n",
            "0.9487564408633099\n",
            "Mean: 4.203 +/- 0.604\n",
            "Mean: 0.970 +/- 0.019\n"
          ]
        }
      ],
      "source": [
        "mm_ed_bi_lstm_WFW= MM_ED_Bi_LSTM_WFW(24,24)\n",
        "mm_ed_bi_lstm_WFW.load_state_dict(torch.load(path+subject+'_mm_ed_bi_lstm_WFW_IMU8.pth'))\n",
        "mm_ed_bi_lstm_WFW.to(device)\n",
        "\n",
        "mm_ed_bi_lstm_WFW.eval()\n",
        "\n",
        "# iterate through batches of test data\n",
        "with torch.no_grad():\n",
        "    for i, (data, data_acc, data_gyr, data_hof, target_present, target_future) in enumerate(test_loader):\n",
        "        output = mm_ed_bi_lstm_WFW(data_acc.to(device).float(),data_gyr.to(device).float())\n",
        "        if i==0:\n",
        "          yhat_5=output\n",
        "          test_target=target_future\n",
        "\n",
        "        yhat_5=torch.cat((yhat_5,output),dim=0)\n",
        "        test_target=torch.cat((test_target,target_future),dim=0)\n",
        "\n",
        "        # clear memory\n",
        "        del data, target_future,output\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "\n",
        "yhat_4 = yhat_5.detach().cpu().numpy()\n",
        "test_target = test_target.detach().cpu().numpy()\n",
        "print(yhat_4.shape)\n",
        "\n",
        "rmse, p=RMSE_prediction(yhat_4,test_target)\n",
        "\n",
        "ablation_11=np.hstack([rmse,p])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsnBqCKbdsUF"
      },
      "source": [
        "## 12. Attention With gating+ Bi-LSTM --Encoder+decoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.lstm_1 = nn.LSTM(input_dim, 128, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.lstm_2 = nn.LSTM(256, 64, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_1, _ = self.lstm_1(x)\n",
        "        out_1=self.dropout(out_1)\n",
        "        out_2, (h_n, c_n) = self.lstm_2(out_1)\n",
        "        out_2=self.dropout(out_2)\n",
        "\n",
        "        return out_2, (h_n, c_n)"
      ],
      "metadata": {
        "id": "TCif9bA0dsUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMDecoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout):\n",
        "        super(LSTMDecoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, bidirectional=True, batch_first=True)\n",
        "        self.fc = nn.Linear(2*hidden_size, output_size)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "        self.gating_net = nn.Sequential(nn.Linear(2*128+6, 2*128+6),nn.Sigmoid())\n",
        "\n",
        "\n",
        "    def forward(self, encoder_hidden, encoder_cell, h_att,  max_len):\n",
        "        batch_size = encoder_hidden.shape[1]\n",
        "        hidden = encoder_hidden\n",
        "        cell = encoder_cell\n",
        "        outputs = []\n",
        "\n",
        "        # Use the last time step of target as the initial input\n",
        "        input_1 = torch.zeros(batch_size,1,6).to(device)\n",
        "        input = torch.cat((input_1, h_att), dim=-1)\n",
        "\n",
        "\n",
        "\n",
        "        for i in range(max_len):\n",
        "\n",
        "            gating_weight=self.gating_net(input)\n",
        "            input=input*gating_weight\n",
        "\n",
        "            # Run one time step of LSTM\n",
        "            output, (hidden, cell) = self.lstm(input, (hidden, cell))\n",
        "            output=self.dropout(output)\n",
        "\n",
        "            # Use the output for prediction\n",
        "            output = self.fc(output.squeeze(1))\n",
        "            outputs.append(output.unsqueeze(1))\n",
        "\n",
        "            # Use the predicted output as the next input\n",
        "            input = torch.cat(( output.unsqueeze(1), input[:,:,6:262]), dim=-1)\n",
        "\n",
        "\n",
        "\n",
        "        # Concatenate all the outputs along the time dimension\n",
        "        outputs = torch.cat(outputs, dim=1)\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "nee0w5oZdsUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TemporalAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(TemporalAttention, self).__init__()\n",
        "\n",
        "        self.W = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, sequence_length, hidden_size)\n",
        "\n",
        "        # Calculate attention weights.\n",
        "        attn = self.V(torch.tanh(self.W(x)))\n",
        "        attn = attn.squeeze(-1)\n",
        "        attn = torch.softmax(attn, dim=1)\n",
        "\n",
        "        # Calculate weighted average of hidden states.\n",
        "        context = attn.unsqueeze(-1) * x\n",
        "        context = context.sum(dim=1)\n",
        "\n",
        "        return context"
      ],
      "metadata": {
        "id": "LDGzivBudsUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_e4MtjUdsUJ"
      },
      "outputs": [],
      "source": [
        "class MM_ED_Bi_LSTM_WF(nn.Module):\n",
        "    def __init__(self, input_acc, input_gyr, drop_prob=0.05):\n",
        "        super(MM_ED_Bi_LSTM_WF, self).__init__()\n",
        "\n",
        "        self.encoder_acc=Encoder(input_acc, drop_prob)\n",
        "        self.encoder_gyr=Encoder(input_gyr, drop_prob)\n",
        "\n",
        "        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)\n",
        "        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)\n",
        "\n",
        "        self.temporal_attn_acc = TemporalAttention(128)\n",
        "        self.temporal_attn_gyr = TemporalAttention(128)\n",
        "\n",
        "\n",
        "        self.decoder=LSTMDecoder(2*128+6, 2*64, 6, 1, 0.05)\n",
        "\n",
        "    def forward(self, x_acc, x_gyr):\n",
        "\n",
        "        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))\n",
        "        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))\n",
        "\n",
        "        x_acc_1=self.BN_acc(x_acc_1)\n",
        "        x_gyr_1=self.BN_gyr(x_gyr_1)\n",
        "\n",
        "        x_acc_2=x_acc_1.view(-1, window, x_acc_1.size(-1))\n",
        "        x_gyr_2=x_gyr_1.view(-1, window, x_gyr_1.size(-1))\n",
        "\n",
        "        x_acc,(h_acc,c_acc)=self.encoder_acc(x_acc_2)\n",
        "        x_gyr,(h_gyr,c_gyr)=self.encoder_gyr(x_gyr_2)\n",
        "\n",
        "        h=torch.cat((h_acc,h_gyr),dim=-1)\n",
        "        c=torch.cat((c_acc,c_gyr),dim=-1)\n",
        "\n",
        "        h_acc_att=self.temporal_attn_acc(x_acc)\n",
        "        h_gyr_att=self.temporal_attn_gyr(x_gyr)\n",
        "\n",
        "\n",
        "        h_att=torch.cat((h_acc_att,h_gyr_att),dim=-1)\n",
        "\n",
        "        #Do it separately\n",
        "\n",
        "        h_att=h_att.unsqueeze(0)\n",
        "        h_att=h_att.transpose(1,0)\n",
        "\n",
        "        out=self.decoder(h, c, h_att, 9)\n",
        "\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "outputId": "98f70c1c-948c-4016-85af-8a504e95973d",
        "id": "YBtexF5tdsUK"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, time: 56.7004, Training Loss: 6.4053,  Validation loss: 3.2725\n",
            "Epoch: 2, time: 53.6240, Training Loss: 3.1471,  Validation loss: 2.6263\n",
            "Epoch: 3, time: 53.1201, Training Loss: 2.7454,  Validation loss: 2.3839\n",
            "Epoch: 4, time: 53.8108, Training Loss: 2.5373,  Validation loss: 2.2559\n",
            "Epoch: 5, time: 53.2904, Training Loss: 2.4056,  Validation loss: 2.1189\n",
            "Epoch: 6, time: 52.9391, Training Loss: 2.2956,  Validation loss: 2.0578\n",
            "Epoch: 7, time: 53.0533, Training Loss: 2.2140,  Validation loss: 1.9902\n",
            "Epoch: 8, time: 53.1736, Training Loss: 2.1524,  Validation loss: 1.9505\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-79-7dfab9401460>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMM_ED_Bi_LSTM_WF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmm_ed_bi_lstm_WF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_mm_m\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_mm_ed_bi_lstm_WF_IMU8.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-62-71b1fe5884af>\u001b[0m in \u001b[0;36mtrain_mm_m\u001b[0;34m(train_loader, learn_rate, EPOCHS, model, filename)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_gyr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_hof\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_present\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_future\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_gyr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-78-aa5a4f1749e0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_acc, x_gyr)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mx_gyr_2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_gyr_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_gyr_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mx_acc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_acc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_acc_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mx_gyr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_gyr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc_gyr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_gyr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_gyr_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-72-8ab18658f892>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mout_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mout_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mout_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_n\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mout_2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    810\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    813\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    814\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "lr = 0.001\n",
        "model = MM_ED_Bi_LSTM_WF(24,24)\n",
        "\n",
        "mm_ed_bi_lstm_WF = train_mm_m(train_loader, lr,12,model,path+subject+'_mm_ed_bi_lstm_WF_IMU8.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e340accf-28d6-451f-a41c-9d2e48749026",
        "id": "45CowuvhdsUK"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ],
      "source": [
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47f881fe-dcdf-49e0-eae8-6f9ac90da461",
        "id": "ZdmtO7LZdsUL"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(18718, 9, 6)\n",
            "5.3735704\n",
            "5.4082613\n",
            "3.5998948\n",
            "3.1868982\n",
            "3.955958\n",
            "3.2462692\n",
            "\n",
            "\n",
            "0.9769849928548764\n",
            "0.978992976963093\n",
            "0.9455737045263066\n",
            "0.983807057313564\n",
            "0.9869372857153033\n",
            "0.9510413275663534\n",
            "Mean: 4.128 +/- 0.643\n",
            "Mean: 0.971 +/- 0.019\n"
          ]
        }
      ],
      "source": [
        "mm_ed_bi_lstm_WF= MM_ED_Bi_LSTM_WF(24,24)\n",
        "mm_ed_bi_lstm_WF.load_state_dict(torch.load(path+subject+'_mm_ed_bi_lstm_WF_IMU8.pth'))\n",
        "mm_ed_bi_lstm_WF.to(device)\n",
        "\n",
        "mm_ed_bi_lstm_WF.eval()\n",
        "\n",
        "# iterate through batches of test data\n",
        "with torch.no_grad():\n",
        "    for i, (data, data_acc, data_gyr, data_hof, target_present, target_future) in enumerate(test_loader):\n",
        "        output = mm_ed_bi_lstm_WF(data_acc.to(device).float(),data_gyr.to(device).float())\n",
        "        if i==0:\n",
        "          yhat_5=output\n",
        "          test_target=target_future\n",
        "\n",
        "        yhat_5=torch.cat((yhat_5,output),dim=0)\n",
        "        test_target=torch.cat((test_target,target_future),dim=0)\n",
        "\n",
        "        # clear memory\n",
        "        del data, target_future,output\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "\n",
        "yhat_4 = yhat_5.detach().cpu().numpy()\n",
        "test_target = test_target.detach().cpu().numpy()\n",
        "print(yhat_4.shape)\n",
        "\n",
        "rmse, p=RMSE_prediction(yhat_4,test_target)\n",
        "\n",
        "ablation_12=np.hstack([rmse,p])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ym4ptIK_tbgZ"
      },
      "source": [
        "## 13. Attention Without gating+ Bi-LSTM+ Bi-GRU --Encoder+decoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder_lstm(nn.Module):\n",
        "    def __init__(self, input_dim, dropout):\n",
        "        super(Encoder_lstm, self).__init__()\n",
        "        self.lstm_1 = nn.LSTM(input_dim, 128, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.lstm_2 = nn.LSTM(256, 64, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_1, _ = self.lstm_1(x)\n",
        "        out_1=self.dropout(out_1)\n",
        "        out_2, (h_n, c_n) = self.lstm_2(out_1)\n",
        "        out_2=self.dropout(out_2)\n",
        "\n",
        "        return out_2, (h_n, c_n)"
      ],
      "metadata": {
        "id": "9dSvEy7-tbge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder_gru(nn.Module):\n",
        "    def __init__(self, input_dim, dropout):\n",
        "        super(Encoder_gru, self).__init__()\n",
        "        self.lstm_1 = nn.GRU(input_dim, 128, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.lstm_2 = nn.GRU(256, 64, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_1, _ = self.lstm_1(x)\n",
        "        out_1=self.dropout(out_1)\n",
        "        out_2, h_n = self.lstm_2(out_1)\n",
        "        out_2=self.dropout(out_2)\n",
        "\n",
        "        return out_2, h_n"
      ],
      "metadata": {
        "id": "qXtgRFRstbge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMDecoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout):\n",
        "        super(LSTMDecoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, bidirectional=True, batch_first=True)\n",
        "        self.gru = nn.GRU(input_size, hidden_size, num_layers, bidirectional=True, batch_first=True)\n",
        "        self.fc = nn.Linear(2*2*hidden_size, output_size)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, encoder_hidden, encoder_cell, h_gru, h_att_lstm, h_att_gru,  max_len):\n",
        "        batch_size = encoder_hidden.shape[1]\n",
        "        hidden = encoder_hidden\n",
        "        hidden_gru=h_gru\n",
        "        cell = encoder_cell\n",
        "        outputs = []\n",
        "\n",
        "        # Use the last time step of target as the initial input\n",
        "        input_1 = torch.zeros(batch_size,1,6).to(device)\n",
        "        input = torch.cat((input_1, h_att_lstm, h_att_gru), dim=-1)\n",
        "\n",
        "\n",
        "        for i in range(max_len):\n",
        "\n",
        "            # Run one time step of LSTM\n",
        "            output_1, (hidden, cell) = self.lstm(input, (hidden, cell))\n",
        "            output_2, hidden_gru = self.gru(input, hidden_gru)\n",
        "\n",
        "            output=torch.cat((output_1,output_2),dim=-1)\n",
        "\n",
        "            output=self.dropout(output)\n",
        "\n",
        "            # Use the output for prediction\n",
        "            output = self.fc(output.squeeze(1))\n",
        "            outputs.append(output.unsqueeze(1))\n",
        "\n",
        "            # Use the predicted output as the next input\n",
        "            input = torch.cat(( output.unsqueeze(1), input[:,:,6:518]), dim=-1)\n",
        "\n",
        "\n",
        "\n",
        "        # Concatenate all the outputs along the time dimension\n",
        "        outputs = torch.cat(outputs, dim=1)\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "CybhSaK4tbge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TemporalAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(TemporalAttention, self).__init__()\n",
        "\n",
        "        self.W = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, sequence_length, hidden_size)\n",
        "\n",
        "        # Calculate attention weights.\n",
        "        attn = self.V(torch.tanh(self.W(x)))\n",
        "        attn = attn.squeeze(-1)\n",
        "        attn = torch.softmax(attn, dim=1)\n",
        "\n",
        "        # Calculate weighted average of hidden states.\n",
        "        context = attn.unsqueeze(-1) * x\n",
        "        context = context.sum(dim=1)\n",
        "\n",
        "        return context"
      ],
      "metadata": {
        "id": "wVk90T7Gtbgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kQm3SRWtbgf"
      },
      "outputs": [],
      "source": [
        "class MM_ED_Bi_LSTM_GRU_WFW(nn.Module):\n",
        "    def __init__(self, input_acc, input_gyr, drop_prob=0.05):\n",
        "        super(MM_ED_Bi_LSTM_GRU_WFW, self).__init__()\n",
        "\n",
        "        self.encoder_acc_lstm=Encoder_lstm(input_acc, drop_prob)\n",
        "        self.encoder_gyr_lstm=Encoder_lstm(input_gyr, drop_prob)\n",
        "\n",
        "        self.encoder_acc_gru=Encoder_gru(input_acc, drop_prob)\n",
        "        self.encoder_gyr_gru=Encoder_gru(input_gyr, drop_prob)\n",
        "\n",
        "        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)\n",
        "        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)\n",
        "\n",
        "        self.temporal_attn_acc = TemporalAttention(128)\n",
        "        self.temporal_attn_gyr = TemporalAttention(128)\n",
        "\n",
        "\n",
        "        self.decoder=LSTMDecoder(2*2*128+6, 2*64, 6, 1, 0.05)\n",
        "\n",
        "    def forward(self, x_acc, x_gyr):\n",
        "\n",
        "        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))\n",
        "        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))\n",
        "\n",
        "        x_acc_1=self.BN_acc(x_acc_1)\n",
        "        x_gyr_1=self.BN_gyr(x_gyr_1)\n",
        "\n",
        "        x_acc_2=x_acc_1.view(-1, window, x_acc_1.size(-1))\n",
        "        x_gyr_2=x_gyr_1.view(-1, window, x_gyr_1.size(-1))\n",
        "\n",
        "        x_acc_lstm,(h_acc_lstm, c_acc_lstm)=self.encoder_acc_lstm(x_acc_2)\n",
        "        x_gyr_lstm,(h_gyr_lstm, c_gyr_lstm)=self.encoder_gyr_lstm(x_gyr_2)\n",
        "\n",
        "\n",
        "        x_acc_gru,h_acc_gru=self.encoder_acc_gru(x_acc_2)\n",
        "        x_gyr_gru,h_gyr_gru=self.encoder_gyr_gru(x_gyr_2)\n",
        "\n",
        "\n",
        "        h_lstm=torch.cat((h_acc_lstm,h_gyr_lstm),dim=-1)\n",
        "        c_lstm=torch.cat((c_acc_lstm,c_gyr_lstm),dim=-1)\n",
        "\n",
        "        h_gru=torch.cat((h_acc_gru,h_gyr_gru),dim=-1)\n",
        "\n",
        "\n",
        "        h_acc_att_lstm=self.temporal_attn_acc(x_acc_lstm)\n",
        "        h_gyr_att_lstm=self.temporal_attn_gyr(x_gyr_lstm)\n",
        "\n",
        "        h_acc_att_gru=self.temporal_attn_acc(x_acc_gru)\n",
        "        h_gyr_att_gru=self.temporal_attn_gyr(x_gyr_gru)\n",
        "\n",
        "\n",
        "        h_att_lstm=torch.cat((h_acc_att_lstm,h_gyr_att_lstm),dim=-1)\n",
        "        h_att_gru=torch.cat((h_acc_att_gru,h_gyr_att_gru),dim=-1)\n",
        "\n",
        "        #Do it separately\n",
        "\n",
        "        h_att_lstm=h_att_lstm.unsqueeze(0)\n",
        "        h_att_lstm=h_att_lstm.transpose(1,0)\n",
        "\n",
        "        h_att_gru=h_att_gru.unsqueeze(0)\n",
        "        h_att_gru=h_att_gru.transpose(1,0)\n",
        "\n",
        "        out=self.decoder(h_lstm, c_lstm, h_gru, h_att_lstm, h_att_gru, 9)\n",
        "\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "outputId": "df4f49ca-d764-4c60-ae1d-587d628ba8c6",
        "id": "LIBS4osatbgf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, time: 77.6309, Training Loss: 4.6769,  Validation loss: 2.8920\n",
            "Epoch: 2, time: 76.9063, Training Loss: 2.8422,  Validation loss: 2.4783\n",
            "Epoch: 3, time: 76.5121, Training Loss: 2.5531,  Validation loss: 2.2534\n",
            "Epoch: 4, time: 77.6159, Training Loss: 2.3707,  Validation loss: 2.2180\n",
            "Epoch: 5, time: 77.1676, Training Loss: 2.2544,  Validation loss: 2.0389\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-84-5de151113d57>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMM_ED_Bi_LSTM_GRU_WFW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmm_ed_bi_lstm_gru_WFW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_mm_m\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_mm_ed_bi_lstm_gru_WFW_IMU8.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-59-71b1fe5884af>\u001b[0m in \u001b[0;36mtrain_mm_m\u001b[0;34m(train_loader, learn_rate, EPOCHS, model, filename)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "lr = 0.001\n",
        "model = MM_ED_Bi_LSTM_GRU_WFW(24,24)\n",
        "\n",
        "mm_ed_bi_lstm_gru_WFW = train_mm_m(train_loader, lr,12,model,path+subject+'_mm_ed_bi_lstm_gru_WFW_IMU8.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d26f7eba-58fd-4f4e-89ba-526a49b54cef",
        "id": "SlIlPH9Ztbgf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ],
      "source": [
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "150ad6dc-cc94-4fab-f447-2e179760759f",
        "id": "XiXWCvVetbgf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(18718, 9, 6)\n",
            "5.1651073\n",
            "5.3739605\n",
            "3.3759341\n",
            "3.054235\n",
            "3.9383116\n",
            "3.5629349\n",
            "\n",
            "\n",
            "0.9801411821043012\n",
            "0.9810012799519433\n",
            "0.9465957776847811\n",
            "0.9856593184863209\n",
            "0.9854452058400446\n",
            "0.9505979458976829\n",
            "Mean: 4.078 +/- 0.594\n",
            "Mean: 0.972 +/- 0.020\n"
          ]
        }
      ],
      "source": [
        "mm_ed_bi_lstm_gru_WFW= MM_ED_Bi_LSTM_GRU_WFW(24,24)\n",
        "mm_ed_bi_lstm_gru_WFW.load_state_dict(torch.load(path+subject+'_mm_ed_bi_lstm_gru_WFW_IMU8.pth'))\n",
        "mm_ed_bi_lstm_gru_WFW.to(device)\n",
        "\n",
        "mm_ed_bi_lstm_gru_WFW.eval()\n",
        "\n",
        "# iterate through batches of test data\n",
        "with torch.no_grad():\n",
        "    for i, (data, data_acc, data_gyr, data_hof, target_present, target_future) in enumerate(test_loader):\n",
        "        output = mm_ed_bi_lstm_gru_WFW(data_acc.to(device).float(),data_gyr.to(device).float())\n",
        "        if i==0:\n",
        "          yhat_5=output\n",
        "          test_target=target_future\n",
        "\n",
        "        yhat_5=torch.cat((yhat_5,output),dim=0)\n",
        "        test_target=torch.cat((test_target,target_future),dim=0)\n",
        "\n",
        "        # clear memory\n",
        "        del data, target_future,output\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "\n",
        "yhat_4 = yhat_5.detach().cpu().numpy()\n",
        "test_target = test_target.detach().cpu().numpy()\n",
        "print(yhat_4.shape)\n",
        "\n",
        "rmse, p=RMSE_prediction(yhat_4,test_target)\n",
        "\n",
        "ablation_13=np.hstack([rmse,p])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ymtgb6Ertbgf"
      },
      "source": [
        "## 14. Attention Without gating+ Bi-LSTM + Bi-GRU --Encoder+decoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder_lstm(nn.Module):\n",
        "    def __init__(self, input_dim, dropout):\n",
        "        super(Encoder_lstm, self).__init__()\n",
        "        self.lstm_1 = nn.LSTM(input_dim, 128, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.lstm_2 = nn.LSTM(256, 64, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_1, _ = self.lstm_1(x)\n",
        "        out_1=self.dropout(out_1)\n",
        "        out_2, (h_n, c_n) = self.lstm_2(out_1)\n",
        "        out_2=self.dropout(out_2)\n",
        "\n",
        "        return out_2, (h_n, c_n)"
      ],
      "metadata": {
        "id": "SR5etmtPtbgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder_gru(nn.Module):\n",
        "    def __init__(self, input_dim, dropout):\n",
        "        super(Encoder_gru, self).__init__()\n",
        "        self.lstm_1 = nn.GRU(input_dim, 128, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.lstm_2 = nn.GRU(256, 64, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_1, _ = self.lstm_1(x)\n",
        "        out_1=self.dropout(out_1)\n",
        "        out_2, h_n = self.lstm_2(out_1)\n",
        "        out_2=self.dropout(out_2)\n",
        "\n",
        "        return out_2, h_n"
      ],
      "metadata": {
        "id": "hvXGVKWbtbgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMDecoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout):\n",
        "        super(LSTMDecoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, bidirectional=True, batch_first=True)\n",
        "        self.gru = nn.GRU(input_size, hidden_size, num_layers, bidirectional=True, batch_first=True)\n",
        "        self.fc = nn.Linear(2*2*hidden_size, output_size)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "        self.gating_net = nn.Sequential(nn.Linear(2*2*128+6, 2*2*128+6),nn.Sigmoid())\n",
        "\n",
        "\n",
        "    def forward(self, encoder_hidden, encoder_cell, h_gru, h_att_lstm, h_att_gru,  max_len):\n",
        "        batch_size = encoder_hidden.shape[1]\n",
        "        hidden = encoder_hidden\n",
        "        hidden_gru=h_gru\n",
        "        cell = encoder_cell\n",
        "        outputs = []\n",
        "\n",
        "        # Use the last time step of target as the initial input\n",
        "        input_1 = torch.zeros(batch_size,1,6).to(device)\n",
        "        input = torch.cat((input_1, h_att_lstm, h_att_gru), dim=-1)\n",
        "\n",
        "\n",
        "        for i in range(max_len):\n",
        "\n",
        "            gating_weight=self.gating_net(input)\n",
        "            input=input*gating_weight\n",
        "\n",
        "            # Run one time step of LSTM\n",
        "            output_1, (hidden, cell) = self.lstm(input, (hidden, cell))\n",
        "            output_2, hidden_gru = self.gru(input, hidden_gru)\n",
        "\n",
        "            output=torch.cat((output_1,output_2),dim=-1)\n",
        "\n",
        "            output=self.dropout(output)\n",
        "\n",
        "            # Use the output for prediction\n",
        "            output = self.fc(output.squeeze(1))\n",
        "            outputs.append(output.unsqueeze(1))\n",
        "\n",
        "            # Use the predicted output as the next input\n",
        "            input = torch.cat(( output.unsqueeze(1), input[:,:,6:518]), dim=-1)\n",
        "\n",
        "\n",
        "\n",
        "        # Concatenate all the outputs along the time dimension\n",
        "        outputs = torch.cat(outputs, dim=1)\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "BnilzHfmtbgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TemporalAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(TemporalAttention, self).__init__()\n",
        "\n",
        "        self.W = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, sequence_length, hidden_size)\n",
        "\n",
        "        # Calculate attention weights.\n",
        "        attn = self.V(torch.tanh(self.W(x)))\n",
        "        attn = attn.squeeze(-1)\n",
        "        attn = torch.softmax(attn, dim=1)\n",
        "\n",
        "        # Calculate weighted average of hidden states.\n",
        "        context = attn.unsqueeze(-1) * x\n",
        "        context = context.sum(dim=1)\n",
        "\n",
        "        return context"
      ],
      "metadata": {
        "id": "XlekrGI9tbgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1TgIlCKtbgg"
      },
      "outputs": [],
      "source": [
        "class MM_ED_Bi_LSTM_GRU_WF(nn.Module):\n",
        "    def __init__(self, input_acc, input_gyr, drop_prob=0.05):\n",
        "        super(MM_ED_Bi_LSTM_GRU_WF, self).__init__()\n",
        "\n",
        "        self.encoder_acc_lstm=Encoder_lstm(input_acc, drop_prob)\n",
        "        self.encoder_gyr_lstm=Encoder_lstm(input_gyr, drop_prob)\n",
        "\n",
        "        self.encoder_acc_gru=Encoder_gru(input_acc, drop_prob)\n",
        "        self.encoder_gyr_gru=Encoder_gru(input_gyr, drop_prob)\n",
        "\n",
        "        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)\n",
        "        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)\n",
        "\n",
        "        self.temporal_attn_acc = TemporalAttention(128)\n",
        "        self.temporal_attn_gyr = TemporalAttention(128)\n",
        "\n",
        "\n",
        "        self.decoder=LSTMDecoder(2*2*128+6, 2*64, 6, 1, 0.05)\n",
        "\n",
        "    def forward(self, x_acc, x_gyr):\n",
        "\n",
        "        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))\n",
        "        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))\n",
        "\n",
        "        x_acc_1=self.BN_acc(x_acc_1)\n",
        "        x_gyr_1=self.BN_gyr(x_gyr_1)\n",
        "\n",
        "        x_acc_2=x_acc_1.view(-1, window, x_acc_1.size(-1))\n",
        "        x_gyr_2=x_gyr_1.view(-1, window, x_gyr_1.size(-1))\n",
        "\n",
        "        x_acc_lstm,(h_acc_lstm, c_acc_lstm)=self.encoder_acc_lstm(x_acc_2)\n",
        "        x_gyr_lstm,(h_gyr_lstm, c_gyr_lstm)=self.encoder_gyr_lstm(x_gyr_2)\n",
        "\n",
        "\n",
        "        x_acc_gru,h_acc_gru=self.encoder_acc_gru(x_acc_2)\n",
        "        x_gyr_gru,h_gyr_gru=self.encoder_gyr_gru(x_gyr_2)\n",
        "\n",
        "\n",
        "        h_lstm=torch.cat((h_acc_lstm,h_gyr_lstm),dim=-1)\n",
        "        c_lstm=torch.cat((c_acc_lstm,c_gyr_lstm),dim=-1)\n",
        "\n",
        "        h_gru=torch.cat((h_acc_gru,h_gyr_gru),dim=-1)\n",
        "\n",
        "\n",
        "        h_acc_att_lstm=self.temporal_attn_acc(x_acc_lstm)\n",
        "        h_gyr_att_lstm=self.temporal_attn_gyr(x_gyr_lstm)\n",
        "\n",
        "\n",
        "        h_acc_att_gru=self.temporal_attn_acc(x_acc_gru)\n",
        "        h_gyr_att_gru=self.temporal_attn_gyr(x_gyr_gru)\n",
        "\n",
        "\n",
        "        h_att_lstm=torch.cat((h_acc_att_lstm,h_gyr_att_lstm),dim=-1)\n",
        "        h_att_gru=torch.cat((h_acc_att_gru,h_gyr_att_gru),dim=-1)\n",
        "\n",
        "        #Do it separately\n",
        "\n",
        "        h_att_lstm=h_att_lstm.unsqueeze(0)\n",
        "        h_att_lstm=h_att_lstm.transpose(1,0)\n",
        "\n",
        "        h_att_gru=h_att_gru.unsqueeze(0)\n",
        "        h_att_gru=h_att_gru.transpose(1,0)\n",
        "\n",
        "\n",
        "\n",
        "        out=self.decoder(h_lstm, c_lstm, h_gru, h_att_lstm, h_att_gru, 9)\n",
        "\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2487acfc-cdec-4db5-e78c-4bbfeda0dd6e",
        "id": "h0yDS_nGtbgg"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, time: 86.9404, Training Loss: 4.9244,  Validation loss: 2.7417\n",
            "Epoch: 2, time: 81.5889, Training Loss: 2.8192,  Validation loss: 2.3727\n",
            "Epoch: 3, time: 81.4215, Training Loss: 2.4946,  Validation loss: 2.1723\n",
            "Epoch: 4, time: 82.7003, Training Loss: 2.3114,  Validation loss: 2.0318\n",
            "Epoch: 5, time: 82.1997, Training Loss: 2.1898,  Validation loss: 1.9916\n",
            "Epoch: 6, time: 83.3610, Training Loss: 2.0968,  Validation loss: 1.9510\n",
            "Epoch: 7, time: 82.3095, Training Loss: 2.0201,  Validation loss: 1.8437\n",
            "Epoch: 8, time: 81.9306, Training Loss: 1.9579,  Validation loss: 1.8353\n",
            "Epoch: 9, time: 81.8455, Training Loss: 1.9016,  Validation loss: 1.8502\n",
            "Epoch: 10, time: 81.9938, Training Loss: 1.8544,  Validation loss: 1.7996\n",
            "Epoch: 11, time: 81.8331, Training Loss: 1.8099,  Validation loss: 1.7566\n",
            "Epoch: 12, time: 82.1858, Training Loss: 1.7705,  Validation loss: 1.7321\n",
            "Training time: 990.5891423225403 seconds\n"
          ]
        }
      ],
      "source": [
        "lr = 0.001\n",
        "model = MM_ED_Bi_LSTM_GRU_WF(24,24)\n",
        "\n",
        "mm_ed_bi_lstm_gru_WF = train_mm_m(train_loader, lr,12,model,path+subject+'_mm_ed_bi_lstm_gru_WF_IMU8.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90590e19-a4ce-4751-d59f-a2c1c4419c19",
        "id": "SV5V-SzNtbgh"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ],
      "source": [
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f24ea9e8-6d0c-45e2-afce-fdb0dbb29f40",
        "id": "ANsrfL6Stbgh"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(18718, 9, 6)\n",
            "5.264242\n",
            "5.434406\n",
            "3.3929915\n",
            "3.2035456\n",
            "3.7005498\n",
            "3.6432614\n",
            "\n",
            "\n",
            "0.9775940941088462\n",
            "0.9790625593898183\n",
            "0.9500854202670922\n",
            "0.9840026385691714\n",
            "0.9873507893704977\n",
            "0.9563770637314714\n",
            "Mean: 4.106 +/- 0.536\n",
            "Mean: 0.972 +/- 0.017\n"
          ]
        }
      ],
      "source": [
        "mm_ed_bi_lstm_gru_WF= MM_ED_Bi_LSTM_GRU_WF(24,24)\n",
        "mm_ed_bi_lstm_gru_WF.load_state_dict(torch.load(path+subject+'_mm_ed_bi_lstm_gru_WF_IMU8.pth'))\n",
        "mm_ed_bi_lstm_gru_WF.to(device)\n",
        "\n",
        "mm_ed_bi_lstm_gru_WF.eval()\n",
        "\n",
        "# iterate through batches of test data\n",
        "with torch.no_grad():\n",
        "    for i, (data, data_acc, data_gyr, data_hof, target_present, target_future) in enumerate(test_loader):\n",
        "        output = mm_ed_bi_lstm_gru_WF(data_acc.to(device).float(),data_gyr.to(device).float())\n",
        "        if i==0:\n",
        "          yhat_5=output\n",
        "          test_target=target_future\n",
        "\n",
        "        yhat_5=torch.cat((yhat_5,output),dim=0)\n",
        "        test_target=torch.cat((test_target,target_future),dim=0)\n",
        "\n",
        "        # clear memory\n",
        "        del data, target_future,output\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "\n",
        "yhat_4 = yhat_5.detach().cpu().numpy()\n",
        "test_target = test_target.detach().cpu().numpy()\n",
        "print(yhat_4.shape)\n",
        "\n",
        "rmse, p=RMSE_prediction(yhat_4,test_target)\n",
        "\n",
        "ablation_14=np.hstack([rmse,p])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "W17jUoA2pwjm",
        "8A4Eoxgop1qX",
        "Qj2C3WmxqC4r",
        "Jl71qLP9hK54",
        "DOb4ZMy9mfku",
        "m58C4hoLmw__",
        "ImUHYy9dtnZ7",
        "xAYgwE-G4pwG",
        "uiMMbyo1ygzg",
        "iYZarb34rns7",
        "tGMs0-M9kEHS",
        "JVK0-v61kEHb",
        "g7ZL1Z8aa_jC",
        "lNsrYRfheX_b",
        "2N-DsNDQkR6P",
        "-CQeFCGAAD8p",
        "SUlkjirxQ2z6",
        "2iJ-wk0eQ20B",
        "5qBNEnsKe5Gw",
        "GzXwU4E6qaWb",
        "sEZxWsCyG0_t",
        "vj45YYMkG0_z",
        "V__uBw92ANwZ",
        "LzFPBtS6ANwi",
        "EDyHRbpDANwj",
        "pqBJRVLLANwk",
        "SU8M0091ANwl",
        "bstAWR6xANwm",
        "zWDohhh1ANwn",
        "gBd8FBNKANwo",
        "Qkc7GwSIANwp",
        "vOHKs6gyANwr",
        "TsnBqCKbdsUF",
        "Ym4ptIK_tbgZ",
        "Ymtgb6Ertbgf",
        "7UBC5eTvpPQR",
        "VfBbLOQ5konb",
        "GPpV-xQapLTm"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNcKL4FY0vh8Iym5z4YxDs3",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}